{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "branch_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9jj876ACCgt",
        "colab_type": "text"
      },
      "source": [
        "#idea box\n",
        "-train model with only home stats and determine result comparing the predicted probabilities of both team wining individually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNCxOniS3fO3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c534cd2f-174d-40cf-845a-652e2f918e9a"
      },
      "source": [
        "#mount the google drive to import data in colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP2JpTCLQ8TP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRCOIEUTQ8VN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "8f8a2f75-f812-4a71-e26e-c03e9147e9b8"
      },
      "source": [
        "#Function to concat all the csv files into one file\n",
        "def concat(outfile = 'concat-data.csv'): #outfile = \"name of the output file\"\n",
        "    filenames = glob('gdrive/My Drive/Colab Notebooks/epl/dataset/E*.csv') #many filenames have similar pattern \n",
        "    dataframes = []\n",
        "    for files in filenames:\n",
        "        dataframes.append(pd.read_csv(files,header= 0,\n",
        "                        encoding= 'unicode_escape'))\n",
        "    concatDF = pd.concat(dataframes, axis=0,sort=False) #axis=0 to concat vertically\n",
        "    concatDF.to_csv('gdrive/My Drive/Colab Notebooks/epl/dataset/'+ outfile, index = None) #index=None for no indexing\n",
        "    \n",
        "concat() #concat function call\n",
        "    \n",
        "data = pd.read_csv('gdrive/My Drive/Colab Notebooks/epl/dataset/concat-data.csv')\n",
        "tester = pd.read_csv('gdrive/My Drive/Colab Notebooks/epl/dataset/tester.csv')\n",
        "# data = data.dropna(axis = 'columns', how = 'any',thresh=4000)  #drops the columns with null values \n",
        "\n",
        "display(data)\n",
        "#views the first five lines"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (53,77) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Div</th>\n",
              "      <th>Date</th>\n",
              "      <th>HomeTeam</th>\n",
              "      <th>AwayTeam</th>\n",
              "      <th>FTHG</th>\n",
              "      <th>FTAG</th>\n",
              "      <th>FTR</th>\n",
              "      <th>HTHG</th>\n",
              "      <th>HTAG</th>\n",
              "      <th>HTR</th>\n",
              "      <th>Referee</th>\n",
              "      <th>HS</th>\n",
              "      <th>AS</th>\n",
              "      <th>HST</th>\n",
              "      <th>AST</th>\n",
              "      <th>HF</th>\n",
              "      <th>AF</th>\n",
              "      <th>HC</th>\n",
              "      <th>AC</th>\n",
              "      <th>HY</th>\n",
              "      <th>AY</th>\n",
              "      <th>HR</th>\n",
              "      <th>AR</th>\n",
              "      <th>B365H</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365A</th>\n",
              "      <th>BWH</th>\n",
              "      <th>BWD</th>\n",
              "      <th>BWA</th>\n",
              "      <th>IWH</th>\n",
              "      <th>IWD</th>\n",
              "      <th>IWA</th>\n",
              "      <th>PSH</th>\n",
              "      <th>PSD</th>\n",
              "      <th>PSA</th>\n",
              "      <th>WHH</th>\n",
              "      <th>WHD</th>\n",
              "      <th>WHA</th>\n",
              "      <th>VCH</th>\n",
              "      <th>VCD</th>\n",
              "      <th>...</th>\n",
              "      <th>AvgAHH</th>\n",
              "      <th>AvgAHA</th>\n",
              "      <th>B365CH</th>\n",
              "      <th>B365CD</th>\n",
              "      <th>B365CA</th>\n",
              "      <th>BWCH</th>\n",
              "      <th>BWCD</th>\n",
              "      <th>BWCA</th>\n",
              "      <th>IWCH</th>\n",
              "      <th>IWCD</th>\n",
              "      <th>IWCA</th>\n",
              "      <th>WHCH</th>\n",
              "      <th>WHCD</th>\n",
              "      <th>WHCA</th>\n",
              "      <th>VCCH</th>\n",
              "      <th>VCCD</th>\n",
              "      <th>VCCA</th>\n",
              "      <th>MaxCH</th>\n",
              "      <th>MaxCD</th>\n",
              "      <th>MaxCA</th>\n",
              "      <th>AvgCH</th>\n",
              "      <th>AvgCD</th>\n",
              "      <th>AvgCA</th>\n",
              "      <th>B365C&gt;2.5</th>\n",
              "      <th>B365C&lt;2.5</th>\n",
              "      <th>PC&gt;2.5</th>\n",
              "      <th>PC&lt;2.5</th>\n",
              "      <th>MaxC&gt;2.5</th>\n",
              "      <th>MaxC&lt;2.5</th>\n",
              "      <th>AvgC&gt;2.5</th>\n",
              "      <th>AvgC&lt;2.5</th>\n",
              "      <th>AHCh</th>\n",
              "      <th>B365CAHH</th>\n",
              "      <th>B365CAHA</th>\n",
              "      <th>PCAHH</th>\n",
              "      <th>PCAHA</th>\n",
              "      <th>MaxCAHH</th>\n",
              "      <th>MaxCAHA</th>\n",
              "      <th>AvgCAHH</th>\n",
              "      <th>AvgCAHA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>E0</td>\n",
              "      <td>10/08/2018</td>\n",
              "      <td>Man United</td>\n",
              "      <td>Leicester</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>H</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>H</td>\n",
              "      <td>A Marriner</td>\n",
              "      <td>8.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.57</td>\n",
              "      <td>3.90</td>\n",
              "      <td>7.50</td>\n",
              "      <td>1.53</td>\n",
              "      <td>4.00</td>\n",
              "      <td>7.50</td>\n",
              "      <td>1.55</td>\n",
              "      <td>3.80</td>\n",
              "      <td>7.00</td>\n",
              "      <td>1.58</td>\n",
              "      <td>3.93</td>\n",
              "      <td>7.50</td>\n",
              "      <td>1.57</td>\n",
              "      <td>3.80</td>\n",
              "      <td>6.00</td>\n",
              "      <td>1.57</td>\n",
              "      <td>4.00</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>E0</td>\n",
              "      <td>11/08/2018</td>\n",
              "      <td>Bournemouth</td>\n",
              "      <td>Cardiff</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>H</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>H</td>\n",
              "      <td>K Friend</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.60</td>\n",
              "      <td>4.50</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.40</td>\n",
              "      <td>4.40</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.50</td>\n",
              "      <td>4.10</td>\n",
              "      <td>1.89</td>\n",
              "      <td>3.63</td>\n",
              "      <td>4.58</td>\n",
              "      <td>1.91</td>\n",
              "      <td>3.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.87</td>\n",
              "      <td>3.60</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>E0</td>\n",
              "      <td>11/08/2018</td>\n",
              "      <td>Fulham</td>\n",
              "      <td>Crystal Palace</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>M Dean</td>\n",
              "      <td>15.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.45</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.95</td>\n",
              "      <td>2.40</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.46</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.45</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.40</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>E0</td>\n",
              "      <td>11/08/2018</td>\n",
              "      <td>Huddersfield</td>\n",
              "      <td>Chelsea</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>A</td>\n",
              "      <td>C Kavanagh</td>\n",
              "      <td>6.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.61</td>\n",
              "      <td>6.25</td>\n",
              "      <td>3.90</td>\n",
              "      <td>1.57</td>\n",
              "      <td>6.20</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.55</td>\n",
              "      <td>6.41</td>\n",
              "      <td>4.02</td>\n",
              "      <td>1.62</td>\n",
              "      <td>5.80</td>\n",
              "      <td>3.90</td>\n",
              "      <td>1.57</td>\n",
              "      <td>6.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>E0</td>\n",
              "      <td>11/08/2018</td>\n",
              "      <td>Newcastle</td>\n",
              "      <td>Tottenham</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>A</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>A</td>\n",
              "      <td>M Atkinson</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.90</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.04</td>\n",
              "      <td>3.80</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.00</td>\n",
              "      <td>3.70</td>\n",
              "      <td>3.35</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.83</td>\n",
              "      <td>3.57</td>\n",
              "      <td>2.08</td>\n",
              "      <td>3.80</td>\n",
              "      <td>3.20</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.90</td>\n",
              "      <td>3.40</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24054</th>\n",
              "      <td>SC0</td>\n",
              "      <td>07/03/2020</td>\n",
              "      <td>Hamilton</td>\n",
              "      <td>Kilmarnock</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>H</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>D</td>\n",
              "      <td>W Collum</td>\n",
              "      <td>11.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.20</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.10</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.20</td>\n",
              "      <td>2.15</td>\n",
              "      <td>3.46</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.19</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.15</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.40</td>\n",
              "      <td>...</td>\n",
              "      <td>1.95</td>\n",
              "      <td>1.86</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.10</td>\n",
              "      <td>3.60</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.00</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.60</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.75</td>\n",
              "      <td>3.59</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.54</td>\n",
              "      <td>3.38</td>\n",
              "      <td>2.06</td>\n",
              "      <td>2.10</td>\n",
              "      <td>1.72</td>\n",
              "      <td>2.13</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.20</td>\n",
              "      <td>1.83</td>\n",
              "      <td>2.08</td>\n",
              "      <td>1.73</td>\n",
              "      <td>0.25</td>\n",
              "      <td>2.05</td>\n",
              "      <td>1.80</td>\n",
              "      <td>2.13</td>\n",
              "      <td>1.80</td>\n",
              "      <td>2.13</td>\n",
              "      <td>1.85</td>\n",
              "      <td>2.05</td>\n",
              "      <td>1.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24055</th>\n",
              "      <td>SC0</td>\n",
              "      <td>07/03/2020</td>\n",
              "      <td>Hearts</td>\n",
              "      <td>Motherwell</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>D</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>D Robertson</td>\n",
              "      <td>12.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.75</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.10</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.35</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.15</td>\n",
              "      <td>3.69</td>\n",
              "      <td>3.37</td>\n",
              "      <td>2.10</td>\n",
              "      <td>3.60</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.10</td>\n",
              "      <td>3.50</td>\n",
              "      <td>...</td>\n",
              "      <td>1.82</td>\n",
              "      <td>2.01</td>\n",
              "      <td>1.9</td>\n",
              "      <td>3.80</td>\n",
              "      <td>3.60</td>\n",
              "      <td>1.95</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.70</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.45</td>\n",
              "      <td>3.80</td>\n",
              "      <td>1.88</td>\n",
              "      <td>3.6</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.87</td>\n",
              "      <td>3.6</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.01</td>\n",
              "      <td>4.01</td>\n",
              "      <td>4.40</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.61</td>\n",
              "      <td>3.77</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.80</td>\n",
              "      <td>2.03</td>\n",
              "      <td>1.87</td>\n",
              "      <td>2.09</td>\n",
              "      <td>1.91</td>\n",
              "      <td>1.97</td>\n",
              "      <td>1.83</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.95</td>\n",
              "      <td>1.96</td>\n",
              "      <td>1.96</td>\n",
              "      <td>1.96</td>\n",
              "      <td>2.02</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24056</th>\n",
              "      <td>SC0</td>\n",
              "      <td>07/03/2020</td>\n",
              "      <td>St Johnstone</td>\n",
              "      <td>Livingston</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>H</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>D</td>\n",
              "      <td>K Clancy</td>\n",
              "      <td>3.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.75</td>\n",
              "      <td>2.60</td>\n",
              "      <td>3.05</td>\n",
              "      <td>2.75</td>\n",
              "      <td>2.67</td>\n",
              "      <td>3.37</td>\n",
              "      <td>2.76</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.75</td>\n",
              "      <td>2.60</td>\n",
              "      <td>3.20</td>\n",
              "      <td>...</td>\n",
              "      <td>1.85</td>\n",
              "      <td>1.98</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.75</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.85</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.1</td>\n",
              "      <td>2.88</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.2</td>\n",
              "      <td>2.88</td>\n",
              "      <td>2.77</td>\n",
              "      <td>3.43</td>\n",
              "      <td>2.98</td>\n",
              "      <td>2.56</td>\n",
              "      <td>3.13</td>\n",
              "      <td>2.82</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1.53</td>\n",
              "      <td>2.53</td>\n",
              "      <td>1.57</td>\n",
              "      <td>2.60</td>\n",
              "      <td>1.64</td>\n",
              "      <td>2.41</td>\n",
              "      <td>1.56</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.83</td>\n",
              "      <td>2.02</td>\n",
              "      <td>1.85</td>\n",
              "      <td>2.07</td>\n",
              "      <td>1.91</td>\n",
              "      <td>2.09</td>\n",
              "      <td>1.82</td>\n",
              "      <td>2.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24057</th>\n",
              "      <td>SC0</td>\n",
              "      <td>08/03/2020</td>\n",
              "      <td>Ross County</td>\n",
              "      <td>Rangers</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>D</td>\n",
              "      <td>A Dallas</td>\n",
              "      <td>3.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.00</td>\n",
              "      <td>7.00</td>\n",
              "      <td>1.20</td>\n",
              "      <td>10.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>1.25</td>\n",
              "      <td>10.00</td>\n",
              "      <td>5.60</td>\n",
              "      <td>1.25</td>\n",
              "      <td>11.11</td>\n",
              "      <td>6.17</td>\n",
              "      <td>1.26</td>\n",
              "      <td>11.00</td>\n",
              "      <td>5.80</td>\n",
              "      <td>1.25</td>\n",
              "      <td>10.50</td>\n",
              "      <td>5.75</td>\n",
              "      <td>...</td>\n",
              "      <td>1.86</td>\n",
              "      <td>1.94</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.50</td>\n",
              "      <td>1.22</td>\n",
              "      <td>10.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>1.25</td>\n",
              "      <td>6.90</td>\n",
              "      <td>5.40</td>\n",
              "      <td>1.33</td>\n",
              "      <td>12.00</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1.24</td>\n",
              "      <td>11.50</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>12.00</td>\n",
              "      <td>6.80</td>\n",
              "      <td>1.33</td>\n",
              "      <td>10.47</td>\n",
              "      <td>5.97</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.53</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1.54</td>\n",
              "      <td>2.58</td>\n",
              "      <td>1.59</td>\n",
              "      <td>2.63</td>\n",
              "      <td>1.52</td>\n",
              "      <td>2.49</td>\n",
              "      <td>1.75</td>\n",
              "      <td>1.88</td>\n",
              "      <td>1.98</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.98</td>\n",
              "      <td>1.96</td>\n",
              "      <td>2.05</td>\n",
              "      <td>1.87</td>\n",
              "      <td>1.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24058</th>\n",
              "      <td>SC0</td>\n",
              "      <td>11/03/2020</td>\n",
              "      <td>St Mirren</td>\n",
              "      <td>Hearts</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>H</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>D</td>\n",
              "      <td>A Muir</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.30</td>\n",
              "      <td>2.90</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.40</td>\n",
              "      <td>2.95</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.40</td>\n",
              "      <td>3.02</td>\n",
              "      <td>3.38</td>\n",
              "      <td>2.46</td>\n",
              "      <td>2.90</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.40</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.30</td>\n",
              "      <td>...</td>\n",
              "      <td>1.76</td>\n",
              "      <td>2.06</td>\n",
              "      <td>3.3</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.20</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.25</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.3</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.3</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.46</td>\n",
              "      <td>3.44</td>\n",
              "      <td>2.33</td>\n",
              "      <td>3.26</td>\n",
              "      <td>3.23</td>\n",
              "      <td>2.23</td>\n",
              "      <td>2.20</td>\n",
              "      <td>1.66</td>\n",
              "      <td>2.23</td>\n",
              "      <td>1.72</td>\n",
              "      <td>2.30</td>\n",
              "      <td>1.74</td>\n",
              "      <td>2.17</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.95</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.98</td>\n",
              "      <td>1.94</td>\n",
              "      <td>2.01</td>\n",
              "      <td>1.96</td>\n",
              "      <td>1.91</td>\n",
              "      <td>1.90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24059 rows Ã— 139 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Div        Date      HomeTeam  ... MaxCAHA  AvgCAHH  AvgCAHA\n",
              "0       E0  10/08/2018    Man United  ...     NaN      NaN      NaN\n",
              "1       E0  11/08/2018   Bournemouth  ...     NaN      NaN      NaN\n",
              "2       E0  11/08/2018        Fulham  ...     NaN      NaN      NaN\n",
              "3       E0  11/08/2018  Huddersfield  ...     NaN      NaN      NaN\n",
              "4       E0  11/08/2018     Newcastle  ...     NaN      NaN      NaN\n",
              "...    ...         ...           ...  ...     ...      ...      ...\n",
              "24054  SC0  07/03/2020      Hamilton  ...    1.85     2.05     1.78\n",
              "24055  SC0  07/03/2020        Hearts  ...    2.02     1.90     1.92\n",
              "24056  SC0  07/03/2020  St Johnstone  ...    2.09     1.82     2.01\n",
              "24057  SC0  08/03/2020   Ross County  ...    2.05     1.87     1.95\n",
              "24058  SC0  11/03/2020     St Mirren  ...    1.96     1.91     1.90\n",
              "\n",
              "[24059 rows x 139 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw0vXRBKQ8VX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "943af7cf-2855-4fc4-ad45-a482686b19b4"
      },
      "source": [
        "#displaying the teams of the league\n",
        "read_data = pd.read_csv('gdrive/My Drive/Colab Notebooks/epl/dataset/tester.csv')\n",
        "team_list = read_data['HomeTeam']\n",
        "team_name=[]\n",
        "for teams in team_list:\n",
        "    if teams not in team_name:\n",
        "        team_name.append(teams)\n",
        "print(\"\\n\\nTeams in Seasons till now\")\n",
        "display(team_name)\n",
        "print (type(team_name))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Teams in Seasons till now\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['Liverpool',\n",
              " 'West Ham',\n",
              " 'Bournemouth',\n",
              " 'Burnley',\n",
              " 'Crystal Palace',\n",
              " 'Watford',\n",
              " 'Tottenham',\n",
              " 'Leicester',\n",
              " 'Newcastle',\n",
              " 'Man United',\n",
              " 'Arsenal',\n",
              " 'Aston Villa',\n",
              " 'Brighton',\n",
              " 'Everton',\n",
              " 'Norwich',\n",
              " 'Southampton',\n",
              " 'Man City',\n",
              " 'Sheffield United',\n",
              " 'Chelsea',\n",
              " 'Wolves']"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddvHZvwSQ8Vi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7a3a3fd3-81bb-4aaf-8460-97bbd6dd21de"
      },
      "source": [
        "#WIN RATE FOR THE HOME TEAM\n",
        "matches = data.shape[0] #[0] for shape of x-axis\n",
        "features = data.shape[1] - 1  #[1] for shape of y-axis (total features - Labels to be determined)\n",
        "\n",
        "homewins = len(data[data.FTR == 'H'])\n",
        "win_rate = (float(homewins)/(matches))*100\n",
        "\n",
        "print(\"Total no of matches: {}\".format(matches))\n",
        "print(\"Number of Features: {}\".format(features))\n",
        "print(\"Number of matches won by HOME: {}\".format(homewins))\n",
        "print(\"Win rate of HOME team: {}\".format(win_rate))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total no of matches: 24059\n",
            "Number of Features: 138\n",
            "Number of matches won by HOME: 10322\n",
            "Win rate of HOME team: 42.90286379317511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXkT0M1lQ8Vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTeamData(teamName):\n",
        "#     print(\"\\n\"+ teamName + \"\\n\")\n",
        "    \n",
        "    #Num of goals in wins and looses\n",
        "    gamesHome = data[data['HomeTeam']== teamName]\n",
        "    totalGoalsScored = gamesHome['FTHG'].sum()\n",
        "    \n",
        "    gamesAway = data[data['AwayTeam'] == teamName]\n",
        "    totalGames = gamesHome.append(gamesAway)\n",
        "    numGames = len(totalGames.index)\n",
        "    totalGoalsScored += gamesAway['FTAG'].sum() \n",
        "    \n",
        "    \n",
        "    #total goals allowed \n",
        "    totalGoalsAllowed = gamesHome['FTHG'].sum()\n",
        "    totalGoalsAllowed += gamesAway['FTAG'].sum()\n",
        "    \n",
        "    #discipline TOTAL RED AND YELLOW CARDS\n",
        "    totalYellowCards = gamesHome['HY'].sum()\n",
        "    totalYellowCards += gamesAway['AY'].sum()\n",
        "    \n",
        "    totalRedCards = gamesHome['HR'].sum()\n",
        "    totalRedCards += gamesAway['AR'].sum()\n",
        "    \n",
        "    \n",
        "    #total Fouls\n",
        "    totalFouls = gamesHome['HF'].sum()\n",
        "    totalFouls += gamesAway['AF'].sum()\n",
        "    \n",
        "    \n",
        "    #total Corners\n",
        "    totalCorners = gamesHome['HC'].sum()\n",
        "    totalCorners += gamesAway['AC'].sum()\n",
        "    \n",
        "    \n",
        "    #shots per game (SPG) = totalshots / totalgames\n",
        "    totalShots = gamesHome['HS'].sum()\n",
        "    totalShots += gamesAway['AS'].sum()\n",
        "    \n",
        "    #avg shots allowed per game\n",
        "    totalShotsAgainst = gamesHome['AS'].sum()\n",
        "    totalShotsAgainst += gamesAway['HS'].sum()\n",
        "    if numGames != 0:\n",
        "        HSPG = totalShots / numGames #HomeShotsPerGame\n",
        "        ASPG = totalShotsAgainst / numGames #AwayShotsPerGame\n",
        "#         display(\"HSPG: {}\".format(HSPG))\n",
        "#         display(\"ASPG: {}\".format(ASPG))\n",
        "    \n",
        "    #games won percentage= GamesWon / numGames\n",
        "    gamesWon = totalGames[totalGames['FTR']== \"H\"]\n",
        "    gamesLost = totalGames[totalGames['FTR'] == \"A\"]\n",
        "    gamesDraw = totalGames[totalGames['FTR'] == \"D\"]\n",
        "    numGamesWon = len(gamesWon.index)\n",
        "    numGamesLost = len(gamesLost.index)\n",
        "    numGamesDraw = len(gamesDraw.index)\n",
        "    \n",
        "    if numGames != 0:\n",
        "        gamesWonPercent = numGamesWon / numGames\n",
        "        gamesLostPercent = numGamesLost / numGames\n",
        "        gamesDrawPercent = numGamesDraw / numGames \n",
        "    \n",
        "#     print(\"Games Win Percent: {}\".format(gamesWonPercent))\n",
        "#     print(\"Games Loose Percent: {}\".format(gamesLostPercent))\n",
        "#     print(\"Games Draw Percent: {}\".format(gamesDrawPercent))\n",
        "    \n",
        "    \n",
        "    #Total shots on target:\n",
        "    totalShotsOnTarget = gamesHome['HST'].sum()\n",
        "    totalShotsOnTarget += gamesAway['AST'].sum()\n",
        "    \n",
        "    #GoalSaves\n",
        "    goalSaves = totalShotsOnTarget - totalGoalsAllowed\n",
        "    \n",
        "    #Goal Save Percentage\n",
        "    if totalShotsOnTarget != 0:\n",
        "        goalSavesPercent = goalSaves / totalShotsOnTarget\n",
        "        \n",
        "    #Goal Save Ratio\n",
        "    if goalSaves != 0:\n",
        "        saveRatio = totalShotsOnTarget / goalSaves\n",
        "    \n",
        "    #Goal scoring Percent\n",
        "    if totalShots != 0 :\n",
        "        scoringPercent = (totalShots - totalGoalsScored)/totalShots\n",
        "    \n",
        "    #Goal scoring Ration\n",
        "    if totalGoalsScored != 0:\n",
        "        scoringRatio = totalShotsOnTarget / totalGoalsScored\n",
        "        \n",
        "    if numGames == 0: \n",
        "        gamesWon = 0\n",
        "        gamesLost = 0\n",
        "        gamesDraw = 0 \n",
        "        totalGoalsScored = 0 \n",
        "        totalShotsOnTarget = 0 \n",
        "        totalGoalsAllowed = 0 \n",
        "        totalYellowCards = 0 \n",
        "        totalRedCards = 0 \n",
        "        totalFouls = 0 \n",
        "        totalCorners = 0 \n",
        "        totalShots = 0 \n",
        "        totalShotsAgainst = 0 \n",
        "        HSPG = 0 #HomeShotsPerGame \n",
        "        ASPG = 0 #AwayShotsPerGame \n",
        "        goalSaves = 0 \n",
        "        goalSavesPercent = 0 \n",
        "        scoringPercent = 0 \n",
        "        saveRatio = 0 \n",
        "        scoringRatio = 0\n",
        "    \n",
        "    return [teamName, totalGoalsScored, totalShotsOnTarget, totalGoalsAllowed, \n",
        "            totalYellowCards, totalRedCards,totalFouls, totalCorners, \n",
        "            totalShots, totalShotsAgainst, HSPG, ASPG, goalSaves, goalSavesPercent, scoringPercent,\n",
        "            saveRatio, scoringRatio]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKUrPQNfQ8V8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getTeamData('Chelsea')   #to check the team stats\n",
        "\n",
        "new_datalist = []\n",
        "for team in team_name:\n",
        "    team_vector = getTeamData(team) \n",
        "    new_datalist.append(team_vector)\n",
        "    \n",
        "teamStats = pd.DataFrame(new_datalist, columns=['TeamName', 'totalGoalsScored', 'totalShotsOnTarget', 'totalGoalsAllowed', \n",
        "            'totalYellowCards', 'totalRedCards','totalFouls', 'totalCorners', \n",
        "            'totalShots', 'totalShotsAgainst', 'HSPG', 'ASPG', 'goalSaves', 'goalSavesPercent', 'scoringPercent',\n",
        "            'saveRatio', 'scoringRatio'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je-r5pMaQ8YN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "outputId": "ff8219b6-3e2d-43e8-8905-28c1710f5cf4"
      },
      "source": [
        "display(teamStats)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TeamName</th>\n",
              "      <th>totalGoalsScored</th>\n",
              "      <th>totalShotsOnTarget</th>\n",
              "      <th>totalGoalsAllowed</th>\n",
              "      <th>totalYellowCards</th>\n",
              "      <th>totalRedCards</th>\n",
              "      <th>totalFouls</th>\n",
              "      <th>totalCorners</th>\n",
              "      <th>totalShots</th>\n",
              "      <th>totalShotsAgainst</th>\n",
              "      <th>HSPG</th>\n",
              "      <th>ASPG</th>\n",
              "      <th>goalSaves</th>\n",
              "      <th>goalSavesPercent</th>\n",
              "      <th>scoringPercent</th>\n",
              "      <th>saveRatio</th>\n",
              "      <th>scoringRatio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Liverpool</td>\n",
              "      <td>644.0</td>\n",
              "      <td>2334.0</td>\n",
              "      <td>644.0</td>\n",
              "      <td>489.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>3447.0</td>\n",
              "      <td>2222.0</td>\n",
              "      <td>5477.0</td>\n",
              "      <td>3351.0</td>\n",
              "      <td>16.014620</td>\n",
              "      <td>9.798246</td>\n",
              "      <td>1690.0</td>\n",
              "      <td>0.724079</td>\n",
              "      <td>0.882417</td>\n",
              "      <td>1.381065</td>\n",
              "      <td>3.624224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>West Ham</td>\n",
              "      <td>465.0</td>\n",
              "      <td>1676.0</td>\n",
              "      <td>465.0</td>\n",
              "      <td>588.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>3621.0</td>\n",
              "      <td>1876.0</td>\n",
              "      <td>4259.0</td>\n",
              "      <td>5033.0</td>\n",
              "      <td>12.168571</td>\n",
              "      <td>14.380000</td>\n",
              "      <td>1211.0</td>\n",
              "      <td>0.722554</td>\n",
              "      <td>0.890819</td>\n",
              "      <td>1.383980</td>\n",
              "      <td>3.604301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bournemouth</td>\n",
              "      <td>490.0</td>\n",
              "      <td>1685.0</td>\n",
              "      <td>490.0</td>\n",
              "      <td>473.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3153.0</td>\n",
              "      <td>2029.0</td>\n",
              "      <td>4352.0</td>\n",
              "      <td>4052.0</td>\n",
              "      <td>12.952381</td>\n",
              "      <td>12.059524</td>\n",
              "      <td>1195.0</td>\n",
              "      <td>0.709199</td>\n",
              "      <td>0.887408</td>\n",
              "      <td>1.410042</td>\n",
              "      <td>3.438776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Burnley</td>\n",
              "      <td>418.0</td>\n",
              "      <td>1464.0</td>\n",
              "      <td>418.0</td>\n",
              "      <td>513.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>3421.0</td>\n",
              "      <td>1596.0</td>\n",
              "      <td>3922.0</td>\n",
              "      <td>4754.0</td>\n",
              "      <td>11.672619</td>\n",
              "      <td>14.148810</td>\n",
              "      <td>1046.0</td>\n",
              "      <td>0.714481</td>\n",
              "      <td>0.893422</td>\n",
              "      <td>1.399618</td>\n",
              "      <td>3.502392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Crystal Palace</td>\n",
              "      <td>355.0</td>\n",
              "      <td>1298.0</td>\n",
              "      <td>355.0</td>\n",
              "      <td>527.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>3623.0</td>\n",
              "      <td>1605.0</td>\n",
              "      <td>3577.0</td>\n",
              "      <td>4183.0</td>\n",
              "      <td>11.178125</td>\n",
              "      <td>13.071875</td>\n",
              "      <td>943.0</td>\n",
              "      <td>0.726502</td>\n",
              "      <td>0.900755</td>\n",
              "      <td>1.376458</td>\n",
              "      <td>3.656338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Watford</td>\n",
              "      <td>474.0</td>\n",
              "      <td>1549.0</td>\n",
              "      <td>474.0</td>\n",
              "      <td>581.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>3826.0</td>\n",
              "      <td>1697.0</td>\n",
              "      <td>3949.0</td>\n",
              "      <td>4179.0</td>\n",
              "      <td>11.752976</td>\n",
              "      <td>12.437500</td>\n",
              "      <td>1075.0</td>\n",
              "      <td>0.693996</td>\n",
              "      <td>0.879970</td>\n",
              "      <td>1.440930</td>\n",
              "      <td>3.267932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Tottenham</td>\n",
              "      <td>596.0</td>\n",
              "      <td>2349.0</td>\n",
              "      <td>596.0</td>\n",
              "      <td>533.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>3530.0</td>\n",
              "      <td>2221.0</td>\n",
              "      <td>5408.0</td>\n",
              "      <td>3751.0</td>\n",
              "      <td>15.812865</td>\n",
              "      <td>10.967836</td>\n",
              "      <td>1753.0</td>\n",
              "      <td>0.746275</td>\n",
              "      <td>0.889793</td>\n",
              "      <td>1.339989</td>\n",
              "      <td>3.941275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Leicester</td>\n",
              "      <td>494.0</td>\n",
              "      <td>1598.0</td>\n",
              "      <td>494.0</td>\n",
              "      <td>446.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>3382.0</td>\n",
              "      <td>1865.0</td>\n",
              "      <td>4236.0</td>\n",
              "      <td>3947.0</td>\n",
              "      <td>12.914634</td>\n",
              "      <td>12.033537</td>\n",
              "      <td>1104.0</td>\n",
              "      <td>0.690864</td>\n",
              "      <td>0.883381</td>\n",
              "      <td>1.447464</td>\n",
              "      <td>3.234818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Newcastle</td>\n",
              "      <td>450.0</td>\n",
              "      <td>1781.0</td>\n",
              "      <td>450.0</td>\n",
              "      <td>578.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>3879.0</td>\n",
              "      <td>1779.0</td>\n",
              "      <td>4397.0</td>\n",
              "      <td>4261.0</td>\n",
              "      <td>12.562857</td>\n",
              "      <td>12.174286</td>\n",
              "      <td>1331.0</td>\n",
              "      <td>0.747333</td>\n",
              "      <td>0.897657</td>\n",
              "      <td>1.338092</td>\n",
              "      <td>3.957778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Man United</td>\n",
              "      <td>615.0</td>\n",
              "      <td>2097.0</td>\n",
              "      <td>615.0</td>\n",
              "      <td>574.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>3782.0</td>\n",
              "      <td>2036.0</td>\n",
              "      <td>4739.0</td>\n",
              "      <td>3836.0</td>\n",
              "      <td>13.856725</td>\n",
              "      <td>11.216374</td>\n",
              "      <td>1482.0</td>\n",
              "      <td>0.706724</td>\n",
              "      <td>0.870226</td>\n",
              "      <td>1.414980</td>\n",
              "      <td>3.409756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Arsenal</td>\n",
              "      <td>646.0</td>\n",
              "      <td>2272.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>535.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>3433.0</td>\n",
              "      <td>2131.0</td>\n",
              "      <td>5058.0</td>\n",
              "      <td>3711.0</td>\n",
              "      <td>14.789474</td>\n",
              "      <td>10.850877</td>\n",
              "      <td>1626.0</td>\n",
              "      <td>0.715669</td>\n",
              "      <td>0.872282</td>\n",
              "      <td>1.397294</td>\n",
              "      <td>3.517028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Aston Villa</td>\n",
              "      <td>430.0</td>\n",
              "      <td>1659.0</td>\n",
              "      <td>430.0</td>\n",
              "      <td>663.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>4141.0</td>\n",
              "      <td>1886.0</td>\n",
              "      <td>4252.0</td>\n",
              "      <td>4682.0</td>\n",
              "      <td>11.617486</td>\n",
              "      <td>12.792350</td>\n",
              "      <td>1229.0</td>\n",
              "      <td>0.740808</td>\n",
              "      <td>0.898871</td>\n",
              "      <td>1.349878</td>\n",
              "      <td>3.858140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Brighton</td>\n",
              "      <td>366.0</td>\n",
              "      <td>1200.0</td>\n",
              "      <td>366.0</td>\n",
              "      <td>516.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>3522.0</td>\n",
              "      <td>1545.0</td>\n",
              "      <td>3645.0</td>\n",
              "      <td>3830.0</td>\n",
              "      <td>11.911765</td>\n",
              "      <td>12.516340</td>\n",
              "      <td>834.0</td>\n",
              "      <td>0.695000</td>\n",
              "      <td>0.899588</td>\n",
              "      <td>1.438849</td>\n",
              "      <td>3.278689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Everton</td>\n",
              "      <td>484.0</td>\n",
              "      <td>1879.0</td>\n",
              "      <td>484.0</td>\n",
              "      <td>520.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>3739.0</td>\n",
              "      <td>1905.0</td>\n",
              "      <td>4433.0</td>\n",
              "      <td>4205.0</td>\n",
              "      <td>12.961988</td>\n",
              "      <td>12.295322</td>\n",
              "      <td>1395.0</td>\n",
              "      <td>0.742416</td>\n",
              "      <td>0.890819</td>\n",
              "      <td>1.346953</td>\n",
              "      <td>3.882231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Norwich</td>\n",
              "      <td>558.0</td>\n",
              "      <td>1978.0</td>\n",
              "      <td>558.0</td>\n",
              "      <td>642.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>4309.0</td>\n",
              "      <td>2117.0</td>\n",
              "      <td>5086.0</td>\n",
              "      <td>4777.0</td>\n",
              "      <td>13.314136</td>\n",
              "      <td>12.505236</td>\n",
              "      <td>1420.0</td>\n",
              "      <td>0.717897</td>\n",
              "      <td>0.890287</td>\n",
              "      <td>1.392958</td>\n",
              "      <td>3.544803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Southampton</td>\n",
              "      <td>424.0</td>\n",
              "      <td>1550.0</td>\n",
              "      <td>424.0</td>\n",
              "      <td>461.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>3426.0</td>\n",
              "      <td>1727.0</td>\n",
              "      <td>4046.0</td>\n",
              "      <td>3481.0</td>\n",
              "      <td>12.967949</td>\n",
              "      <td>11.157051</td>\n",
              "      <td>1126.0</td>\n",
              "      <td>0.726452</td>\n",
              "      <td>0.895205</td>\n",
              "      <td>1.376554</td>\n",
              "      <td>3.655660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Man City</td>\n",
              "      <td>756.0</td>\n",
              "      <td>2441.0</td>\n",
              "      <td>756.0</td>\n",
              "      <td>576.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>3596.0</td>\n",
              "      <td>2430.0</td>\n",
              "      <td>5677.0</td>\n",
              "      <td>3005.0</td>\n",
              "      <td>16.599415</td>\n",
              "      <td>8.786550</td>\n",
              "      <td>1685.0</td>\n",
              "      <td>0.690291</td>\n",
              "      <td>0.866831</td>\n",
              "      <td>1.448665</td>\n",
              "      <td>3.228836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Sheffield United</td>\n",
              "      <td>554.0</td>\n",
              "      <td>1850.0</td>\n",
              "      <td>554.0</td>\n",
              "      <td>585.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>4140.0</td>\n",
              "      <td>2340.0</td>\n",
              "      <td>4244.0</td>\n",
              "      <td>3715.0</td>\n",
              "      <td>11.532609</td>\n",
              "      <td>10.095109</td>\n",
              "      <td>1296.0</td>\n",
              "      <td>0.700541</td>\n",
              "      <td>0.869463</td>\n",
              "      <td>1.427469</td>\n",
              "      <td>3.339350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Chelsea</td>\n",
              "      <td>622.0</td>\n",
              "      <td>2247.0</td>\n",
              "      <td>622.0</td>\n",
              "      <td>539.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>3485.0</td>\n",
              "      <td>2127.0</td>\n",
              "      <td>5394.0</td>\n",
              "      <td>3598.0</td>\n",
              "      <td>15.771930</td>\n",
              "      <td>10.520468</td>\n",
              "      <td>1625.0</td>\n",
              "      <td>0.723186</td>\n",
              "      <td>0.884687</td>\n",
              "      <td>1.382769</td>\n",
              "      <td>3.612540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Wolves</td>\n",
              "      <td>392.0</td>\n",
              "      <td>1354.0</td>\n",
              "      <td>392.0</td>\n",
              "      <td>480.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>3359.0</td>\n",
              "      <td>1586.0</td>\n",
              "      <td>3538.0</td>\n",
              "      <td>3809.0</td>\n",
              "      <td>11.872483</td>\n",
              "      <td>12.781879</td>\n",
              "      <td>962.0</td>\n",
              "      <td>0.710487</td>\n",
              "      <td>0.889203</td>\n",
              "      <td>1.407484</td>\n",
              "      <td>3.454082</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            TeamName  totalGoalsScored  ...  saveRatio  scoringRatio\n",
              "0          Liverpool             644.0  ...   1.381065      3.624224\n",
              "1           West Ham             465.0  ...   1.383980      3.604301\n",
              "2        Bournemouth             490.0  ...   1.410042      3.438776\n",
              "3            Burnley             418.0  ...   1.399618      3.502392\n",
              "4     Crystal Palace             355.0  ...   1.376458      3.656338\n",
              "5            Watford             474.0  ...   1.440930      3.267932\n",
              "6          Tottenham             596.0  ...   1.339989      3.941275\n",
              "7          Leicester             494.0  ...   1.447464      3.234818\n",
              "8          Newcastle             450.0  ...   1.338092      3.957778\n",
              "9         Man United             615.0  ...   1.414980      3.409756\n",
              "10           Arsenal             646.0  ...   1.397294      3.517028\n",
              "11       Aston Villa             430.0  ...   1.349878      3.858140\n",
              "12          Brighton             366.0  ...   1.438849      3.278689\n",
              "13           Everton             484.0  ...   1.346953      3.882231\n",
              "14           Norwich             558.0  ...   1.392958      3.544803\n",
              "15       Southampton             424.0  ...   1.376554      3.655660\n",
              "16          Man City             756.0  ...   1.448665      3.228836\n",
              "17  Sheffield United             554.0  ...   1.427469      3.339350\n",
              "18           Chelsea             622.0  ...   1.382769      3.612540\n",
              "19            Wolves             392.0  ...   1.407484      3.454082\n",
              "\n",
              "[20 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd13915OQ8YU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# display(data.isnull().sum())\n",
        "# teamStats.isnull().sum()\n",
        "# #comparision of the attributes for nullness in whole data and selected data\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ-XszpbQ8Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import seaborn as se\n",
        "# se.set(rc={'figure.figsize':(13.7,10.27)})\n",
        "# totalGoalsScored = se.barplot(teamStats.TeamName, teamStats.totalGoalsScored,palette=\"Blues_d\") \n",
        "# for item in totalGoalsScored.get_xticklabels():\n",
        "#     item.set_rotation(90)\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZlSjGfzQ8Yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# totalShotsOnTargetPlot = se.barplot(teamStats.TeamName, teamStats.totalShotsOnTarget,palette=\"Blues_d\")\n",
        "# for item in totalShotsOnTargetPlot.get_xticklabels():\n",
        "#     item.set_rotation(90)\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_id_s7gQ8Yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# totalShotsPlot = se.barplot(teamStats.TeamName, teamStats.totalShots,palette=\"Blues_d\") \n",
        "# for item in totalShotsPlot.get_xticklabels():\n",
        "#     item.set_rotation(90)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuM2WzyoQ8Y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scoringRatioPlot = se.barplot(teamStats.TeamName, teamStats.scoringRatio,palette=\"Blues_d\") \n",
        "# for item in scoringRatioPlot.get_xticklabels():\n",
        "#     item.set_rotation(90)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meEM62z1Q8Y7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ffb9d14-eb6e-4305-c00d-0afb25bb024d"
      },
      "source": [
        "# # #filtering out old teams of the season \n",
        "# filteredData = data[(data.HomeTeam.isin(team_name))]\n",
        "# data = filteredData[(filteredData.AwayTeam.isin(team_name))]\n",
        "display(data.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(24059, 139)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvq4aR6G7RkB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "add4aa18-908b-47f5-b89f-a369d7d7eae6"
      },
      "source": [
        "#droping the columns with nan in following columns\n",
        "data=data.dropna(subset=['HTHG','HTAG','HS','AS','HST','AST','HF','AF','HY','AY','HR','AR','HC','AC','HTR'])\n",
        "data.replace({\"HTR\":{'H':1,'A':2,'D':0}},inplace=True)\n",
        "display(data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Div</th>\n",
              "      <th>Date</th>\n",
              "      <th>HomeTeam</th>\n",
              "      <th>AwayTeam</th>\n",
              "      <th>FTHG</th>\n",
              "      <th>FTAG</th>\n",
              "      <th>FTR</th>\n",
              "      <th>HTHG</th>\n",
              "      <th>HTAG</th>\n",
              "      <th>HTR</th>\n",
              "      <th>Referee</th>\n",
              "      <th>HS</th>\n",
              "      <th>AS</th>\n",
              "      <th>HST</th>\n",
              "      <th>AST</th>\n",
              "      <th>HF</th>\n",
              "      <th>AF</th>\n",
              "      <th>HC</th>\n",
              "      <th>AC</th>\n",
              "      <th>HY</th>\n",
              "      <th>AY</th>\n",
              "      <th>HR</th>\n",
              "      <th>AR</th>\n",
              "      <th>B365H</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365A</th>\n",
              "      <th>BWH</th>\n",
              "      <th>BWD</th>\n",
              "      <th>BWA</th>\n",
              "      <th>IWH</th>\n",
              "      <th>IWD</th>\n",
              "      <th>IWA</th>\n",
              "      <th>PSH</th>\n",
              "      <th>PSD</th>\n",
              "      <th>PSA</th>\n",
              "      <th>WHH</th>\n",
              "      <th>WHD</th>\n",
              "      <th>WHA</th>\n",
              "      <th>VCH</th>\n",
              "      <th>VCD</th>\n",
              "      <th>...</th>\n",
              "      <th>AvgAHH</th>\n",
              "      <th>AvgAHA</th>\n",
              "      <th>B365CH</th>\n",
              "      <th>B365CD</th>\n",
              "      <th>B365CA</th>\n",
              "      <th>BWCH</th>\n",
              "      <th>BWCD</th>\n",
              "      <th>BWCA</th>\n",
              "      <th>IWCH</th>\n",
              "      <th>IWCD</th>\n",
              "      <th>IWCA</th>\n",
              "      <th>WHCH</th>\n",
              "      <th>WHCD</th>\n",
              "      <th>WHCA</th>\n",
              "      <th>VCCH</th>\n",
              "      <th>VCCD</th>\n",
              "      <th>VCCA</th>\n",
              "      <th>MaxCH</th>\n",
              "      <th>MaxCD</th>\n",
              "      <th>MaxCA</th>\n",
              "      <th>AvgCH</th>\n",
              "      <th>AvgCD</th>\n",
              "      <th>AvgCA</th>\n",
              "      <th>B365C&gt;2.5</th>\n",
              "      <th>B365C&lt;2.5</th>\n",
              "      <th>PC&gt;2.5</th>\n",
              "      <th>PC&lt;2.5</th>\n",
              "      <th>MaxC&gt;2.5</th>\n",
              "      <th>MaxC&lt;2.5</th>\n",
              "      <th>AvgC&gt;2.5</th>\n",
              "      <th>AvgC&lt;2.5</th>\n",
              "      <th>AHCh</th>\n",
              "      <th>B365CAHH</th>\n",
              "      <th>B365CAHA</th>\n",
              "      <th>PCAHH</th>\n",
              "      <th>PCAHA</th>\n",
              "      <th>MaxCAHH</th>\n",
              "      <th>MaxCAHA</th>\n",
              "      <th>AvgCAHH</th>\n",
              "      <th>AvgCAHA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>E0</td>\n",
              "      <td>10/08/2018</td>\n",
              "      <td>Man United</td>\n",
              "      <td>Leicester</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>H</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>A Marriner</td>\n",
              "      <td>8.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.57</td>\n",
              "      <td>3.90</td>\n",
              "      <td>7.50</td>\n",
              "      <td>1.53</td>\n",
              "      <td>4.00</td>\n",
              "      <td>7.50</td>\n",
              "      <td>1.55</td>\n",
              "      <td>3.80</td>\n",
              "      <td>7.00</td>\n",
              "      <td>1.58</td>\n",
              "      <td>3.93</td>\n",
              "      <td>7.50</td>\n",
              "      <td>1.57</td>\n",
              "      <td>3.80</td>\n",
              "      <td>6.00</td>\n",
              "      <td>1.57</td>\n",
              "      <td>4.00</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>E0</td>\n",
              "      <td>11/08/2018</td>\n",
              "      <td>Bournemouth</td>\n",
              "      <td>Cardiff</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>H</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>K Friend</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.60</td>\n",
              "      <td>4.50</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.40</td>\n",
              "      <td>4.40</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.50</td>\n",
              "      <td>4.10</td>\n",
              "      <td>1.89</td>\n",
              "      <td>3.63</td>\n",
              "      <td>4.58</td>\n",
              "      <td>1.91</td>\n",
              "      <td>3.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.87</td>\n",
              "      <td>3.60</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>E0</td>\n",
              "      <td>11/08/2018</td>\n",
              "      <td>Fulham</td>\n",
              "      <td>Crystal Palace</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>M Dean</td>\n",
              "      <td>15.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.45</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.95</td>\n",
              "      <td>2.40</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.46</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.45</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.40</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>E0</td>\n",
              "      <td>11/08/2018</td>\n",
              "      <td>Huddersfield</td>\n",
              "      <td>Chelsea</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>C Kavanagh</td>\n",
              "      <td>6.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.61</td>\n",
              "      <td>6.25</td>\n",
              "      <td>3.90</td>\n",
              "      <td>1.57</td>\n",
              "      <td>6.20</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.55</td>\n",
              "      <td>6.41</td>\n",
              "      <td>4.02</td>\n",
              "      <td>1.62</td>\n",
              "      <td>5.80</td>\n",
              "      <td>3.90</td>\n",
              "      <td>1.57</td>\n",
              "      <td>6.50</td>\n",
              "      <td>4.00</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>E0</td>\n",
              "      <td>11/08/2018</td>\n",
              "      <td>Newcastle</td>\n",
              "      <td>Tottenham</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>A</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>M Atkinson</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.90</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.04</td>\n",
              "      <td>3.80</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.00</td>\n",
              "      <td>3.70</td>\n",
              "      <td>3.35</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.83</td>\n",
              "      <td>3.57</td>\n",
              "      <td>2.08</td>\n",
              "      <td>3.80</td>\n",
              "      <td>3.20</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.90</td>\n",
              "      <td>3.40</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24054</th>\n",
              "      <td>SC0</td>\n",
              "      <td>07/03/2020</td>\n",
              "      <td>Hamilton</td>\n",
              "      <td>Kilmarnock</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>H</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>W Collum</td>\n",
              "      <td>11.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.20</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.10</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.20</td>\n",
              "      <td>2.15</td>\n",
              "      <td>3.46</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.19</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.15</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.40</td>\n",
              "      <td>...</td>\n",
              "      <td>1.95</td>\n",
              "      <td>1.86</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.10</td>\n",
              "      <td>3.60</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.00</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.60</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.75</td>\n",
              "      <td>3.59</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.54</td>\n",
              "      <td>3.38</td>\n",
              "      <td>2.06</td>\n",
              "      <td>2.10</td>\n",
              "      <td>1.72</td>\n",
              "      <td>2.13</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.20</td>\n",
              "      <td>1.83</td>\n",
              "      <td>2.08</td>\n",
              "      <td>1.73</td>\n",
              "      <td>0.25</td>\n",
              "      <td>2.05</td>\n",
              "      <td>1.80</td>\n",
              "      <td>2.13</td>\n",
              "      <td>1.80</td>\n",
              "      <td>2.13</td>\n",
              "      <td>1.85</td>\n",
              "      <td>2.05</td>\n",
              "      <td>1.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24055</th>\n",
              "      <td>SC0</td>\n",
              "      <td>07/03/2020</td>\n",
              "      <td>Hearts</td>\n",
              "      <td>Motherwell</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>D</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>D Robertson</td>\n",
              "      <td>12.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.75</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.10</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.35</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.15</td>\n",
              "      <td>3.69</td>\n",
              "      <td>3.37</td>\n",
              "      <td>2.10</td>\n",
              "      <td>3.60</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.10</td>\n",
              "      <td>3.50</td>\n",
              "      <td>...</td>\n",
              "      <td>1.82</td>\n",
              "      <td>2.01</td>\n",
              "      <td>1.9</td>\n",
              "      <td>3.80</td>\n",
              "      <td>3.60</td>\n",
              "      <td>1.95</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.70</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.45</td>\n",
              "      <td>3.80</td>\n",
              "      <td>1.88</td>\n",
              "      <td>3.6</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.87</td>\n",
              "      <td>3.6</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.01</td>\n",
              "      <td>4.01</td>\n",
              "      <td>4.40</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.61</td>\n",
              "      <td>3.77</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.80</td>\n",
              "      <td>2.03</td>\n",
              "      <td>1.87</td>\n",
              "      <td>2.09</td>\n",
              "      <td>1.91</td>\n",
              "      <td>1.97</td>\n",
              "      <td>1.83</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.95</td>\n",
              "      <td>1.96</td>\n",
              "      <td>1.96</td>\n",
              "      <td>1.96</td>\n",
              "      <td>2.02</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24056</th>\n",
              "      <td>SC0</td>\n",
              "      <td>07/03/2020</td>\n",
              "      <td>St Johnstone</td>\n",
              "      <td>Livingston</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>H</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>K Clancy</td>\n",
              "      <td>3.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.75</td>\n",
              "      <td>2.60</td>\n",
              "      <td>3.05</td>\n",
              "      <td>2.75</td>\n",
              "      <td>2.67</td>\n",
              "      <td>3.37</td>\n",
              "      <td>2.76</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.75</td>\n",
              "      <td>2.60</td>\n",
              "      <td>3.20</td>\n",
              "      <td>...</td>\n",
              "      <td>1.85</td>\n",
              "      <td>1.98</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.75</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.85</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.1</td>\n",
              "      <td>2.88</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.2</td>\n",
              "      <td>2.88</td>\n",
              "      <td>2.77</td>\n",
              "      <td>3.43</td>\n",
              "      <td>2.98</td>\n",
              "      <td>2.56</td>\n",
              "      <td>3.13</td>\n",
              "      <td>2.82</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1.53</td>\n",
              "      <td>2.53</td>\n",
              "      <td>1.57</td>\n",
              "      <td>2.60</td>\n",
              "      <td>1.64</td>\n",
              "      <td>2.41</td>\n",
              "      <td>1.56</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.83</td>\n",
              "      <td>2.02</td>\n",
              "      <td>1.85</td>\n",
              "      <td>2.07</td>\n",
              "      <td>1.91</td>\n",
              "      <td>2.09</td>\n",
              "      <td>1.82</td>\n",
              "      <td>2.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24057</th>\n",
              "      <td>SC0</td>\n",
              "      <td>08/03/2020</td>\n",
              "      <td>Ross County</td>\n",
              "      <td>Rangers</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>A Dallas</td>\n",
              "      <td>3.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.00</td>\n",
              "      <td>7.00</td>\n",
              "      <td>1.20</td>\n",
              "      <td>10.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>1.25</td>\n",
              "      <td>10.00</td>\n",
              "      <td>5.60</td>\n",
              "      <td>1.25</td>\n",
              "      <td>11.11</td>\n",
              "      <td>6.17</td>\n",
              "      <td>1.26</td>\n",
              "      <td>11.00</td>\n",
              "      <td>5.80</td>\n",
              "      <td>1.25</td>\n",
              "      <td>10.50</td>\n",
              "      <td>5.75</td>\n",
              "      <td>...</td>\n",
              "      <td>1.86</td>\n",
              "      <td>1.94</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.50</td>\n",
              "      <td>1.22</td>\n",
              "      <td>10.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>1.25</td>\n",
              "      <td>6.90</td>\n",
              "      <td>5.40</td>\n",
              "      <td>1.33</td>\n",
              "      <td>12.00</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1.24</td>\n",
              "      <td>11.50</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>12.00</td>\n",
              "      <td>6.80</td>\n",
              "      <td>1.33</td>\n",
              "      <td>10.47</td>\n",
              "      <td>5.97</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.53</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1.54</td>\n",
              "      <td>2.58</td>\n",
              "      <td>1.59</td>\n",
              "      <td>2.63</td>\n",
              "      <td>1.52</td>\n",
              "      <td>2.49</td>\n",
              "      <td>1.75</td>\n",
              "      <td>1.88</td>\n",
              "      <td>1.98</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.98</td>\n",
              "      <td>1.96</td>\n",
              "      <td>2.05</td>\n",
              "      <td>1.87</td>\n",
              "      <td>1.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24058</th>\n",
              "      <td>SC0</td>\n",
              "      <td>11/03/2020</td>\n",
              "      <td>St Mirren</td>\n",
              "      <td>Hearts</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>H</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>A Muir</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.30</td>\n",
              "      <td>2.90</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.40</td>\n",
              "      <td>2.95</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.40</td>\n",
              "      <td>3.02</td>\n",
              "      <td>3.38</td>\n",
              "      <td>2.46</td>\n",
              "      <td>2.90</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.40</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.30</td>\n",
              "      <td>...</td>\n",
              "      <td>1.76</td>\n",
              "      <td>2.06</td>\n",
              "      <td>3.3</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.20</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.25</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.3</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.3</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.46</td>\n",
              "      <td>3.44</td>\n",
              "      <td>2.33</td>\n",
              "      <td>3.26</td>\n",
              "      <td>3.23</td>\n",
              "      <td>2.23</td>\n",
              "      <td>2.20</td>\n",
              "      <td>1.66</td>\n",
              "      <td>2.23</td>\n",
              "      <td>1.72</td>\n",
              "      <td>2.30</td>\n",
              "      <td>1.74</td>\n",
              "      <td>2.17</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.95</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.98</td>\n",
              "      <td>1.94</td>\n",
              "      <td>2.01</td>\n",
              "      <td>1.96</td>\n",
              "      <td>1.91</td>\n",
              "      <td>1.90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>21931 rows Ã— 139 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Div        Date      HomeTeam  ... MaxCAHA  AvgCAHH  AvgCAHA\n",
              "0       E0  10/08/2018    Man United  ...     NaN      NaN      NaN\n",
              "1       E0  11/08/2018   Bournemouth  ...     NaN      NaN      NaN\n",
              "2       E0  11/08/2018        Fulham  ...     NaN      NaN      NaN\n",
              "3       E0  11/08/2018  Huddersfield  ...     NaN      NaN      NaN\n",
              "4       E0  11/08/2018     Newcastle  ...     NaN      NaN      NaN\n",
              "...    ...         ...           ...  ...     ...      ...      ...\n",
              "24054  SC0  07/03/2020      Hamilton  ...    1.85     2.05     1.78\n",
              "24055  SC0  07/03/2020        Hearts  ...    2.02     1.90     1.92\n",
              "24056  SC0  07/03/2020  St Johnstone  ...    2.09     1.82     2.01\n",
              "24057  SC0  08/03/2020   Ross County  ...    2.05     1.87     1.95\n",
              "24058  SC0  11/03/2020     St Mirren  ...    1.96     1.91     1.90\n",
              "\n",
              "[21931 rows x 139 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fciTKgZYQ8ZA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "ede73459-c414-4758-f8de-e0ca99926a06"
      },
      "source": [
        "# Separate into feature set and target variable\n",
        "X_data = data.drop(['FTR'],1)\n",
        "y_data = data['FTR']\n",
        "y_data = pd.DataFrame(y_data)\n",
        "y_data.replace({\"FTR\":{'H':1,'A':2,'D':0}},inplace=True)     #replace the strings with integer\n",
        "\n",
        "# Z_data = X_data.drop(['Date','FTHG','FTAG','HTHG','HTAG','HTR','Referee'],1)\n",
        "# Z_data = X_data.drop(['Date','HTR','Referee'],1)\n",
        "Z_data = data[['HS','AS','HST','AST','HF','AF','HY','AY','HR','AR','HC','AC','HTR']]\n",
        "pred_data = tester[['HS','AS','HST','AST','HF','AF','HY','AY','HR','AR','HC','AC','HTR']]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(X_data)\n",
        "# print(y_data)\n",
        "# print((Z_data))\n",
        "\n",
        "print(Z_data.isnull().sum())\n",
        "Z_data['HS'].head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HS     0\n",
            "AS     0\n",
            "HST    0\n",
            "AST    0\n",
            "HF     0\n",
            "AF     0\n",
            "HY     0\n",
            "AY     0\n",
            "HR     0\n",
            "AR     0\n",
            "HC     0\n",
            "AC     0\n",
            "HTR    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     8.0\n",
              "1    12.0\n",
              "2    15.0\n",
              "3     6.0\n",
              "4    15.0\n",
              "Name: HS, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhojh0zIQ8ZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(type(y_data))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubczFizsQ8ZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "5c34e6e0-df68-4137-ce68-c3700fbcb48b"
      },
      "source": [
        "#Standardising the data.\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "Z_data = scaler.fit_transform(Z_data)\n",
        "Z_data = pd.DataFrame(Z_data,columns=['HS','AS','HST','AST','HF','AF','HY','AY','HR','AR','HC','AC','HTR'])\n",
        "\n",
        "print((Z_data))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             HS        AS       HST   AST  ...   AR        HC        AC  HTR\n",
            "0      0.186047  0.371429  0.250000  0.20  ...  0.0  0.090909  0.238095  0.5\n",
            "1      0.279070  0.285714  0.166667  0.05  ...  0.0  0.318182  0.190476  0.5\n",
            "2      0.348837  0.285714  0.250000  0.45  ...  0.0  0.227273  0.238095  1.0\n",
            "3      0.139535  0.371429  0.041667  0.20  ...  0.0  0.090909  0.238095  1.0\n",
            "4      0.348837  0.428571  0.083333  0.25  ...  0.0  0.136364  0.238095  1.0\n",
            "...         ...       ...       ...   ...  ...  ...       ...       ...  ...\n",
            "21926  0.255814  0.400000  0.083333  0.25  ...  0.0  0.272727  0.428571  0.0\n",
            "21927  0.279070  0.314286  0.083333  0.25  ...  0.0  0.318182  0.238095  1.0\n",
            "21928  0.069767  0.257143  0.125000  0.20  ...  0.0  0.227273  0.333333  0.0\n",
            "21929  0.069767  0.342857  0.083333  0.35  ...  0.0  0.227273  0.714286  0.0\n",
            "21930  0.186047  0.142857  0.125000  0.05  ...  0.0  0.227273  0.142857  0.0\n",
            "\n",
            "[21931 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvVkeWq6Q8Z4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "5fd5e90a-346b-49b2-ad88-97b0fc980051"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Shuffle and split the dataset into training and testing set.\n",
        "X_train, X_test, y_train, y_test = train_test_split(Z_data, y_data, \n",
        "                                                    test_size = 0.3,\n",
        "                                                    random_state = 3,\n",
        "                                                    shuffle=True, #shuffle is true by default\n",
        "                                                    stratify = y_data)  #to bring the output format in y_data format\n",
        "print(X_train)\n",
        "print(y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             HS        AS       HST   AST  ...        AR        HC        AC  HTR\n",
            "19937  0.162791  0.400000  0.166667  0.30  ...  0.000000  0.090909  0.333333  0.0\n",
            "17939  0.279070  0.371429  0.208333  0.40  ...  0.000000  0.500000  0.142857  1.0\n",
            "816    0.255814  0.571429  0.166667  0.20  ...  0.000000  0.136364  0.238095  0.5\n",
            "5854   0.209302  0.285714  0.166667  0.15  ...  0.000000  0.181818  0.190476  0.5\n",
            "14006  0.279070  0.114286  0.125000  0.15  ...  0.000000  0.500000  0.000000  1.0\n",
            "...         ...       ...       ...   ...  ...       ...       ...       ...  ...\n",
            "14397  0.279070  0.228571  0.208333  0.20  ...  0.000000  0.409091  0.238095  1.0\n",
            "6754   0.325581  0.628571  0.208333  0.10  ...  0.333333  0.227273  0.238095  0.5\n",
            "21366  0.372093  0.200000  0.500000  0.20  ...  0.000000  0.272727  0.142857  1.0\n",
            "16011  0.302326  0.628571  0.250000  0.40  ...  0.000000  0.181818  0.523810  1.0\n",
            "18610  0.465116  0.142857  0.291667  0.15  ...  0.000000  0.227273  0.000000  0.5\n",
            "\n",
            "[15351 rows x 13 columns]\n",
            "       FTR\n",
            "16648    0\n",
            "8952     1\n",
            "8190     1\n",
            "23673    0\n",
            "2572     2\n",
            "...    ...\n",
            "4932     1\n",
            "11207    2\n",
            "18557    2\n",
            "673      1\n",
            "2280     2\n",
            "\n",
            "[6580 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyMMaq0pQ8Z8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "6be2e09a-0556-4a86-f383-4a724d947ef8"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "display((y_train))\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzfRRBdVmNMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "X_train,y_train=shuffle(X_train,y_train)\n",
        "X_test,y_test=shuffle(X_test,y_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq18PIY_VOJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout\n",
        "from keras.optimizers import SGD,Adagrad\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBj5fIXbQ8aA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  input_shape= int(X_train.shape[1])\n",
        "  global model\n",
        "  model = Sequential([\n",
        "      Dense(13,input_shape=(input_shape,)),\n",
        "      Dense(52,activation='relu'),\n",
        "      Dense(156,activation='relu'),\n",
        "      Dense(468,activation='relu'),\n",
        "      # Dropout(0.5),\n",
        "      Dense(156,activation='relu'),\n",
        "      Dense(78,activation='relu'),\n",
        "      Dense(27,activation='relu'),\n",
        "      # Dropout(0.4),\n",
        "      Dense(3,activation='softmax'),\n",
        "  ])\n",
        "  # model.compile(optimizer=Adagrad(lr=0.001),loss='mean_squared_error',metrics=['accuracy'])\n",
        "\n",
        "  model.compile(optimizer=Adagrad(lr=0.0008,decay=6e-8,clipvalue=1.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "\n",
        "  ###training the model\n",
        "  global History\n",
        "  History=model.fit(x=X_train,y=y_train,batch_size=65,epochs=600,\n",
        "                    # validation_split=0.2,\n",
        "                    # callbacks=[EarlyStopping(monitor='val_loss',patience=0.001)],\n",
        "                    validation_data=(X_test,y_test),\n",
        "          )\n",
        "\n",
        "\n",
        "  model.evaluate(x=X_test,y=y_test,batch_size=90)\n",
        "\n",
        "  # model.save('gdrive/My Drive/Colab Notebooks/epl/model.h5')\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGcrw0hIffvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model1():\n",
        "  input_shape= int(X_train.shape[1])\n",
        "  global model_1\n",
        "  model_1 = Sequential([\n",
        "      Dense(13,input_shape=(input_shape,)),\n",
        "      Dense(50,activation='relu'),\n",
        "      Dense(50,activation='relu'),\n",
        "      Dense(50,activation='relu'),\n",
        "      # Dropout(0.5),\n",
        "      Dense(50,activation='relu'),\n",
        "      Dense(50,activation='relu'),\n",
        "      Dense(50,activation='relu'),\n",
        "      # Dropout(0.4),\n",
        "      Dense(3,activation='softmax'),\n",
        "  ])\n",
        "  # model.compile(optimizer=Adagrad(lr=0.001),loss='mean_squared_error',metrics=['accuracy'])\n",
        "\n",
        "  model_1.compile(optimizer=Adagrad(lr=0.0008,decay=6e-8,clipvalue=1.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  model_1.summary()\n",
        "\n",
        "\n",
        "  ###training the model\n",
        "  global History_1\n",
        "  History_1=model_1.fit(x=X_train,y=y_train,batch_size=65,epochs=600,\n",
        "                    # validation_split=0.2,\n",
        "                    # callbacks=[EarlyStopping(monitor='val_loss',patience=0.001)],\n",
        "                    validation_data=(X_test,y_test),\n",
        "          )\n",
        "\n",
        "\n",
        "  model_1.evaluate(x=X_test,y=y_test,batch_size=90)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UvZ0OEJfh2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model2():\n",
        "  input_shape= int(X_train.shape[1])\n",
        "  global model_2\n",
        "  model_2 = Sequential([\n",
        "      Dense(13,input_shape=(input_shape,)),\n",
        "      Dense(460,activation='relu'),\n",
        "      Dense(370,activation='relu'),\n",
        "      Dense(280,activation='relu'),\n",
        "      # Dropout(0.5),\n",
        "      Dense(190,activation='relu'),\n",
        "      Dense(100,activation='relu'),\n",
        "      Dense(10,activation='relu'),\n",
        "\n",
        "      Dense(100,activation='relu'),\n",
        "      Dense(190,activation='relu'),\n",
        "      Dense(280,activation='relu'),\n",
        "      Dense(370,activation='relu'),\n",
        "      Dense(460,activation='relu'),\n",
        "      # Dropout(0.4),\n",
        "      Dense(3,activation='softmax'),\n",
        "  ])\n",
        "  # model.compile(optimizer=Adagrad(lr=0.001),loss='mean_squared_error',metrics=['accuracy'])\n",
        "\n",
        "  model_2.compile(optimizer=Adagrad(lr=0.0008,decay=6e-8,clipvalue=1.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  model_2.summary()\n",
        "\n",
        "\n",
        "  ###training the model\n",
        "  global History_2\n",
        "  History_2=model_2.fit(x=X_train,y=y_train,batch_size=65,epochs=600,\n",
        "                    # validation_split=0.2,\n",
        "                    # callbacks=[EarlyStopping(monitor='val_loss',patience=0.001)],\n",
        "                    validation_data=(X_test,y_test),\n",
        "          )\n",
        "\n",
        "\n",
        "  model_2.evaluate(x=X_test,y=y_test,batch_size=90)\n",
        "\n",
        "  # model_2.save('gdrive/My Drive/Colab Notebooks/epl/model_2.h5')\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ggcktZYVh3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_stats(model_history):\n",
        "  history=model_history\n",
        "  ###training stats\n",
        "  import matplotlib.pyplot as plt\n",
        "  \n",
        "  plt.plot(history.history['accuracy'],label='accuracy')\n",
        "  plt.plot(history.history['val_accuracy'],label='val_accuracy')\n",
        "  plt.plot(history.history['loss'],label='loss')\n",
        "  plt.plot(history.history['val_loss'],label='val_loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy and Loss')\n",
        "  #plt.ylim([0.5,3])\n",
        "  plt.legend(loc='best')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFfhjLeYY03p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "caea6366-c5ef-44d8-ced2-3ee434b333d9"
      },
      "source": [
        "create_model()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 13)                182       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 52)                728       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 156)               8268      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 468)               73476     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 156)               73164     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 78)                12246     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 27)                2133      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 84        \n",
            "=================================================================\n",
            "Total params: 170,281\n",
            "Trainable params: 170,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 1.0852 - accuracy: 0.4307 - val_loss: 1.0791 - val_accuracy: 0.4307\n",
            "Epoch 2/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0753 - accuracy: 0.4307 - val_loss: 1.0716 - val_accuracy: 0.4307\n",
            "Epoch 3/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0691 - accuracy: 0.4307 - val_loss: 1.0661 - val_accuracy: 0.4307\n",
            "Epoch 4/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0636 - accuracy: 0.4307 - val_loss: 1.0603 - val_accuracy: 0.4307\n",
            "Epoch 5/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0580 - accuracy: 0.4307 - val_loss: 1.0546 - val_accuracy: 0.4307\n",
            "Epoch 6/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0522 - accuracy: 0.4307 - val_loss: 1.0483 - val_accuracy: 0.4309\n",
            "Epoch 7/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0456 - accuracy: 0.4467 - val_loss: 1.0412 - val_accuracy: 0.4742\n",
            "Epoch 8/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0381 - accuracy: 0.4961 - val_loss: 1.0326 - val_accuracy: 0.5264\n",
            "Epoch 9/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0287 - accuracy: 0.5362 - val_loss: 1.0226 - val_accuracy: 0.5445\n",
            "Epoch 10/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0185 - accuracy: 0.5526 - val_loss: 1.0117 - val_accuracy: 0.5550\n",
            "Epoch 11/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0071 - accuracy: 0.5622 - val_loss: 0.9996 - val_accuracy: 0.5663\n",
            "Epoch 12/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9950 - accuracy: 0.5670 - val_loss: 0.9872 - val_accuracy: 0.5679\n",
            "Epoch 13/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9830 - accuracy: 0.5695 - val_loss: 0.9751 - val_accuracy: 0.5723\n",
            "Epoch 14/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9715 - accuracy: 0.5712 - val_loss: 0.9640 - val_accuracy: 0.5736\n",
            "Epoch 15/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9612 - accuracy: 0.5723 - val_loss: 0.9543 - val_accuracy: 0.5745\n",
            "Epoch 16/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9523 - accuracy: 0.5733 - val_loss: 0.9461 - val_accuracy: 0.5758\n",
            "Epoch 17/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9447 - accuracy: 0.5738 - val_loss: 0.9389 - val_accuracy: 0.5751\n",
            "Epoch 18/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9382 - accuracy: 0.5742 - val_loss: 0.9329 - val_accuracy: 0.5757\n",
            "Epoch 19/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9324 - accuracy: 0.5745 - val_loss: 0.9277 - val_accuracy: 0.5767\n",
            "Epoch 20/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9269 - accuracy: 0.5748 - val_loss: 0.9222 - val_accuracy: 0.5766\n",
            "Epoch 21/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9219 - accuracy: 0.5751 - val_loss: 0.9171 - val_accuracy: 0.5774\n",
            "Epoch 22/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9171 - accuracy: 0.5749 - val_loss: 0.9125 - val_accuracy: 0.5775\n",
            "Epoch 23/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9125 - accuracy: 0.5753 - val_loss: 0.9081 - val_accuracy: 0.5771\n",
            "Epoch 24/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9082 - accuracy: 0.5761 - val_loss: 0.9037 - val_accuracy: 0.5771\n",
            "Epoch 25/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9038 - accuracy: 0.5755 - val_loss: 0.8994 - val_accuracy: 0.5774\n",
            "Epoch 26/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8994 - accuracy: 0.5759 - val_loss: 0.8951 - val_accuracy: 0.5777\n",
            "Epoch 27/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8950 - accuracy: 0.5761 - val_loss: 0.8910 - val_accuracy: 0.5774\n",
            "Epoch 28/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8909 - accuracy: 0.5757 - val_loss: 0.8870 - val_accuracy: 0.5795\n",
            "Epoch 29/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8867 - accuracy: 0.5777 - val_loss: 0.8834 - val_accuracy: 0.5828\n",
            "Epoch 30/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8828 - accuracy: 0.5820 - val_loss: 0.8793 - val_accuracy: 0.5854\n",
            "Epoch 31/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8790 - accuracy: 0.5857 - val_loss: 0.8758 - val_accuracy: 0.5892\n",
            "Epoch 32/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8755 - accuracy: 0.5875 - val_loss: 0.8724 - val_accuracy: 0.5912\n",
            "Epoch 33/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8718 - accuracy: 0.5921 - val_loss: 0.8700 - val_accuracy: 0.5933\n",
            "Epoch 34/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8689 - accuracy: 0.5922 - val_loss: 0.8659 - val_accuracy: 0.5967\n",
            "Epoch 35/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8656 - accuracy: 0.5931 - val_loss: 0.8634 - val_accuracy: 0.5997\n",
            "Epoch 36/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8628 - accuracy: 0.5939 - val_loss: 0.8609 - val_accuracy: 0.6009\n",
            "Epoch 37/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8602 - accuracy: 0.5955 - val_loss: 0.8582 - val_accuracy: 0.6018\n",
            "Epoch 38/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8578 - accuracy: 0.5973 - val_loss: 0.8566 - val_accuracy: 0.6064\n",
            "Epoch 39/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8555 - accuracy: 0.5981 - val_loss: 0.8539 - val_accuracy: 0.6050\n",
            "Epoch 40/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8533 - accuracy: 0.5990 - val_loss: 0.8522 - val_accuracy: 0.6084\n",
            "Epoch 41/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8513 - accuracy: 0.5997 - val_loss: 0.8502 - val_accuracy: 0.6082\n",
            "Epoch 42/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8495 - accuracy: 0.6020 - val_loss: 0.8482 - val_accuracy: 0.6090\n",
            "Epoch 43/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8476 - accuracy: 0.6015 - val_loss: 0.8469 - val_accuracy: 0.6100\n",
            "Epoch 44/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8458 - accuracy: 0.6021 - val_loss: 0.8455 - val_accuracy: 0.6106\n",
            "Epoch 45/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8442 - accuracy: 0.6024 - val_loss: 0.8439 - val_accuracy: 0.6106\n",
            "Epoch 46/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8426 - accuracy: 0.6025 - val_loss: 0.8427 - val_accuracy: 0.6128\n",
            "Epoch 47/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8411 - accuracy: 0.6038 - val_loss: 0.8408 - val_accuracy: 0.6109\n",
            "Epoch 48/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8397 - accuracy: 0.6053 - val_loss: 0.8397 - val_accuracy: 0.6108\n",
            "Epoch 49/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8384 - accuracy: 0.6054 - val_loss: 0.8383 - val_accuracy: 0.6120\n",
            "Epoch 50/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8370 - accuracy: 0.6056 - val_loss: 0.8371 - val_accuracy: 0.6141\n",
            "Epoch 51/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8357 - accuracy: 0.6069 - val_loss: 0.8366 - val_accuracy: 0.6132\n",
            "Epoch 52/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8344 - accuracy: 0.6075 - val_loss: 0.8354 - val_accuracy: 0.6122\n",
            "Epoch 53/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8334 - accuracy: 0.6080 - val_loss: 0.8338 - val_accuracy: 0.6150\n",
            "Epoch 54/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8322 - accuracy: 0.6086 - val_loss: 0.8328 - val_accuracy: 0.6149\n",
            "Epoch 55/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8312 - accuracy: 0.6079 - val_loss: 0.8319 - val_accuracy: 0.6167\n",
            "Epoch 56/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8301 - accuracy: 0.6097 - val_loss: 0.8308 - val_accuracy: 0.6161\n",
            "Epoch 57/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8290 - accuracy: 0.6099 - val_loss: 0.8300 - val_accuracy: 0.6161\n",
            "Epoch 58/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8280 - accuracy: 0.6106 - val_loss: 0.8288 - val_accuracy: 0.6170\n",
            "Epoch 59/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8267 - accuracy: 0.6117 - val_loss: 0.8278 - val_accuracy: 0.6182\n",
            "Epoch 60/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8258 - accuracy: 0.6107 - val_loss: 0.8274 - val_accuracy: 0.6202\n",
            "Epoch 61/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8248 - accuracy: 0.6127 - val_loss: 0.8262 - val_accuracy: 0.6201\n",
            "Epoch 62/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8240 - accuracy: 0.6117 - val_loss: 0.8253 - val_accuracy: 0.6211\n",
            "Epoch 63/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8232 - accuracy: 0.6131 - val_loss: 0.8245 - val_accuracy: 0.6216\n",
            "Epoch 64/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8223 - accuracy: 0.6144 - val_loss: 0.8240 - val_accuracy: 0.6225\n",
            "Epoch 65/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8217 - accuracy: 0.6155 - val_loss: 0.8232 - val_accuracy: 0.6226\n",
            "Epoch 66/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8209 - accuracy: 0.6153 - val_loss: 0.8228 - val_accuracy: 0.6222\n",
            "Epoch 67/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8203 - accuracy: 0.6162 - val_loss: 0.8228 - val_accuracy: 0.6187\n",
            "Epoch 68/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8197 - accuracy: 0.6144 - val_loss: 0.8214 - val_accuracy: 0.6222\n",
            "Epoch 69/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8190 - accuracy: 0.6168 - val_loss: 0.8211 - val_accuracy: 0.6237\n",
            "Epoch 70/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8183 - accuracy: 0.6166 - val_loss: 0.8204 - val_accuracy: 0.6277\n",
            "Epoch 71/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8177 - accuracy: 0.6175 - val_loss: 0.8199 - val_accuracy: 0.6280\n",
            "Epoch 72/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8173 - accuracy: 0.6176 - val_loss: 0.8195 - val_accuracy: 0.6287\n",
            "Epoch 73/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.8166 - accuracy: 0.6182 - val_loss: 0.8196 - val_accuracy: 0.6289\n",
            "Epoch 74/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.8162 - accuracy: 0.6186 - val_loss: 0.8185 - val_accuracy: 0.6292\n",
            "Epoch 75/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.8157 - accuracy: 0.6194 - val_loss: 0.8180 - val_accuracy: 0.6269\n",
            "Epoch 76/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8151 - accuracy: 0.6185 - val_loss: 0.8177 - val_accuracy: 0.6271\n",
            "Epoch 77/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8148 - accuracy: 0.6190 - val_loss: 0.8174 - val_accuracy: 0.6264\n",
            "Epoch 78/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8144 - accuracy: 0.6205 - val_loss: 0.8172 - val_accuracy: 0.6258\n",
            "Epoch 79/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8139 - accuracy: 0.6196 - val_loss: 0.8169 - val_accuracy: 0.6278\n",
            "Epoch 80/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8136 - accuracy: 0.6201 - val_loss: 0.8162 - val_accuracy: 0.6295\n",
            "Epoch 81/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8132 - accuracy: 0.6191 - val_loss: 0.8161 - val_accuracy: 0.6290\n",
            "Epoch 82/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8128 - accuracy: 0.6199 - val_loss: 0.8163 - val_accuracy: 0.6266\n",
            "Epoch 83/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8123 - accuracy: 0.6207 - val_loss: 0.8151 - val_accuracy: 0.6290\n",
            "Epoch 84/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8121 - accuracy: 0.6197 - val_loss: 0.8148 - val_accuracy: 0.6295\n",
            "Epoch 85/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8118 - accuracy: 0.6202 - val_loss: 0.8144 - val_accuracy: 0.6296\n",
            "Epoch 86/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8115 - accuracy: 0.6201 - val_loss: 0.8142 - val_accuracy: 0.6292\n",
            "Epoch 87/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8111 - accuracy: 0.6208 - val_loss: 0.8139 - val_accuracy: 0.6296\n",
            "Epoch 88/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8108 - accuracy: 0.6200 - val_loss: 0.8137 - val_accuracy: 0.6290\n",
            "Epoch 89/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8105 - accuracy: 0.6206 - val_loss: 0.8134 - val_accuracy: 0.6277\n",
            "Epoch 90/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8102 - accuracy: 0.6208 - val_loss: 0.8133 - val_accuracy: 0.6298\n",
            "Epoch 91/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8100 - accuracy: 0.6211 - val_loss: 0.8128 - val_accuracy: 0.6298\n",
            "Epoch 92/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8096 - accuracy: 0.6211 - val_loss: 0.8126 - val_accuracy: 0.6292\n",
            "Epoch 93/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8094 - accuracy: 0.6218 - val_loss: 0.8126 - val_accuracy: 0.6307\n",
            "Epoch 94/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8093 - accuracy: 0.6211 - val_loss: 0.8123 - val_accuracy: 0.6284\n",
            "Epoch 95/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8090 - accuracy: 0.6223 - val_loss: 0.8119 - val_accuracy: 0.6289\n",
            "Epoch 96/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8086 - accuracy: 0.6214 - val_loss: 0.8121 - val_accuracy: 0.6281\n",
            "Epoch 97/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8085 - accuracy: 0.6224 - val_loss: 0.8119 - val_accuracy: 0.6283\n",
            "Epoch 98/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8083 - accuracy: 0.6222 - val_loss: 0.8113 - val_accuracy: 0.6296\n",
            "Epoch 99/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8080 - accuracy: 0.6224 - val_loss: 0.8116 - val_accuracy: 0.6293\n",
            "Epoch 100/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8078 - accuracy: 0.6218 - val_loss: 0.8110 - val_accuracy: 0.6293\n",
            "Epoch 101/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8076 - accuracy: 0.6220 - val_loss: 0.8109 - val_accuracy: 0.6298\n",
            "Epoch 102/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8073 - accuracy: 0.6224 - val_loss: 0.8108 - val_accuracy: 0.6289\n",
            "Epoch 103/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8074 - accuracy: 0.6213 - val_loss: 0.8105 - val_accuracy: 0.6301\n",
            "Epoch 104/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8072 - accuracy: 0.6208 - val_loss: 0.8102 - val_accuracy: 0.6298\n",
            "Epoch 105/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8068 - accuracy: 0.6222 - val_loss: 0.8101 - val_accuracy: 0.6305\n",
            "Epoch 106/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8068 - accuracy: 0.6225 - val_loss: 0.8099 - val_accuracy: 0.6304\n",
            "Epoch 107/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8065 - accuracy: 0.6228 - val_loss: 0.8098 - val_accuracy: 0.6309\n",
            "Epoch 108/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8064 - accuracy: 0.6225 - val_loss: 0.8097 - val_accuracy: 0.6299\n",
            "Epoch 109/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8061 - accuracy: 0.6235 - val_loss: 0.8099 - val_accuracy: 0.6298\n",
            "Epoch 110/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8060 - accuracy: 0.6232 - val_loss: 0.8094 - val_accuracy: 0.6302\n",
            "Epoch 111/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8059 - accuracy: 0.6230 - val_loss: 0.8092 - val_accuracy: 0.6309\n",
            "Epoch 112/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8056 - accuracy: 0.6229 - val_loss: 0.8092 - val_accuracy: 0.6286\n",
            "Epoch 113/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8056 - accuracy: 0.6236 - val_loss: 0.8090 - val_accuracy: 0.6299\n",
            "Epoch 114/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8054 - accuracy: 0.6237 - val_loss: 0.8088 - val_accuracy: 0.6305\n",
            "Epoch 115/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8054 - accuracy: 0.6233 - val_loss: 0.8087 - val_accuracy: 0.6310\n",
            "Epoch 116/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8050 - accuracy: 0.6236 - val_loss: 0.8085 - val_accuracy: 0.6310\n",
            "Epoch 117/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8049 - accuracy: 0.6241 - val_loss: 0.8083 - val_accuracy: 0.6309\n",
            "Epoch 118/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8048 - accuracy: 0.6228 - val_loss: 0.8083 - val_accuracy: 0.6310\n",
            "Epoch 119/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8046 - accuracy: 0.6240 - val_loss: 0.8084 - val_accuracy: 0.6319\n",
            "Epoch 120/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8045 - accuracy: 0.6230 - val_loss: 0.8081 - val_accuracy: 0.6298\n",
            "Epoch 121/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8043 - accuracy: 0.6241 - val_loss: 0.8080 - val_accuracy: 0.6319\n",
            "Epoch 122/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8042 - accuracy: 0.6235 - val_loss: 0.8080 - val_accuracy: 0.6293\n",
            "Epoch 123/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8040 - accuracy: 0.6242 - val_loss: 0.8084 - val_accuracy: 0.6301\n",
            "Epoch 124/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8040 - accuracy: 0.6244 - val_loss: 0.8075 - val_accuracy: 0.6312\n",
            "Epoch 125/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8039 - accuracy: 0.6246 - val_loss: 0.8074 - val_accuracy: 0.6313\n",
            "Epoch 126/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8038 - accuracy: 0.6243 - val_loss: 0.8074 - val_accuracy: 0.6304\n",
            "Epoch 127/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8036 - accuracy: 0.6245 - val_loss: 0.8075 - val_accuracy: 0.6309\n",
            "Epoch 128/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8034 - accuracy: 0.6252 - val_loss: 0.8073 - val_accuracy: 0.6293\n",
            "Epoch 129/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8033 - accuracy: 0.6241 - val_loss: 0.8082 - val_accuracy: 0.6293\n",
            "Epoch 130/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8033 - accuracy: 0.6246 - val_loss: 0.8069 - val_accuracy: 0.6309\n",
            "Epoch 131/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8031 - accuracy: 0.6239 - val_loss: 0.8068 - val_accuracy: 0.6305\n",
            "Epoch 132/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8029 - accuracy: 0.6243 - val_loss: 0.8068 - val_accuracy: 0.6307\n",
            "Epoch 133/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8030 - accuracy: 0.6250 - val_loss: 0.8066 - val_accuracy: 0.6309\n",
            "Epoch 134/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8028 - accuracy: 0.6251 - val_loss: 0.8065 - val_accuracy: 0.6310\n",
            "Epoch 135/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8026 - accuracy: 0.6245 - val_loss: 0.8064 - val_accuracy: 0.6315\n",
            "Epoch 136/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8025 - accuracy: 0.6241 - val_loss: 0.8063 - val_accuracy: 0.6307\n",
            "Epoch 137/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8025 - accuracy: 0.6246 - val_loss: 0.8062 - val_accuracy: 0.6299\n",
            "Epoch 138/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8024 - accuracy: 0.6252 - val_loss: 0.8061 - val_accuracy: 0.6305\n",
            "Epoch 139/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8021 - accuracy: 0.6258 - val_loss: 0.8062 - val_accuracy: 0.6298\n",
            "Epoch 140/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8020 - accuracy: 0.6252 - val_loss: 0.8061 - val_accuracy: 0.6313\n",
            "Epoch 141/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8019 - accuracy: 0.6248 - val_loss: 0.8061 - val_accuracy: 0.6312\n",
            "Epoch 142/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8018 - accuracy: 0.6258 - val_loss: 0.8066 - val_accuracy: 0.6312\n",
            "Epoch 143/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.8017 - accuracy: 0.6264 - val_loss: 0.8059 - val_accuracy: 0.6310\n",
            "Epoch 144/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.8016 - accuracy: 0.6260 - val_loss: 0.8059 - val_accuracy: 0.6313\n",
            "Epoch 145/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.8016 - accuracy: 0.6263 - val_loss: 0.8056 - val_accuracy: 0.6309\n",
            "Epoch 146/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.8014 - accuracy: 0.6267 - val_loss: 0.8057 - val_accuracy: 0.6313\n",
            "Epoch 147/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.8014 - accuracy: 0.6265 - val_loss: 0.8055 - val_accuracy: 0.6312\n",
            "Epoch 148/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.8013 - accuracy: 0.6269 - val_loss: 0.8052 - val_accuracy: 0.6318\n",
            "Epoch 149/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8011 - accuracy: 0.6273 - val_loss: 0.8052 - val_accuracy: 0.6315\n",
            "Epoch 150/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8011 - accuracy: 0.6266 - val_loss: 0.8051 - val_accuracy: 0.6316\n",
            "Epoch 151/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8010 - accuracy: 0.6265 - val_loss: 0.8050 - val_accuracy: 0.6322\n",
            "Epoch 152/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8009 - accuracy: 0.6269 - val_loss: 0.8049 - val_accuracy: 0.6318\n",
            "Epoch 153/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8006 - accuracy: 0.6266 - val_loss: 0.8054 - val_accuracy: 0.6299\n",
            "Epoch 154/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8007 - accuracy: 0.6279 - val_loss: 0.8048 - val_accuracy: 0.6307\n",
            "Epoch 155/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8007 - accuracy: 0.6265 - val_loss: 0.8047 - val_accuracy: 0.6318\n",
            "Epoch 156/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8005 - accuracy: 0.6268 - val_loss: 0.8049 - val_accuracy: 0.6299\n",
            "Epoch 157/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8005 - accuracy: 0.6273 - val_loss: 0.8045 - val_accuracy: 0.6319\n",
            "Epoch 158/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8003 - accuracy: 0.6273 - val_loss: 0.8044 - val_accuracy: 0.6309\n",
            "Epoch 159/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8003 - accuracy: 0.6261 - val_loss: 0.8045 - val_accuracy: 0.6309\n",
            "Epoch 160/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8002 - accuracy: 0.6282 - val_loss: 0.8043 - val_accuracy: 0.6310\n",
            "Epoch 161/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8000 - accuracy: 0.6276 - val_loss: 0.8045 - val_accuracy: 0.6318\n",
            "Epoch 162/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8000 - accuracy: 0.6280 - val_loss: 0.8043 - val_accuracy: 0.6302\n",
            "Epoch 163/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7999 - accuracy: 0.6273 - val_loss: 0.8041 - val_accuracy: 0.6309\n",
            "Epoch 164/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7998 - accuracy: 0.6273 - val_loss: 0.8040 - val_accuracy: 0.6327\n",
            "Epoch 165/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7998 - accuracy: 0.6267 - val_loss: 0.8039 - val_accuracy: 0.6313\n",
            "Epoch 166/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7996 - accuracy: 0.6271 - val_loss: 0.8039 - val_accuracy: 0.6315\n",
            "Epoch 167/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7995 - accuracy: 0.6269 - val_loss: 0.8042 - val_accuracy: 0.6301\n",
            "Epoch 168/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7996 - accuracy: 0.6293 - val_loss: 0.8039 - val_accuracy: 0.6322\n",
            "Epoch 169/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7994 - accuracy: 0.6282 - val_loss: 0.8043 - val_accuracy: 0.6305\n",
            "Epoch 170/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7994 - accuracy: 0.6284 - val_loss: 0.8037 - val_accuracy: 0.6310\n",
            "Epoch 171/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7991 - accuracy: 0.6277 - val_loss: 0.8038 - val_accuracy: 0.6325\n",
            "Epoch 172/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7992 - accuracy: 0.6281 - val_loss: 0.8034 - val_accuracy: 0.6318\n",
            "Epoch 173/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7990 - accuracy: 0.6279 - val_loss: 0.8050 - val_accuracy: 0.6301\n",
            "Epoch 174/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7990 - accuracy: 0.6275 - val_loss: 0.8035 - val_accuracy: 0.6313\n",
            "Epoch 175/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7991 - accuracy: 0.6285 - val_loss: 0.8036 - val_accuracy: 0.6321\n",
            "Epoch 176/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7989 - accuracy: 0.6273 - val_loss: 0.8034 - val_accuracy: 0.6315\n",
            "Epoch 177/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7987 - accuracy: 0.6287 - val_loss: 0.8035 - val_accuracy: 0.6315\n",
            "Epoch 178/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7988 - accuracy: 0.6286 - val_loss: 0.8030 - val_accuracy: 0.6309\n",
            "Epoch 179/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7987 - accuracy: 0.6299 - val_loss: 0.8030 - val_accuracy: 0.6312\n",
            "Epoch 180/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7985 - accuracy: 0.6284 - val_loss: 0.8031 - val_accuracy: 0.6304\n",
            "Epoch 181/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7985 - accuracy: 0.6282 - val_loss: 0.8031 - val_accuracy: 0.6298\n",
            "Epoch 182/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7985 - accuracy: 0.6293 - val_loss: 0.8029 - val_accuracy: 0.6309\n",
            "Epoch 183/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7984 - accuracy: 0.6281 - val_loss: 0.8029 - val_accuracy: 0.6318\n",
            "Epoch 184/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7984 - accuracy: 0.6291 - val_loss: 0.8028 - val_accuracy: 0.6322\n",
            "Epoch 185/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7983 - accuracy: 0.6295 - val_loss: 0.8026 - val_accuracy: 0.6312\n",
            "Epoch 186/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7982 - accuracy: 0.6299 - val_loss: 0.8026 - val_accuracy: 0.6310\n",
            "Epoch 187/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7980 - accuracy: 0.6290 - val_loss: 0.8025 - val_accuracy: 0.6313\n",
            "Epoch 188/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7978 - accuracy: 0.6297 - val_loss: 0.8036 - val_accuracy: 0.6310\n",
            "Epoch 189/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7980 - accuracy: 0.6284 - val_loss: 0.8027 - val_accuracy: 0.6302\n",
            "Epoch 190/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7979 - accuracy: 0.6295 - val_loss: 0.8031 - val_accuracy: 0.6319\n",
            "Epoch 191/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7978 - accuracy: 0.6289 - val_loss: 0.8024 - val_accuracy: 0.6310\n",
            "Epoch 192/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7978 - accuracy: 0.6298 - val_loss: 0.8022 - val_accuracy: 0.6309\n",
            "Epoch 193/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7976 - accuracy: 0.6293 - val_loss: 0.8028 - val_accuracy: 0.6305\n",
            "Epoch 194/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7976 - accuracy: 0.6295 - val_loss: 0.8023 - val_accuracy: 0.6312\n",
            "Epoch 195/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7976 - accuracy: 0.6297 - val_loss: 0.8027 - val_accuracy: 0.6309\n",
            "Epoch 196/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7975 - accuracy: 0.6299 - val_loss: 0.8022 - val_accuracy: 0.6301\n",
            "Epoch 197/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7973 - accuracy: 0.6297 - val_loss: 0.8020 - val_accuracy: 0.6309\n",
            "Epoch 198/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7973 - accuracy: 0.6303 - val_loss: 0.8021 - val_accuracy: 0.6301\n",
            "Epoch 199/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7971 - accuracy: 0.6297 - val_loss: 0.8026 - val_accuracy: 0.6305\n",
            "Epoch 200/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7973 - accuracy: 0.6297 - val_loss: 0.8019 - val_accuracy: 0.6302\n",
            "Epoch 201/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7972 - accuracy: 0.6299 - val_loss: 0.8017 - val_accuracy: 0.6309\n",
            "Epoch 202/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7970 - accuracy: 0.6299 - val_loss: 0.8020 - val_accuracy: 0.6305\n",
            "Epoch 203/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7970 - accuracy: 0.6305 - val_loss: 0.8018 - val_accuracy: 0.6310\n",
            "Epoch 204/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7970 - accuracy: 0.6301 - val_loss: 0.8018 - val_accuracy: 0.6322\n",
            "Epoch 205/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7969 - accuracy: 0.6298 - val_loss: 0.8015 - val_accuracy: 0.6312\n",
            "Epoch 206/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7968 - accuracy: 0.6295 - val_loss: 0.8014 - val_accuracy: 0.6313\n",
            "Epoch 207/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7968 - accuracy: 0.6303 - val_loss: 0.8014 - val_accuracy: 0.6315\n",
            "Epoch 208/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7966 - accuracy: 0.6297 - val_loss: 0.8013 - val_accuracy: 0.6315\n",
            "Epoch 209/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7965 - accuracy: 0.6301 - val_loss: 0.8013 - val_accuracy: 0.6316\n",
            "Epoch 210/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7964 - accuracy: 0.6308 - val_loss: 0.8014 - val_accuracy: 0.6293\n",
            "Epoch 211/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7964 - accuracy: 0.6301 - val_loss: 0.8013 - val_accuracy: 0.6315\n",
            "Epoch 212/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7964 - accuracy: 0.6303 - val_loss: 0.8012 - val_accuracy: 0.6299\n",
            "Epoch 213/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7962 - accuracy: 0.6310 - val_loss: 0.8011 - val_accuracy: 0.6309\n",
            "Epoch 214/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7962 - accuracy: 0.6304 - val_loss: 0.8013 - val_accuracy: 0.6318\n",
            "Epoch 215/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7960 - accuracy: 0.6303 - val_loss: 0.8011 - val_accuracy: 0.6316\n",
            "Epoch 216/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7962 - accuracy: 0.6297 - val_loss: 0.8010 - val_accuracy: 0.6302\n",
            "Epoch 217/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7961 - accuracy: 0.6306 - val_loss: 0.8012 - val_accuracy: 0.6309\n",
            "Epoch 218/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7961 - accuracy: 0.6307 - val_loss: 0.8010 - val_accuracy: 0.6310\n",
            "Epoch 219/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7960 - accuracy: 0.6311 - val_loss: 0.8009 - val_accuracy: 0.6296\n",
            "Epoch 220/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7959 - accuracy: 0.6319 - val_loss: 0.8007 - val_accuracy: 0.6310\n",
            "Epoch 221/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7958 - accuracy: 0.6318 - val_loss: 0.8008 - val_accuracy: 0.6319\n",
            "Epoch 222/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7958 - accuracy: 0.6317 - val_loss: 0.8011 - val_accuracy: 0.6319\n",
            "Epoch 223/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7958 - accuracy: 0.6313 - val_loss: 0.8008 - val_accuracy: 0.6304\n",
            "Epoch 224/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7958 - accuracy: 0.6319 - val_loss: 0.8005 - val_accuracy: 0.6318\n",
            "Epoch 225/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7956 - accuracy: 0.6316 - val_loss: 0.8007 - val_accuracy: 0.6299\n",
            "Epoch 226/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7956 - accuracy: 0.6314 - val_loss: 0.8005 - val_accuracy: 0.6312\n",
            "Epoch 227/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7955 - accuracy: 0.6316 - val_loss: 0.8004 - val_accuracy: 0.6307\n",
            "Epoch 228/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7955 - accuracy: 0.6314 - val_loss: 0.8005 - val_accuracy: 0.6305\n",
            "Epoch 229/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7954 - accuracy: 0.6315 - val_loss: 0.8003 - val_accuracy: 0.6312\n",
            "Epoch 230/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7954 - accuracy: 0.6316 - val_loss: 0.8003 - val_accuracy: 0.6309\n",
            "Epoch 231/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7952 - accuracy: 0.6310 - val_loss: 0.8003 - val_accuracy: 0.6301\n",
            "Epoch 232/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7952 - accuracy: 0.6312 - val_loss: 0.8006 - val_accuracy: 0.6307\n",
            "Epoch 233/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7953 - accuracy: 0.6324 - val_loss: 0.8002 - val_accuracy: 0.6305\n",
            "Epoch 234/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7951 - accuracy: 0.6316 - val_loss: 0.8001 - val_accuracy: 0.6304\n",
            "Epoch 235/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7951 - accuracy: 0.6306 - val_loss: 0.8000 - val_accuracy: 0.6301\n",
            "Epoch 236/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7951 - accuracy: 0.6322 - val_loss: 0.8000 - val_accuracy: 0.6302\n",
            "Epoch 237/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7949 - accuracy: 0.6310 - val_loss: 0.8005 - val_accuracy: 0.6325\n",
            "Epoch 238/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7948 - accuracy: 0.6312 - val_loss: 0.8000 - val_accuracy: 0.6298\n",
            "Epoch 239/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7948 - accuracy: 0.6322 - val_loss: 0.8001 - val_accuracy: 0.6307\n",
            "Epoch 240/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7948 - accuracy: 0.6318 - val_loss: 0.8001 - val_accuracy: 0.6318\n",
            "Epoch 241/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7948 - accuracy: 0.6315 - val_loss: 0.7999 - val_accuracy: 0.6316\n",
            "Epoch 242/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7947 - accuracy: 0.6318 - val_loss: 0.7998 - val_accuracy: 0.6302\n",
            "Epoch 243/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7947 - accuracy: 0.6328 - val_loss: 0.7999 - val_accuracy: 0.6309\n",
            "Epoch 244/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7946 - accuracy: 0.6320 - val_loss: 0.7997 - val_accuracy: 0.6295\n",
            "Epoch 245/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7945 - accuracy: 0.6316 - val_loss: 0.7996 - val_accuracy: 0.6301\n",
            "Epoch 246/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7945 - accuracy: 0.6323 - val_loss: 0.7996 - val_accuracy: 0.6305\n",
            "Epoch 247/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7945 - accuracy: 0.6323 - val_loss: 0.7999 - val_accuracy: 0.6307\n",
            "Epoch 248/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7944 - accuracy: 0.6323 - val_loss: 0.7996 - val_accuracy: 0.6305\n",
            "Epoch 249/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7943 - accuracy: 0.6319 - val_loss: 0.7995 - val_accuracy: 0.6299\n",
            "Epoch 250/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7943 - accuracy: 0.6311 - val_loss: 0.7995 - val_accuracy: 0.6315\n",
            "Epoch 251/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7942 - accuracy: 0.6318 - val_loss: 0.7998 - val_accuracy: 0.6319\n",
            "Epoch 252/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7942 - accuracy: 0.6318 - val_loss: 0.7994 - val_accuracy: 0.6305\n",
            "Epoch 253/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7941 - accuracy: 0.6326 - val_loss: 0.7996 - val_accuracy: 0.6301\n",
            "Epoch 254/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7941 - accuracy: 0.6325 - val_loss: 0.7992 - val_accuracy: 0.6301\n",
            "Epoch 255/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7940 - accuracy: 0.6319 - val_loss: 0.7995 - val_accuracy: 0.6305\n",
            "Epoch 256/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7940 - accuracy: 0.6315 - val_loss: 0.7993 - val_accuracy: 0.6309\n",
            "Epoch 257/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7939 - accuracy: 0.6317 - val_loss: 0.7991 - val_accuracy: 0.6310\n",
            "Epoch 258/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7937 - accuracy: 0.6321 - val_loss: 0.7994 - val_accuracy: 0.6330\n",
            "Epoch 259/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7939 - accuracy: 0.6323 - val_loss: 0.7991 - val_accuracy: 0.6309\n",
            "Epoch 260/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7938 - accuracy: 0.6324 - val_loss: 0.7991 - val_accuracy: 0.6302\n",
            "Epoch 261/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7938 - accuracy: 0.6321 - val_loss: 0.7991 - val_accuracy: 0.6313\n",
            "Epoch 262/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7937 - accuracy: 0.6325 - val_loss: 0.7992 - val_accuracy: 0.6305\n",
            "Epoch 263/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7937 - accuracy: 0.6325 - val_loss: 0.7990 - val_accuracy: 0.6305\n",
            "Epoch 264/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7936 - accuracy: 0.6327 - val_loss: 0.7993 - val_accuracy: 0.6322\n",
            "Epoch 265/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7935 - accuracy: 0.6323 - val_loss: 0.7989 - val_accuracy: 0.6301\n",
            "Epoch 266/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7934 - accuracy: 0.6329 - val_loss: 0.7989 - val_accuracy: 0.6309\n",
            "Epoch 267/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7935 - accuracy: 0.6329 - val_loss: 0.7988 - val_accuracy: 0.6302\n",
            "Epoch 268/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7934 - accuracy: 0.6331 - val_loss: 0.7991 - val_accuracy: 0.6330\n",
            "Epoch 269/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7934 - accuracy: 0.6329 - val_loss: 0.7989 - val_accuracy: 0.6318\n",
            "Epoch 270/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7933 - accuracy: 0.6318 - val_loss: 0.7991 - val_accuracy: 0.6302\n",
            "Epoch 271/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7934 - accuracy: 0.6328 - val_loss: 0.7987 - val_accuracy: 0.6313\n",
            "Epoch 272/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7931 - accuracy: 0.6327 - val_loss: 0.7986 - val_accuracy: 0.6302\n",
            "Epoch 273/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7932 - accuracy: 0.6332 - val_loss: 0.7985 - val_accuracy: 0.6301\n",
            "Epoch 274/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7932 - accuracy: 0.6325 - val_loss: 0.7987 - val_accuracy: 0.6319\n",
            "Epoch 275/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7931 - accuracy: 0.6324 - val_loss: 0.7985 - val_accuracy: 0.6301\n",
            "Epoch 276/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7931 - accuracy: 0.6331 - val_loss: 0.7984 - val_accuracy: 0.6312\n",
            "Epoch 277/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7930 - accuracy: 0.6322 - val_loss: 0.7984 - val_accuracy: 0.6301\n",
            "Epoch 278/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7929 - accuracy: 0.6332 - val_loss: 0.7984 - val_accuracy: 0.6302\n",
            "Epoch 279/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7928 - accuracy: 0.6332 - val_loss: 0.7987 - val_accuracy: 0.6316\n",
            "Epoch 280/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7929 - accuracy: 0.6336 - val_loss: 0.7985 - val_accuracy: 0.6313\n",
            "Epoch 281/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7929 - accuracy: 0.6321 - val_loss: 0.7983 - val_accuracy: 0.6305\n",
            "Epoch 282/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7928 - accuracy: 0.6331 - val_loss: 0.7982 - val_accuracy: 0.6304\n",
            "Epoch 283/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7927 - accuracy: 0.6327 - val_loss: 0.7984 - val_accuracy: 0.6321\n",
            "Epoch 284/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7927 - accuracy: 0.6333 - val_loss: 0.7982 - val_accuracy: 0.6310\n",
            "Epoch 285/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7925 - accuracy: 0.6316 - val_loss: 0.7981 - val_accuracy: 0.6302\n",
            "Epoch 286/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7926 - accuracy: 0.6323 - val_loss: 0.7981 - val_accuracy: 0.6313\n",
            "Epoch 287/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7925 - accuracy: 0.6333 - val_loss: 0.7981 - val_accuracy: 0.6304\n",
            "Epoch 288/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7925 - accuracy: 0.6319 - val_loss: 0.7980 - val_accuracy: 0.6309\n",
            "Epoch 289/600\n",
            "237/237 [==============================] - 2s 6ms/step - loss: 0.7923 - accuracy: 0.6325 - val_loss: 0.7983 - val_accuracy: 0.6316\n",
            "Epoch 290/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7925 - accuracy: 0.6336 - val_loss: 0.7979 - val_accuracy: 0.6309\n",
            "Epoch 291/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7923 - accuracy: 0.6338 - val_loss: 0.7980 - val_accuracy: 0.6307\n",
            "Epoch 292/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7923 - accuracy: 0.6325 - val_loss: 0.7982 - val_accuracy: 0.6322\n",
            "Epoch 293/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7923 - accuracy: 0.6335 - val_loss: 0.7978 - val_accuracy: 0.6312\n",
            "Epoch 294/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7921 - accuracy: 0.6340 - val_loss: 0.7981 - val_accuracy: 0.6305\n",
            "Epoch 295/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7922 - accuracy: 0.6338 - val_loss: 0.7978 - val_accuracy: 0.6312\n",
            "Epoch 296/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7922 - accuracy: 0.6332 - val_loss: 0.7978 - val_accuracy: 0.6301\n",
            "Epoch 297/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7920 - accuracy: 0.6341 - val_loss: 0.7982 - val_accuracy: 0.6324\n",
            "Epoch 298/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7921 - accuracy: 0.6332 - val_loss: 0.7979 - val_accuracy: 0.6310\n",
            "Epoch 299/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7920 - accuracy: 0.6338 - val_loss: 0.7978 - val_accuracy: 0.6301\n",
            "Epoch 300/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7920 - accuracy: 0.6331 - val_loss: 0.7977 - val_accuracy: 0.6309\n",
            "Epoch 301/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7919 - accuracy: 0.6336 - val_loss: 0.7977 - val_accuracy: 0.6309\n",
            "Epoch 302/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7919 - accuracy: 0.6336 - val_loss: 0.7976 - val_accuracy: 0.6305\n",
            "Epoch 303/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7919 - accuracy: 0.6331 - val_loss: 0.7977 - val_accuracy: 0.6307\n",
            "Epoch 304/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7918 - accuracy: 0.6331 - val_loss: 0.7975 - val_accuracy: 0.6307\n",
            "Epoch 305/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7918 - accuracy: 0.6335 - val_loss: 0.7975 - val_accuracy: 0.6305\n",
            "Epoch 306/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7918 - accuracy: 0.6332 - val_loss: 0.7975 - val_accuracy: 0.6309\n",
            "Epoch 307/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7917 - accuracy: 0.6334 - val_loss: 0.7975 - val_accuracy: 0.6310\n",
            "Epoch 308/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7917 - accuracy: 0.6337 - val_loss: 0.7974 - val_accuracy: 0.6304\n",
            "Epoch 309/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7917 - accuracy: 0.6336 - val_loss: 0.7975 - val_accuracy: 0.6315\n",
            "Epoch 310/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7916 - accuracy: 0.6337 - val_loss: 0.7974 - val_accuracy: 0.6304\n",
            "Epoch 311/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7916 - accuracy: 0.6343 - val_loss: 0.7973 - val_accuracy: 0.6310\n",
            "Epoch 312/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7915 - accuracy: 0.6332 - val_loss: 0.7974 - val_accuracy: 0.6309\n",
            "Epoch 313/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7914 - accuracy: 0.6338 - val_loss: 0.7973 - val_accuracy: 0.6310\n",
            "Epoch 314/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7914 - accuracy: 0.6327 - val_loss: 0.7973 - val_accuracy: 0.6316\n",
            "Epoch 315/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7914 - accuracy: 0.6346 - val_loss: 0.7974 - val_accuracy: 0.6310\n",
            "Epoch 316/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7913 - accuracy: 0.6350 - val_loss: 0.7972 - val_accuracy: 0.6316\n",
            "Epoch 317/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7913 - accuracy: 0.6344 - val_loss: 0.7972 - val_accuracy: 0.6313\n",
            "Epoch 318/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7913 - accuracy: 0.6340 - val_loss: 0.7977 - val_accuracy: 0.6318\n",
            "Epoch 319/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7913 - accuracy: 0.6343 - val_loss: 0.7971 - val_accuracy: 0.6312\n",
            "Epoch 320/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7912 - accuracy: 0.6336 - val_loss: 0.7973 - val_accuracy: 0.6325\n",
            "Epoch 321/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7912 - accuracy: 0.6333 - val_loss: 0.7973 - val_accuracy: 0.6307\n",
            "Epoch 322/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7911 - accuracy: 0.6339 - val_loss: 0.7971 - val_accuracy: 0.6310\n",
            "Epoch 323/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7910 - accuracy: 0.6339 - val_loss: 0.7974 - val_accuracy: 0.6310\n",
            "Epoch 324/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7911 - accuracy: 0.6332 - val_loss: 0.7971 - val_accuracy: 0.6315\n",
            "Epoch 325/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7911 - accuracy: 0.6342 - val_loss: 0.7969 - val_accuracy: 0.6309\n",
            "Epoch 326/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7910 - accuracy: 0.6335 - val_loss: 0.7970 - val_accuracy: 0.6313\n",
            "Epoch 327/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7909 - accuracy: 0.6333 - val_loss: 0.7969 - val_accuracy: 0.6312\n",
            "Epoch 328/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7908 - accuracy: 0.6346 - val_loss: 0.7972 - val_accuracy: 0.6319\n",
            "Epoch 329/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7908 - accuracy: 0.6336 - val_loss: 0.7978 - val_accuracy: 0.6343\n",
            "Epoch 330/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7908 - accuracy: 0.6350 - val_loss: 0.7968 - val_accuracy: 0.6316\n",
            "Epoch 331/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7908 - accuracy: 0.6338 - val_loss: 0.7969 - val_accuracy: 0.6312\n",
            "Epoch 332/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7909 - accuracy: 0.6346 - val_loss: 0.7967 - val_accuracy: 0.6316\n",
            "Epoch 333/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7907 - accuracy: 0.6339 - val_loss: 0.7970 - val_accuracy: 0.6315\n",
            "Epoch 334/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7907 - accuracy: 0.6336 - val_loss: 0.7968 - val_accuracy: 0.6315\n",
            "Epoch 335/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7907 - accuracy: 0.6338 - val_loss: 0.7970 - val_accuracy: 0.6328\n",
            "Epoch 336/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7905 - accuracy: 0.6347 - val_loss: 0.7967 - val_accuracy: 0.6315\n",
            "Epoch 337/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7906 - accuracy: 0.6343 - val_loss: 0.7966 - val_accuracy: 0.6313\n",
            "Epoch 338/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7905 - accuracy: 0.6351 - val_loss: 0.7967 - val_accuracy: 0.6312\n",
            "Epoch 339/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7904 - accuracy: 0.6353 - val_loss: 0.7965 - val_accuracy: 0.6318\n",
            "Epoch 340/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7905 - accuracy: 0.6339 - val_loss: 0.7968 - val_accuracy: 0.6333\n",
            "Epoch 341/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7905 - accuracy: 0.6345 - val_loss: 0.7965 - val_accuracy: 0.6316\n",
            "Epoch 342/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7904 - accuracy: 0.6341 - val_loss: 0.7965 - val_accuracy: 0.6316\n",
            "Epoch 343/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7904 - accuracy: 0.6334 - val_loss: 0.7965 - val_accuracy: 0.6315\n",
            "Epoch 344/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7903 - accuracy: 0.6349 - val_loss: 0.7964 - val_accuracy: 0.6321\n",
            "Epoch 345/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7902 - accuracy: 0.6339 - val_loss: 0.7964 - val_accuracy: 0.6316\n",
            "Epoch 346/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7903 - accuracy: 0.6349 - val_loss: 0.7964 - val_accuracy: 0.6309\n",
            "Epoch 347/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7903 - accuracy: 0.6338 - val_loss: 0.7964 - val_accuracy: 0.6316\n",
            "Epoch 348/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7902 - accuracy: 0.6348 - val_loss: 0.7963 - val_accuracy: 0.6316\n",
            "Epoch 349/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7902 - accuracy: 0.6346 - val_loss: 0.7963 - val_accuracy: 0.6318\n",
            "Epoch 350/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7902 - accuracy: 0.6340 - val_loss: 0.7965 - val_accuracy: 0.6319\n",
            "Epoch 351/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7900 - accuracy: 0.6336 - val_loss: 0.7967 - val_accuracy: 0.6324\n",
            "Epoch 352/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7901 - accuracy: 0.6346 - val_loss: 0.7966 - val_accuracy: 0.6309\n",
            "Epoch 353/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7900 - accuracy: 0.6344 - val_loss: 0.7962 - val_accuracy: 0.6325\n",
            "Epoch 354/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7901 - accuracy: 0.6351 - val_loss: 0.7962 - val_accuracy: 0.6322\n",
            "Epoch 355/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7900 - accuracy: 0.6339 - val_loss: 0.7963 - val_accuracy: 0.6318\n",
            "Epoch 356/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7899 - accuracy: 0.6332 - val_loss: 0.7965 - val_accuracy: 0.6321\n",
            "Epoch 357/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7899 - accuracy: 0.6342 - val_loss: 0.7963 - val_accuracy: 0.6318\n",
            "Epoch 358/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7900 - accuracy: 0.6343 - val_loss: 0.7962 - val_accuracy: 0.6319\n",
            "Epoch 359/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7899 - accuracy: 0.6345 - val_loss: 0.7963 - val_accuracy: 0.6324\n",
            "Epoch 360/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7898 - accuracy: 0.6339 - val_loss: 0.7962 - val_accuracy: 0.6325\n",
            "Epoch 361/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7898 - accuracy: 0.6337 - val_loss: 0.7962 - val_accuracy: 0.6318\n",
            "Epoch 362/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7897 - accuracy: 0.6347 - val_loss: 0.7961 - val_accuracy: 0.6324\n",
            "Epoch 363/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7896 - accuracy: 0.6357 - val_loss: 0.7967 - val_accuracy: 0.6324\n",
            "Epoch 364/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7898 - accuracy: 0.6340 - val_loss: 0.7960 - val_accuracy: 0.6322\n",
            "Epoch 365/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7896 - accuracy: 0.6343 - val_loss: 0.7961 - val_accuracy: 0.6316\n",
            "Epoch 366/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7896 - accuracy: 0.6338 - val_loss: 0.7960 - val_accuracy: 0.6316\n",
            "Epoch 367/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7896 - accuracy: 0.6338 - val_loss: 0.7960 - val_accuracy: 0.6321\n",
            "Epoch 368/600\n",
            "237/237 [==============================] - 2s 6ms/step - loss: 0.7895 - accuracy: 0.6342 - val_loss: 0.7959 - val_accuracy: 0.6331\n",
            "Epoch 369/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7895 - accuracy: 0.6341 - val_loss: 0.7961 - val_accuracy: 0.6330\n",
            "Epoch 370/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7895 - accuracy: 0.6347 - val_loss: 0.7959 - val_accuracy: 0.6327\n",
            "Epoch 371/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7895 - accuracy: 0.6346 - val_loss: 0.7959 - val_accuracy: 0.6330\n",
            "Epoch 372/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7895 - accuracy: 0.6347 - val_loss: 0.7959 - val_accuracy: 0.6324\n",
            "Epoch 373/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7894 - accuracy: 0.6344 - val_loss: 0.7959 - val_accuracy: 0.6328\n",
            "Epoch 374/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7893 - accuracy: 0.6346 - val_loss: 0.7958 - val_accuracy: 0.6322\n",
            "Epoch 375/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7893 - accuracy: 0.6340 - val_loss: 0.7958 - val_accuracy: 0.6324\n",
            "Epoch 376/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7893 - accuracy: 0.6344 - val_loss: 0.7958 - val_accuracy: 0.6319\n",
            "Epoch 377/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7893 - accuracy: 0.6338 - val_loss: 0.7962 - val_accuracy: 0.6331\n",
            "Epoch 378/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7892 - accuracy: 0.6347 - val_loss: 0.7957 - val_accuracy: 0.6327\n",
            "Epoch 379/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7893 - accuracy: 0.6338 - val_loss: 0.7959 - val_accuracy: 0.6318\n",
            "Epoch 380/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7891 - accuracy: 0.6349 - val_loss: 0.7957 - val_accuracy: 0.6337\n",
            "Epoch 381/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7892 - accuracy: 0.6347 - val_loss: 0.7957 - val_accuracy: 0.6316\n",
            "Epoch 382/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7891 - accuracy: 0.6345 - val_loss: 0.7963 - val_accuracy: 0.6330\n",
            "Epoch 383/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7892 - accuracy: 0.6342 - val_loss: 0.7957 - val_accuracy: 0.6312\n",
            "Epoch 384/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7891 - accuracy: 0.6340 - val_loss: 0.7957 - val_accuracy: 0.6319\n",
            "Epoch 385/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7889 - accuracy: 0.6342 - val_loss: 0.7956 - val_accuracy: 0.6321\n",
            "Epoch 386/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7890 - accuracy: 0.6344 - val_loss: 0.7956 - val_accuracy: 0.6333\n",
            "Epoch 387/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7889 - accuracy: 0.6338 - val_loss: 0.7956 - val_accuracy: 0.6330\n",
            "Epoch 388/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7890 - accuracy: 0.6348 - val_loss: 0.7956 - val_accuracy: 0.6328\n",
            "Epoch 389/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7889 - accuracy: 0.6342 - val_loss: 0.7957 - val_accuracy: 0.6324\n",
            "Epoch 390/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7889 - accuracy: 0.6341 - val_loss: 0.7955 - val_accuracy: 0.6321\n",
            "Epoch 391/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7889 - accuracy: 0.6338 - val_loss: 0.7956 - val_accuracy: 0.6333\n",
            "Epoch 392/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7889 - accuracy: 0.6338 - val_loss: 0.7956 - val_accuracy: 0.6325\n",
            "Epoch 393/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7889 - accuracy: 0.6341 - val_loss: 0.7955 - val_accuracy: 0.6336\n",
            "Epoch 394/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7888 - accuracy: 0.6341 - val_loss: 0.7955 - val_accuracy: 0.6321\n",
            "Epoch 395/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7888 - accuracy: 0.6352 - val_loss: 0.7956 - val_accuracy: 0.6327\n",
            "Epoch 396/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7886 - accuracy: 0.6342 - val_loss: 0.7961 - val_accuracy: 0.6336\n",
            "Epoch 397/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7888 - accuracy: 0.6344 - val_loss: 0.7954 - val_accuracy: 0.6324\n",
            "Epoch 398/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7887 - accuracy: 0.6345 - val_loss: 0.7954 - val_accuracy: 0.6325\n",
            "Epoch 399/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7886 - accuracy: 0.6344 - val_loss: 0.7955 - val_accuracy: 0.6327\n",
            "Epoch 400/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7886 - accuracy: 0.6336 - val_loss: 0.7954 - val_accuracy: 0.6325\n",
            "Epoch 401/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7886 - accuracy: 0.6340 - val_loss: 0.7954 - val_accuracy: 0.6327\n",
            "Epoch 402/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7885 - accuracy: 0.6340 - val_loss: 0.7953 - val_accuracy: 0.6325\n",
            "Epoch 403/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7885 - accuracy: 0.6348 - val_loss: 0.7954 - val_accuracy: 0.6328\n",
            "Epoch 404/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7885 - accuracy: 0.6346 - val_loss: 0.7955 - val_accuracy: 0.6328\n",
            "Epoch 405/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7884 - accuracy: 0.6342 - val_loss: 0.7952 - val_accuracy: 0.6328\n",
            "Epoch 406/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7885 - accuracy: 0.6347 - val_loss: 0.7952 - val_accuracy: 0.6330\n",
            "Epoch 407/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7884 - accuracy: 0.6346 - val_loss: 0.7952 - val_accuracy: 0.6322\n",
            "Epoch 408/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7884 - accuracy: 0.6347 - val_loss: 0.7952 - val_accuracy: 0.6325\n",
            "Epoch 409/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7883 - accuracy: 0.6347 - val_loss: 0.7953 - val_accuracy: 0.6321\n",
            "Epoch 410/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7884 - accuracy: 0.6353 - val_loss: 0.7953 - val_accuracy: 0.6333\n",
            "Epoch 411/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7883 - accuracy: 0.6346 - val_loss: 0.7952 - val_accuracy: 0.6333\n",
            "Epoch 412/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7883 - accuracy: 0.6344 - val_loss: 0.7952 - val_accuracy: 0.6328\n",
            "Epoch 413/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7883 - accuracy: 0.6341 - val_loss: 0.7951 - val_accuracy: 0.6322\n",
            "Epoch 414/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7882 - accuracy: 0.6343 - val_loss: 0.7959 - val_accuracy: 0.6337\n",
            "Epoch 415/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7883 - accuracy: 0.6342 - val_loss: 0.7956 - val_accuracy: 0.6337\n",
            "Epoch 416/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7883 - accuracy: 0.6340 - val_loss: 0.7952 - val_accuracy: 0.6325\n",
            "Epoch 417/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7881 - accuracy: 0.6346 - val_loss: 0.7952 - val_accuracy: 0.6321\n",
            "Epoch 418/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7882 - accuracy: 0.6344 - val_loss: 0.7950 - val_accuracy: 0.6325\n",
            "Epoch 419/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7881 - accuracy: 0.6344 - val_loss: 0.7951 - val_accuracy: 0.6325\n",
            "Epoch 420/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7881 - accuracy: 0.6352 - val_loss: 0.7951 - val_accuracy: 0.6333\n",
            "Epoch 421/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7880 - accuracy: 0.6340 - val_loss: 0.7950 - val_accuracy: 0.6327\n",
            "Epoch 422/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7880 - accuracy: 0.6338 - val_loss: 0.7950 - val_accuracy: 0.6322\n",
            "Epoch 423/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7880 - accuracy: 0.6329 - val_loss: 0.7951 - val_accuracy: 0.6325\n",
            "Epoch 424/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7880 - accuracy: 0.6357 - val_loss: 0.7949 - val_accuracy: 0.6330\n",
            "Epoch 425/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7880 - accuracy: 0.6342 - val_loss: 0.7949 - val_accuracy: 0.6327\n",
            "Epoch 426/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7879 - accuracy: 0.6344 - val_loss: 0.7949 - val_accuracy: 0.6331\n",
            "Epoch 427/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7879 - accuracy: 0.6332 - val_loss: 0.7950 - val_accuracy: 0.6330\n",
            "Epoch 428/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7879 - accuracy: 0.6344 - val_loss: 0.7949 - val_accuracy: 0.6327\n",
            "Epoch 429/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7878 - accuracy: 0.6351 - val_loss: 0.7951 - val_accuracy: 0.6327\n",
            "Epoch 430/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7879 - accuracy: 0.6344 - val_loss: 0.7949 - val_accuracy: 0.6331\n",
            "Epoch 431/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7878 - accuracy: 0.6344 - val_loss: 0.7948 - val_accuracy: 0.6330\n",
            "Epoch 432/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7877 - accuracy: 0.6338 - val_loss: 0.7949 - val_accuracy: 0.6330\n",
            "Epoch 433/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7877 - accuracy: 0.6340 - val_loss: 0.7948 - val_accuracy: 0.6324\n",
            "Epoch 434/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7877 - accuracy: 0.6342 - val_loss: 0.7950 - val_accuracy: 0.6333\n",
            "Epoch 435/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7877 - accuracy: 0.6345 - val_loss: 0.7949 - val_accuracy: 0.6328\n",
            "Epoch 436/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7877 - accuracy: 0.6335 - val_loss: 0.7949 - val_accuracy: 0.6333\n",
            "Epoch 437/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7876 - accuracy: 0.6345 - val_loss: 0.7948 - val_accuracy: 0.6336\n",
            "Epoch 438/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7876 - accuracy: 0.6340 - val_loss: 0.7948 - val_accuracy: 0.6327\n",
            "Epoch 439/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7876 - accuracy: 0.6341 - val_loss: 0.7947 - val_accuracy: 0.6321\n",
            "Epoch 440/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7876 - accuracy: 0.6344 - val_loss: 0.7947 - val_accuracy: 0.6330\n",
            "Epoch 441/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7875 - accuracy: 0.6343 - val_loss: 0.7947 - val_accuracy: 0.6327\n",
            "Epoch 442/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7875 - accuracy: 0.6330 - val_loss: 0.7947 - val_accuracy: 0.6324\n",
            "Epoch 443/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7875 - accuracy: 0.6349 - val_loss: 0.7946 - val_accuracy: 0.6316\n",
            "Epoch 444/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7876 - accuracy: 0.6341 - val_loss: 0.7946 - val_accuracy: 0.6321\n",
            "Epoch 445/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7874 - accuracy: 0.6337 - val_loss: 0.7946 - val_accuracy: 0.6319\n",
            "Epoch 446/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7874 - accuracy: 0.6329 - val_loss: 0.7946 - val_accuracy: 0.6324\n",
            "Epoch 447/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7874 - accuracy: 0.6339 - val_loss: 0.7948 - val_accuracy: 0.6337\n",
            "Epoch 448/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7874 - accuracy: 0.6339 - val_loss: 0.7946 - val_accuracy: 0.6324\n",
            "Epoch 449/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7873 - accuracy: 0.6335 - val_loss: 0.7946 - val_accuracy: 0.6318\n",
            "Epoch 450/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7873 - accuracy: 0.6342 - val_loss: 0.7946 - val_accuracy: 0.6318\n",
            "Epoch 451/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7873 - accuracy: 0.6343 - val_loss: 0.7946 - val_accuracy: 0.6328\n",
            "Epoch 452/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7874 - accuracy: 0.6344 - val_loss: 0.7946 - val_accuracy: 0.6330\n",
            "Epoch 453/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7873 - accuracy: 0.6340 - val_loss: 0.7945 - val_accuracy: 0.6328\n",
            "Epoch 454/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7871 - accuracy: 0.6335 - val_loss: 0.7950 - val_accuracy: 0.6327\n",
            "Epoch 455/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7872 - accuracy: 0.6344 - val_loss: 0.7945 - val_accuracy: 0.6315\n",
            "Epoch 456/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7872 - accuracy: 0.6332 - val_loss: 0.7945 - val_accuracy: 0.6328\n",
            "Epoch 457/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7872 - accuracy: 0.6340 - val_loss: 0.7947 - val_accuracy: 0.6334\n",
            "Epoch 458/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7872 - accuracy: 0.6343 - val_loss: 0.7945 - val_accuracy: 0.6334\n",
            "Epoch 459/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7872 - accuracy: 0.6355 - val_loss: 0.7944 - val_accuracy: 0.6325\n",
            "Epoch 460/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7870 - accuracy: 0.6346 - val_loss: 0.7946 - val_accuracy: 0.6339\n",
            "Epoch 461/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7871 - accuracy: 0.6338 - val_loss: 0.7944 - val_accuracy: 0.6330\n",
            "Epoch 462/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7870 - accuracy: 0.6353 - val_loss: 0.7944 - val_accuracy: 0.6328\n",
            "Epoch 463/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7870 - accuracy: 0.6340 - val_loss: 0.7944 - val_accuracy: 0.6328\n",
            "Epoch 464/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7870 - accuracy: 0.6346 - val_loss: 0.7944 - val_accuracy: 0.6325\n",
            "Epoch 465/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7869 - accuracy: 0.6342 - val_loss: 0.7944 - val_accuracy: 0.6321\n",
            "Epoch 466/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7868 - accuracy: 0.6348 - val_loss: 0.7944 - val_accuracy: 0.6340\n",
            "Epoch 467/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7870 - accuracy: 0.6340 - val_loss: 0.7943 - val_accuracy: 0.6324\n",
            "Epoch 468/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7868 - accuracy: 0.6346 - val_loss: 0.7946 - val_accuracy: 0.6336\n",
            "Epoch 469/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7869 - accuracy: 0.6334 - val_loss: 0.7946 - val_accuracy: 0.6333\n",
            "Epoch 470/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7868 - accuracy: 0.6331 - val_loss: 0.7943 - val_accuracy: 0.6327\n",
            "Epoch 471/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7869 - accuracy: 0.6329 - val_loss: 0.7942 - val_accuracy: 0.6315\n",
            "Epoch 472/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7868 - accuracy: 0.6334 - val_loss: 0.7944 - val_accuracy: 0.6325\n",
            "Epoch 473/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7868 - accuracy: 0.6340 - val_loss: 0.7943 - val_accuracy: 0.6324\n",
            "Epoch 474/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7868 - accuracy: 0.6341 - val_loss: 0.7943 - val_accuracy: 0.6327\n",
            "Epoch 475/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7868 - accuracy: 0.6344 - val_loss: 0.7942 - val_accuracy: 0.6321\n",
            "Epoch 476/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7868 - accuracy: 0.6333 - val_loss: 0.7942 - val_accuracy: 0.6330\n",
            "Epoch 477/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7868 - accuracy: 0.6346 - val_loss: 0.7942 - val_accuracy: 0.6322\n",
            "Epoch 478/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7867 - accuracy: 0.6347 - val_loss: 0.7942 - val_accuracy: 0.6330\n",
            "Epoch 479/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7867 - accuracy: 0.6334 - val_loss: 0.7941 - val_accuracy: 0.6319\n",
            "Epoch 480/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7867 - accuracy: 0.6340 - val_loss: 0.7944 - val_accuracy: 0.6324\n",
            "Epoch 481/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7867 - accuracy: 0.6334 - val_loss: 0.7942 - val_accuracy: 0.6321\n",
            "Epoch 482/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7867 - accuracy: 0.6343 - val_loss: 0.7941 - val_accuracy: 0.6322\n",
            "Epoch 483/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7866 - accuracy: 0.6347 - val_loss: 0.7945 - val_accuracy: 0.6330\n",
            "Epoch 484/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7866 - accuracy: 0.6336 - val_loss: 0.7943 - val_accuracy: 0.6327\n",
            "Epoch 485/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7866 - accuracy: 0.6347 - val_loss: 0.7940 - val_accuracy: 0.6319\n",
            "Epoch 486/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7865 - accuracy: 0.6340 - val_loss: 0.7941 - val_accuracy: 0.6336\n",
            "Epoch 487/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7866 - accuracy: 0.6342 - val_loss: 0.7942 - val_accuracy: 0.6325\n",
            "Epoch 488/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7866 - accuracy: 0.6334 - val_loss: 0.7940 - val_accuracy: 0.6321\n",
            "Epoch 489/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7865 - accuracy: 0.6334 - val_loss: 0.7940 - val_accuracy: 0.6318\n",
            "Epoch 490/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7865 - accuracy: 0.6347 - val_loss: 0.7940 - val_accuracy: 0.6327\n",
            "Epoch 491/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7865 - accuracy: 0.6338 - val_loss: 0.7940 - val_accuracy: 0.6327\n",
            "Epoch 492/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7864 - accuracy: 0.6346 - val_loss: 0.7940 - val_accuracy: 0.6327\n",
            "Epoch 493/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7864 - accuracy: 0.6331 - val_loss: 0.7942 - val_accuracy: 0.6327\n",
            "Epoch 494/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7864 - accuracy: 0.6344 - val_loss: 0.7940 - val_accuracy: 0.6319\n",
            "Epoch 495/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7864 - accuracy: 0.6341 - val_loss: 0.7939 - val_accuracy: 0.6319\n",
            "Epoch 496/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7863 - accuracy: 0.6348 - val_loss: 0.7941 - val_accuracy: 0.6321\n",
            "Epoch 497/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7863 - accuracy: 0.6355 - val_loss: 0.7941 - val_accuracy: 0.6325\n",
            "Epoch 498/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7863 - accuracy: 0.6340 - val_loss: 0.7939 - val_accuracy: 0.6327\n",
            "Epoch 499/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7863 - accuracy: 0.6344 - val_loss: 0.7940 - val_accuracy: 0.6324\n",
            "Epoch 500/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7863 - accuracy: 0.6335 - val_loss: 0.7939 - val_accuracy: 0.6328\n",
            "Epoch 501/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7862 - accuracy: 0.6344 - val_loss: 0.7941 - val_accuracy: 0.6339\n",
            "Epoch 502/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7863 - accuracy: 0.6338 - val_loss: 0.7939 - val_accuracy: 0.6324\n",
            "Epoch 503/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7861 - accuracy: 0.6351 - val_loss: 0.7940 - val_accuracy: 0.6322\n",
            "Epoch 504/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7861 - accuracy: 0.6339 - val_loss: 0.7938 - val_accuracy: 0.6322\n",
            "Epoch 505/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7862 - accuracy: 0.6333 - val_loss: 0.7938 - val_accuracy: 0.6325\n",
            "Epoch 506/600\n",
            "237/237 [==============================] - 2s 6ms/step - loss: 0.7861 - accuracy: 0.6344 - val_loss: 0.7942 - val_accuracy: 0.6324\n",
            "Epoch 507/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7861 - accuracy: 0.6338 - val_loss: 0.7938 - val_accuracy: 0.6327\n",
            "Epoch 508/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7860 - accuracy: 0.6348 - val_loss: 0.7941 - val_accuracy: 0.6340\n",
            "Epoch 509/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7861 - accuracy: 0.6336 - val_loss: 0.7938 - val_accuracy: 0.6321\n",
            "Epoch 510/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7861 - accuracy: 0.6342 - val_loss: 0.7937 - val_accuracy: 0.6324\n",
            "Epoch 511/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7860 - accuracy: 0.6334 - val_loss: 0.7939 - val_accuracy: 0.6334\n",
            "Epoch 512/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7860 - accuracy: 0.6344 - val_loss: 0.7937 - val_accuracy: 0.6324\n",
            "Epoch 513/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7860 - accuracy: 0.6338 - val_loss: 0.7937 - val_accuracy: 0.6327\n",
            "Epoch 514/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7860 - accuracy: 0.6347 - val_loss: 0.7937 - val_accuracy: 0.6325\n",
            "Epoch 515/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7860 - accuracy: 0.6340 - val_loss: 0.7937 - val_accuracy: 0.6324\n",
            "Epoch 516/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7860 - accuracy: 0.6349 - val_loss: 0.7938 - val_accuracy: 0.6319\n",
            "Epoch 517/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7859 - accuracy: 0.6338 - val_loss: 0.7937 - val_accuracy: 0.6328\n",
            "Epoch 518/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7859 - accuracy: 0.6343 - val_loss: 0.7938 - val_accuracy: 0.6319\n",
            "Epoch 519/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7859 - accuracy: 0.6340 - val_loss: 0.7936 - val_accuracy: 0.6333\n",
            "Epoch 520/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7858 - accuracy: 0.6346 - val_loss: 0.7936 - val_accuracy: 0.6324\n",
            "Epoch 521/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7859 - accuracy: 0.6335 - val_loss: 0.7936 - val_accuracy: 0.6316\n",
            "Epoch 522/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7858 - accuracy: 0.6348 - val_loss: 0.7936 - val_accuracy: 0.6327\n",
            "Epoch 523/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7858 - accuracy: 0.6341 - val_loss: 0.7937 - val_accuracy: 0.6318\n",
            "Epoch 524/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7858 - accuracy: 0.6332 - val_loss: 0.7938 - val_accuracy: 0.6327\n",
            "Epoch 525/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7857 - accuracy: 0.6345 - val_loss: 0.7937 - val_accuracy: 0.6322\n",
            "Epoch 526/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7857 - accuracy: 0.6336 - val_loss: 0.7936 - val_accuracy: 0.6316\n",
            "Epoch 527/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7858 - accuracy: 0.6345 - val_loss: 0.7936 - val_accuracy: 0.6333\n",
            "Epoch 528/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7857 - accuracy: 0.6335 - val_loss: 0.7936 - val_accuracy: 0.6316\n",
            "Epoch 529/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7857 - accuracy: 0.6342 - val_loss: 0.7936 - val_accuracy: 0.6331\n",
            "Epoch 530/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7856 - accuracy: 0.6336 - val_loss: 0.7942 - val_accuracy: 0.6325\n",
            "Epoch 531/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7857 - accuracy: 0.6351 - val_loss: 0.7935 - val_accuracy: 0.6319\n",
            "Epoch 532/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7856 - accuracy: 0.6335 - val_loss: 0.7937 - val_accuracy: 0.6330\n",
            "Epoch 533/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7856 - accuracy: 0.6332 - val_loss: 0.7937 - val_accuracy: 0.6328\n",
            "Epoch 534/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7856 - accuracy: 0.6337 - val_loss: 0.7935 - val_accuracy: 0.6327\n",
            "Epoch 535/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7856 - accuracy: 0.6334 - val_loss: 0.7935 - val_accuracy: 0.6319\n",
            "Epoch 536/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7855 - accuracy: 0.6336 - val_loss: 0.7935 - val_accuracy: 0.6327\n",
            "Epoch 537/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7855 - accuracy: 0.6343 - val_loss: 0.7938 - val_accuracy: 0.6330\n",
            "Epoch 538/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7855 - accuracy: 0.6330 - val_loss: 0.7934 - val_accuracy: 0.6324\n",
            "Epoch 539/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7855 - accuracy: 0.6345 - val_loss: 0.7937 - val_accuracy: 0.6330\n",
            "Epoch 540/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7856 - accuracy: 0.6338 - val_loss: 0.7934 - val_accuracy: 0.6322\n",
            "Epoch 541/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7854 - accuracy: 0.6349 - val_loss: 0.7936 - val_accuracy: 0.6333\n",
            "Epoch 542/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7855 - accuracy: 0.6352 - val_loss: 0.7934 - val_accuracy: 0.6319\n",
            "Epoch 543/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7854 - accuracy: 0.6347 - val_loss: 0.7934 - val_accuracy: 0.6325\n",
            "Epoch 544/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7854 - accuracy: 0.6346 - val_loss: 0.7934 - val_accuracy: 0.6331\n",
            "Epoch 545/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7854 - accuracy: 0.6340 - val_loss: 0.7934 - val_accuracy: 0.6331\n",
            "Epoch 546/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7854 - accuracy: 0.6339 - val_loss: 0.7934 - val_accuracy: 0.6325\n",
            "Epoch 547/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7854 - accuracy: 0.6340 - val_loss: 0.7937 - val_accuracy: 0.6324\n",
            "Epoch 548/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7854 - accuracy: 0.6347 - val_loss: 0.7933 - val_accuracy: 0.6319\n",
            "Epoch 549/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7853 - accuracy: 0.6338 - val_loss: 0.7934 - val_accuracy: 0.6327\n",
            "Epoch 550/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7853 - accuracy: 0.6344 - val_loss: 0.7933 - val_accuracy: 0.6322\n",
            "Epoch 551/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7852 - accuracy: 0.6348 - val_loss: 0.7933 - val_accuracy: 0.6327\n",
            "Epoch 552/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7853 - accuracy: 0.6337 - val_loss: 0.7933 - val_accuracy: 0.6321\n",
            "Epoch 553/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7853 - accuracy: 0.6336 - val_loss: 0.7933 - val_accuracy: 0.6325\n",
            "Epoch 554/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7853 - accuracy: 0.6349 - val_loss: 0.7934 - val_accuracy: 0.6327\n",
            "Epoch 555/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7853 - accuracy: 0.6342 - val_loss: 0.7933 - val_accuracy: 0.6313\n",
            "Epoch 556/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7852 - accuracy: 0.6338 - val_loss: 0.7934 - val_accuracy: 0.6330\n",
            "Epoch 557/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7852 - accuracy: 0.6338 - val_loss: 0.7933 - val_accuracy: 0.6319\n",
            "Epoch 558/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7852 - accuracy: 0.6345 - val_loss: 0.7933 - val_accuracy: 0.6325\n",
            "Epoch 559/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7852 - accuracy: 0.6338 - val_loss: 0.7933 - val_accuracy: 0.6330\n",
            "Epoch 560/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7851 - accuracy: 0.6342 - val_loss: 0.7933 - val_accuracy: 0.6312\n",
            "Epoch 561/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7851 - accuracy: 0.6347 - val_loss: 0.7933 - val_accuracy: 0.6324\n",
            "Epoch 562/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7851 - accuracy: 0.6336 - val_loss: 0.7934 - val_accuracy: 0.6331\n",
            "Epoch 563/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7851 - accuracy: 0.6346 - val_loss: 0.7932 - val_accuracy: 0.6325\n",
            "Epoch 564/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7850 - accuracy: 0.6347 - val_loss: 0.7935 - val_accuracy: 0.6330\n",
            "Epoch 565/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7851 - accuracy: 0.6353 - val_loss: 0.7932 - val_accuracy: 0.6331\n",
            "Epoch 566/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7851 - accuracy: 0.6339 - val_loss: 0.7933 - val_accuracy: 0.6330\n",
            "Epoch 567/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7850 - accuracy: 0.6338 - val_loss: 0.7932 - val_accuracy: 0.6330\n",
            "Epoch 568/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7850 - accuracy: 0.6341 - val_loss: 0.7932 - val_accuracy: 0.6316\n",
            "Epoch 569/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7850 - accuracy: 0.6338 - val_loss: 0.7932 - val_accuracy: 0.6322\n",
            "Epoch 570/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7849 - accuracy: 0.6340 - val_loss: 0.7932 - val_accuracy: 0.6327\n",
            "Epoch 571/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7850 - accuracy: 0.6343 - val_loss: 0.7931 - val_accuracy: 0.6327\n",
            "Epoch 572/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7849 - accuracy: 0.6349 - val_loss: 0.7936 - val_accuracy: 0.6340\n",
            "Epoch 573/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7850 - accuracy: 0.6350 - val_loss: 0.7934 - val_accuracy: 0.6325\n",
            "Epoch 574/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7848 - accuracy: 0.6349 - val_loss: 0.7932 - val_accuracy: 0.6328\n",
            "Epoch 575/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7849 - accuracy: 0.6340 - val_loss: 0.7932 - val_accuracy: 0.6328\n",
            "Epoch 576/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7849 - accuracy: 0.6340 - val_loss: 0.7931 - val_accuracy: 0.6315\n",
            "Epoch 577/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7849 - accuracy: 0.6346 - val_loss: 0.7931 - val_accuracy: 0.6327\n",
            "Epoch 578/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7849 - accuracy: 0.6347 - val_loss: 0.7931 - val_accuracy: 0.6324\n",
            "Epoch 579/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7849 - accuracy: 0.6342 - val_loss: 0.7931 - val_accuracy: 0.6333\n",
            "Epoch 580/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7848 - accuracy: 0.6353 - val_loss: 0.7931 - val_accuracy: 0.6330\n",
            "Epoch 581/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7847 - accuracy: 0.6355 - val_loss: 0.7931 - val_accuracy: 0.6331\n",
            "Epoch 582/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7847 - accuracy: 0.6346 - val_loss: 0.7931 - val_accuracy: 0.6325\n",
            "Epoch 583/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7847 - accuracy: 0.6339 - val_loss: 0.7932 - val_accuracy: 0.6328\n",
            "Epoch 584/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7847 - accuracy: 0.6345 - val_loss: 0.7931 - val_accuracy: 0.6328\n",
            "Epoch 585/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7847 - accuracy: 0.6351 - val_loss: 0.7931 - val_accuracy: 0.6324\n",
            "Epoch 586/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7847 - accuracy: 0.6354 - val_loss: 0.7930 - val_accuracy: 0.6318\n",
            "Epoch 587/600\n",
            "237/237 [==============================] - 2s 7ms/step - loss: 0.7847 - accuracy: 0.6345 - val_loss: 0.7930 - val_accuracy: 0.6319\n",
            "Epoch 588/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7847 - accuracy: 0.6354 - val_loss: 0.7930 - val_accuracy: 0.6331\n",
            "Epoch 589/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7846 - accuracy: 0.6352 - val_loss: 0.7931 - val_accuracy: 0.6331\n",
            "Epoch 590/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7846 - accuracy: 0.6347 - val_loss: 0.7930 - val_accuracy: 0.6324\n",
            "Epoch 591/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7846 - accuracy: 0.6349 - val_loss: 0.7930 - val_accuracy: 0.6327\n",
            "Epoch 592/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7846 - accuracy: 0.6346 - val_loss: 0.7931 - val_accuracy: 0.6325\n",
            "Epoch 593/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7845 - accuracy: 0.6348 - val_loss: 0.7931 - val_accuracy: 0.6315\n",
            "Epoch 594/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7846 - accuracy: 0.6349 - val_loss: 0.7931 - val_accuracy: 0.6319\n",
            "Epoch 595/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7845 - accuracy: 0.6346 - val_loss: 0.7930 - val_accuracy: 0.6322\n",
            "Epoch 596/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7845 - accuracy: 0.6342 - val_loss: 0.7931 - val_accuracy: 0.6327\n",
            "Epoch 597/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7845 - accuracy: 0.6353 - val_loss: 0.7929 - val_accuracy: 0.6324\n",
            "Epoch 598/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7845 - accuracy: 0.6348 - val_loss: 0.7929 - val_accuracy: 0.6327\n",
            "Epoch 599/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7845 - accuracy: 0.6340 - val_loss: 0.7931 - val_accuracy: 0.6319\n",
            "Epoch 600/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7845 - accuracy: 0.6346 - val_loss: 0.7929 - val_accuracy: 0.6315\n",
            "74/74 [==============================] - 0s 3ms/step - loss: 0.7929 - accuracy: 0.6315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjeil1B7iPYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ddb72ba0-b658-4356-e00a-6c21401d8871"
      },
      "source": [
        "create_model1()\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 13)                182       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 50)                700       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 13,785\n",
            "Trainable params: 13,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0961 - accuracy: 0.4006 - val_loss: 1.0931 - val_accuracy: 0.4424\n",
            "Epoch 2/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0911 - accuracy: 0.4353 - val_loss: 1.0894 - val_accuracy: 0.4322\n",
            "Epoch 3/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0878 - accuracy: 0.4313 - val_loss: 1.0863 - val_accuracy: 0.4312\n",
            "Epoch 4/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0847 - accuracy: 0.4307 - val_loss: 1.0831 - val_accuracy: 0.4310\n",
            "Epoch 5/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 1.0815 - accuracy: 0.4308 - val_loss: 1.0799 - val_accuracy: 0.4310\n",
            "Epoch 6/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 1.0784 - accuracy: 0.4309 - val_loss: 1.0768 - val_accuracy: 0.4316\n",
            "Epoch 7/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0754 - accuracy: 0.4318 - val_loss: 1.0738 - val_accuracy: 0.4333\n",
            "Epoch 8/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 1.0723 - accuracy: 0.4352 - val_loss: 1.0705 - val_accuracy: 0.4403\n",
            "Epoch 9/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 1.0689 - accuracy: 0.4459 - val_loss: 1.0667 - val_accuracy: 0.4605\n",
            "Epoch 10/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 1.0647 - accuracy: 0.4760 - val_loss: 1.0621 - val_accuracy: 0.4994\n",
            "Epoch 11/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 1.0600 - accuracy: 0.5101 - val_loss: 1.0571 - val_accuracy: 0.5266\n",
            "Epoch 12/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0548 - accuracy: 0.5329 - val_loss: 1.0516 - val_accuracy: 0.5430\n",
            "Epoch 13/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 1.0493 - accuracy: 0.5451 - val_loss: 1.0458 - val_accuracy: 0.5515\n",
            "Epoch 14/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0432 - accuracy: 0.5555 - val_loss: 1.0393 - val_accuracy: 0.5573\n",
            "Epoch 15/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 1.0366 - accuracy: 0.5589 - val_loss: 1.0323 - val_accuracy: 0.5584\n",
            "Epoch 16/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0294 - accuracy: 0.5598 - val_loss: 1.0248 - val_accuracy: 0.5606\n",
            "Epoch 17/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 1.0216 - accuracy: 0.5604 - val_loss: 1.0167 - val_accuracy: 0.5609\n",
            "Epoch 18/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 1.0134 - accuracy: 0.5617 - val_loss: 1.0083 - val_accuracy: 0.5605\n",
            "Epoch 19/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 1.0049 - accuracy: 0.5619 - val_loss: 0.9999 - val_accuracy: 0.5658\n",
            "Epoch 20/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9964 - accuracy: 0.5629 - val_loss: 0.9911 - val_accuracy: 0.5649\n",
            "Epoch 21/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9879 - accuracy: 0.5640 - val_loss: 0.9827 - val_accuracy: 0.5652\n",
            "Epoch 22/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9796 - accuracy: 0.5652 - val_loss: 0.9746 - val_accuracy: 0.5664\n",
            "Epoch 23/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9716 - accuracy: 0.5657 - val_loss: 0.9666 - val_accuracy: 0.5670\n",
            "Epoch 24/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.9637 - accuracy: 0.5675 - val_loss: 0.9589 - val_accuracy: 0.5684\n",
            "Epoch 25/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.9560 - accuracy: 0.5687 - val_loss: 0.9511 - val_accuracy: 0.5699\n",
            "Epoch 26/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.9479 - accuracy: 0.5710 - val_loss: 0.9434 - val_accuracy: 0.5720\n",
            "Epoch 27/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.9399 - accuracy: 0.5753 - val_loss: 0.9349 - val_accuracy: 0.5758\n",
            "Epoch 28/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9315 - accuracy: 0.5774 - val_loss: 0.9262 - val_accuracy: 0.5786\n",
            "Epoch 29/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9229 - accuracy: 0.5817 - val_loss: 0.9182 - val_accuracy: 0.5831\n",
            "Epoch 30/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.9146 - accuracy: 0.5851 - val_loss: 0.9098 - val_accuracy: 0.5866\n",
            "Epoch 31/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.9065 - accuracy: 0.5876 - val_loss: 0.9018 - val_accuracy: 0.5900\n",
            "Epoch 32/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8988 - accuracy: 0.5894 - val_loss: 0.8944 - val_accuracy: 0.5922\n",
            "Epoch 33/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8915 - accuracy: 0.5918 - val_loss: 0.8875 - val_accuracy: 0.5932\n",
            "Epoch 34/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8850 - accuracy: 0.5927 - val_loss: 0.8813 - val_accuracy: 0.5933\n",
            "Epoch 35/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8790 - accuracy: 0.5945 - val_loss: 0.8755 - val_accuracy: 0.5936\n",
            "Epoch 36/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8736 - accuracy: 0.5955 - val_loss: 0.8704 - val_accuracy: 0.5938\n",
            "Epoch 37/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8687 - accuracy: 0.5968 - val_loss: 0.8659 - val_accuracy: 0.5944\n",
            "Epoch 38/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8644 - accuracy: 0.5981 - val_loss: 0.8619 - val_accuracy: 0.5964\n",
            "Epoch 39/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8606 - accuracy: 0.5987 - val_loss: 0.8582 - val_accuracy: 0.5971\n",
            "Epoch 40/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8571 - accuracy: 0.6009 - val_loss: 0.8554 - val_accuracy: 0.5992\n",
            "Epoch 41/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8542 - accuracy: 0.6054 - val_loss: 0.8526 - val_accuracy: 0.6050\n",
            "Epoch 42/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8515 - accuracy: 0.6063 - val_loss: 0.8499 - val_accuracy: 0.6053\n",
            "Epoch 43/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8491 - accuracy: 0.6063 - val_loss: 0.8482 - val_accuracy: 0.6052\n",
            "Epoch 44/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8469 - accuracy: 0.6086 - val_loss: 0.8461 - val_accuracy: 0.6068\n",
            "Epoch 45/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8452 - accuracy: 0.6087 - val_loss: 0.8441 - val_accuracy: 0.6076\n",
            "Epoch 46/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8434 - accuracy: 0.6093 - val_loss: 0.8424 - val_accuracy: 0.6067\n",
            "Epoch 47/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8417 - accuracy: 0.6104 - val_loss: 0.8409 - val_accuracy: 0.6076\n",
            "Epoch 48/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8405 - accuracy: 0.6126 - val_loss: 0.8399 - val_accuracy: 0.6084\n",
            "Epoch 49/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8392 - accuracy: 0.6122 - val_loss: 0.8385 - val_accuracy: 0.6084\n",
            "Epoch 50/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8380 - accuracy: 0.6116 - val_loss: 0.8374 - val_accuracy: 0.6108\n",
            "Epoch 51/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8369 - accuracy: 0.6133 - val_loss: 0.8363 - val_accuracy: 0.6119\n",
            "Epoch 52/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8359 - accuracy: 0.6127 - val_loss: 0.8354 - val_accuracy: 0.6126\n",
            "Epoch 53/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8349 - accuracy: 0.6140 - val_loss: 0.8346 - val_accuracy: 0.6125\n",
            "Epoch 54/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8341 - accuracy: 0.6137 - val_loss: 0.8339 - val_accuracy: 0.6129\n",
            "Epoch 55/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8332 - accuracy: 0.6150 - val_loss: 0.8329 - val_accuracy: 0.6138\n",
            "Epoch 56/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8323 - accuracy: 0.6157 - val_loss: 0.8327 - val_accuracy: 0.6155\n",
            "Epoch 57/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8317 - accuracy: 0.6164 - val_loss: 0.8316 - val_accuracy: 0.6147\n",
            "Epoch 58/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8309 - accuracy: 0.6166 - val_loss: 0.8312 - val_accuracy: 0.6158\n",
            "Epoch 59/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8303 - accuracy: 0.6164 - val_loss: 0.8303 - val_accuracy: 0.6149\n",
            "Epoch 60/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8296 - accuracy: 0.6162 - val_loss: 0.8297 - val_accuracy: 0.6164\n",
            "Epoch 61/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8289 - accuracy: 0.6183 - val_loss: 0.8293 - val_accuracy: 0.6158\n",
            "Epoch 62/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8284 - accuracy: 0.6174 - val_loss: 0.8286 - val_accuracy: 0.6190\n",
            "Epoch 63/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8278 - accuracy: 0.6180 - val_loss: 0.8280 - val_accuracy: 0.6188\n",
            "Epoch 64/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8272 - accuracy: 0.6179 - val_loss: 0.8278 - val_accuracy: 0.6169\n",
            "Epoch 65/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8268 - accuracy: 0.6189 - val_loss: 0.8271 - val_accuracy: 0.6185\n",
            "Epoch 66/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8263 - accuracy: 0.6200 - val_loss: 0.8267 - val_accuracy: 0.6191\n",
            "Epoch 67/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8257 - accuracy: 0.6193 - val_loss: 0.8263 - val_accuracy: 0.6184\n",
            "Epoch 68/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8252 - accuracy: 0.6199 - val_loss: 0.8258 - val_accuracy: 0.6187\n",
            "Epoch 69/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8248 - accuracy: 0.6204 - val_loss: 0.8254 - val_accuracy: 0.6179\n",
            "Epoch 70/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8243 - accuracy: 0.6204 - val_loss: 0.8249 - val_accuracy: 0.6201\n",
            "Epoch 71/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8237 - accuracy: 0.6189 - val_loss: 0.8248 - val_accuracy: 0.6185\n",
            "Epoch 72/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8234 - accuracy: 0.6204 - val_loss: 0.8242 - val_accuracy: 0.6179\n",
            "Epoch 73/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8230 - accuracy: 0.6213 - val_loss: 0.8238 - val_accuracy: 0.6193\n",
            "Epoch 74/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8225 - accuracy: 0.6207 - val_loss: 0.8236 - val_accuracy: 0.6193\n",
            "Epoch 75/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8222 - accuracy: 0.6218 - val_loss: 0.8230 - val_accuracy: 0.6201\n",
            "Epoch 76/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8218 - accuracy: 0.6216 - val_loss: 0.8227 - val_accuracy: 0.6211\n",
            "Epoch 77/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8214 - accuracy: 0.6217 - val_loss: 0.8224 - val_accuracy: 0.6211\n",
            "Epoch 78/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8211 - accuracy: 0.6210 - val_loss: 0.8220 - val_accuracy: 0.6214\n",
            "Epoch 79/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8207 - accuracy: 0.6223 - val_loss: 0.8217 - val_accuracy: 0.6214\n",
            "Epoch 80/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8204 - accuracy: 0.6210 - val_loss: 0.8214 - val_accuracy: 0.6220\n",
            "Epoch 81/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8201 - accuracy: 0.6222 - val_loss: 0.8211 - val_accuracy: 0.6223\n",
            "Epoch 82/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8197 - accuracy: 0.6211 - val_loss: 0.8208 - val_accuracy: 0.6229\n",
            "Epoch 83/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8194 - accuracy: 0.6220 - val_loss: 0.8205 - val_accuracy: 0.6225\n",
            "Epoch 84/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8191 - accuracy: 0.6217 - val_loss: 0.8205 - val_accuracy: 0.6219\n",
            "Epoch 85/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8189 - accuracy: 0.6220 - val_loss: 0.8201 - val_accuracy: 0.6219\n",
            "Epoch 86/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8185 - accuracy: 0.6222 - val_loss: 0.8197 - val_accuracy: 0.6229\n",
            "Epoch 87/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8182 - accuracy: 0.6217 - val_loss: 0.8195 - val_accuracy: 0.6225\n",
            "Epoch 88/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8180 - accuracy: 0.6228 - val_loss: 0.8192 - val_accuracy: 0.6223\n",
            "Epoch 89/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8177 - accuracy: 0.6233 - val_loss: 0.8191 - val_accuracy: 0.6219\n",
            "Epoch 90/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8175 - accuracy: 0.6217 - val_loss: 0.8187 - val_accuracy: 0.6225\n",
            "Epoch 91/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8172 - accuracy: 0.6224 - val_loss: 0.8186 - val_accuracy: 0.6220\n",
            "Epoch 92/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8169 - accuracy: 0.6228 - val_loss: 0.8184 - val_accuracy: 0.6225\n",
            "Epoch 93/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8167 - accuracy: 0.6217 - val_loss: 0.8181 - val_accuracy: 0.6226\n",
            "Epoch 94/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8163 - accuracy: 0.6232 - val_loss: 0.8179 - val_accuracy: 0.6222\n",
            "Epoch 95/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8161 - accuracy: 0.6234 - val_loss: 0.8176 - val_accuracy: 0.6226\n",
            "Epoch 96/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8159 - accuracy: 0.6226 - val_loss: 0.8174 - val_accuracy: 0.6226\n",
            "Epoch 97/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8155 - accuracy: 0.6241 - val_loss: 0.8173 - val_accuracy: 0.6225\n",
            "Epoch 98/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8154 - accuracy: 0.6213 - val_loss: 0.8170 - val_accuracy: 0.6222\n",
            "Epoch 99/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8152 - accuracy: 0.6221 - val_loss: 0.8168 - val_accuracy: 0.6229\n",
            "Epoch 100/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8149 - accuracy: 0.6233 - val_loss: 0.8166 - val_accuracy: 0.6236\n",
            "Epoch 101/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8148 - accuracy: 0.6231 - val_loss: 0.8165 - val_accuracy: 0.6240\n",
            "Epoch 102/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8145 - accuracy: 0.6241 - val_loss: 0.8162 - val_accuracy: 0.6229\n",
            "Epoch 103/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8143 - accuracy: 0.6238 - val_loss: 0.8160 - val_accuracy: 0.6234\n",
            "Epoch 104/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8141 - accuracy: 0.6236 - val_loss: 0.8158 - val_accuracy: 0.6228\n",
            "Epoch 105/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8139 - accuracy: 0.6238 - val_loss: 0.8156 - val_accuracy: 0.6229\n",
            "Epoch 106/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8137 - accuracy: 0.6230 - val_loss: 0.8155 - val_accuracy: 0.6240\n",
            "Epoch 107/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8135 - accuracy: 0.6237 - val_loss: 0.8153 - val_accuracy: 0.6245\n",
            "Epoch 108/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8131 - accuracy: 0.6243 - val_loss: 0.8153 - val_accuracy: 0.6217\n",
            "Epoch 109/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8130 - accuracy: 0.6236 - val_loss: 0.8149 - val_accuracy: 0.6245\n",
            "Epoch 110/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8128 - accuracy: 0.6230 - val_loss: 0.8148 - val_accuracy: 0.6234\n",
            "Epoch 111/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8127 - accuracy: 0.6246 - val_loss: 0.8146 - val_accuracy: 0.6246\n",
            "Epoch 112/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8125 - accuracy: 0.6229 - val_loss: 0.8145 - val_accuracy: 0.6252\n",
            "Epoch 113/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8123 - accuracy: 0.6244 - val_loss: 0.8142 - val_accuracy: 0.6229\n",
            "Epoch 114/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8121 - accuracy: 0.6241 - val_loss: 0.8143 - val_accuracy: 0.6257\n",
            "Epoch 115/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8120 - accuracy: 0.6248 - val_loss: 0.8139 - val_accuracy: 0.6248\n",
            "Epoch 116/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8118 - accuracy: 0.6252 - val_loss: 0.8137 - val_accuracy: 0.6234\n",
            "Epoch 117/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8115 - accuracy: 0.6248 - val_loss: 0.8136 - val_accuracy: 0.6231\n",
            "Epoch 118/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8113 - accuracy: 0.6247 - val_loss: 0.8134 - val_accuracy: 0.6222\n",
            "Epoch 119/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8112 - accuracy: 0.6256 - val_loss: 0.8133 - val_accuracy: 0.6236\n",
            "Epoch 120/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8110 - accuracy: 0.6246 - val_loss: 0.8131 - val_accuracy: 0.6228\n",
            "Epoch 121/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8109 - accuracy: 0.6250 - val_loss: 0.8130 - val_accuracy: 0.6234\n",
            "Epoch 122/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8107 - accuracy: 0.6253 - val_loss: 0.8128 - val_accuracy: 0.6246\n",
            "Epoch 123/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8105 - accuracy: 0.6255 - val_loss: 0.8127 - val_accuracy: 0.6248\n",
            "Epoch 124/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8104 - accuracy: 0.6250 - val_loss: 0.8126 - val_accuracy: 0.6258\n",
            "Epoch 125/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8102 - accuracy: 0.6248 - val_loss: 0.8124 - val_accuracy: 0.6254\n",
            "Epoch 126/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8100 - accuracy: 0.6252 - val_loss: 0.8124 - val_accuracy: 0.6257\n",
            "Epoch 127/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8099 - accuracy: 0.6254 - val_loss: 0.8121 - val_accuracy: 0.6251\n",
            "Epoch 128/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8097 - accuracy: 0.6252 - val_loss: 0.8120 - val_accuracy: 0.6255\n",
            "Epoch 129/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8096 - accuracy: 0.6264 - val_loss: 0.8118 - val_accuracy: 0.6249\n",
            "Epoch 130/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8094 - accuracy: 0.6255 - val_loss: 0.8120 - val_accuracy: 0.6255\n",
            "Epoch 131/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8093 - accuracy: 0.6260 - val_loss: 0.8117 - val_accuracy: 0.6255\n",
            "Epoch 132/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8092 - accuracy: 0.6256 - val_loss: 0.8114 - val_accuracy: 0.6255\n",
            "Epoch 133/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8090 - accuracy: 0.6252 - val_loss: 0.8114 - val_accuracy: 0.6260\n",
            "Epoch 134/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8089 - accuracy: 0.6258 - val_loss: 0.8112 - val_accuracy: 0.6249\n",
            "Epoch 135/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8087 - accuracy: 0.6256 - val_loss: 0.8112 - val_accuracy: 0.6243\n",
            "Epoch 136/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8086 - accuracy: 0.6251 - val_loss: 0.8109 - val_accuracy: 0.6252\n",
            "Epoch 137/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8084 - accuracy: 0.6258 - val_loss: 0.8108 - val_accuracy: 0.6257\n",
            "Epoch 138/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8083 - accuracy: 0.6262 - val_loss: 0.8107 - val_accuracy: 0.6257\n",
            "Epoch 139/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8081 - accuracy: 0.6255 - val_loss: 0.8106 - val_accuracy: 0.6260\n",
            "Epoch 140/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8080 - accuracy: 0.6263 - val_loss: 0.8106 - val_accuracy: 0.6255\n",
            "Epoch 141/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8079 - accuracy: 0.6267 - val_loss: 0.8103 - val_accuracy: 0.6263\n",
            "Epoch 142/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8078 - accuracy: 0.6252 - val_loss: 0.8102 - val_accuracy: 0.6261\n",
            "Epoch 143/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8076 - accuracy: 0.6262 - val_loss: 0.8101 - val_accuracy: 0.6263\n",
            "Epoch 144/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8074 - accuracy: 0.6264 - val_loss: 0.8101 - val_accuracy: 0.6252\n",
            "Epoch 145/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8074 - accuracy: 0.6259 - val_loss: 0.8099 - val_accuracy: 0.6261\n",
            "Epoch 146/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8072 - accuracy: 0.6254 - val_loss: 0.8097 - val_accuracy: 0.6267\n",
            "Epoch 147/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8071 - accuracy: 0.6261 - val_loss: 0.8097 - val_accuracy: 0.6269\n",
            "Epoch 148/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8070 - accuracy: 0.6259 - val_loss: 0.8095 - val_accuracy: 0.6272\n",
            "Epoch 149/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8069 - accuracy: 0.6254 - val_loss: 0.8096 - val_accuracy: 0.6272\n",
            "Epoch 150/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8068 - accuracy: 0.6261 - val_loss: 0.8094 - val_accuracy: 0.6267\n",
            "Epoch 151/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8066 - accuracy: 0.6261 - val_loss: 0.8093 - val_accuracy: 0.6277\n",
            "Epoch 152/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8065 - accuracy: 0.6265 - val_loss: 0.8093 - val_accuracy: 0.6251\n",
            "Epoch 153/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8064 - accuracy: 0.6265 - val_loss: 0.8092 - val_accuracy: 0.6249\n",
            "Epoch 154/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8063 - accuracy: 0.6263 - val_loss: 0.8089 - val_accuracy: 0.6278\n",
            "Epoch 155/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8062 - accuracy: 0.6264 - val_loss: 0.8088 - val_accuracy: 0.6275\n",
            "Epoch 156/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8061 - accuracy: 0.6267 - val_loss: 0.8087 - val_accuracy: 0.6281\n",
            "Epoch 157/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8059 - accuracy: 0.6265 - val_loss: 0.8086 - val_accuracy: 0.6274\n",
            "Epoch 158/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8058 - accuracy: 0.6256 - val_loss: 0.8086 - val_accuracy: 0.6281\n",
            "Epoch 159/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8057 - accuracy: 0.6271 - val_loss: 0.8084 - val_accuracy: 0.6274\n",
            "Epoch 160/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8056 - accuracy: 0.6274 - val_loss: 0.8083 - val_accuracy: 0.6280\n",
            "Epoch 161/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8055 - accuracy: 0.6267 - val_loss: 0.8082 - val_accuracy: 0.6280\n",
            "Epoch 162/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8054 - accuracy: 0.6266 - val_loss: 0.8081 - val_accuracy: 0.6280\n",
            "Epoch 163/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8053 - accuracy: 0.6265 - val_loss: 0.8081 - val_accuracy: 0.6286\n",
            "Epoch 164/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8051 - accuracy: 0.6270 - val_loss: 0.8079 - val_accuracy: 0.6277\n",
            "Epoch 165/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8051 - accuracy: 0.6270 - val_loss: 0.8079 - val_accuracy: 0.6277\n",
            "Epoch 166/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8050 - accuracy: 0.6271 - val_loss: 0.8077 - val_accuracy: 0.6289\n",
            "Epoch 167/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8049 - accuracy: 0.6269 - val_loss: 0.8079 - val_accuracy: 0.6287\n",
            "Epoch 168/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8048 - accuracy: 0.6267 - val_loss: 0.8076 - val_accuracy: 0.6292\n",
            "Epoch 169/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8047 - accuracy: 0.6271 - val_loss: 0.8075 - val_accuracy: 0.6286\n",
            "Epoch 170/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8046 - accuracy: 0.6261 - val_loss: 0.8074 - val_accuracy: 0.6281\n",
            "Epoch 171/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8045 - accuracy: 0.6277 - val_loss: 0.8073 - val_accuracy: 0.6277\n",
            "Epoch 172/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8044 - accuracy: 0.6273 - val_loss: 0.8072 - val_accuracy: 0.6281\n",
            "Epoch 173/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8043 - accuracy: 0.6274 - val_loss: 0.8072 - val_accuracy: 0.6289\n",
            "Epoch 174/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8042 - accuracy: 0.6280 - val_loss: 0.8070 - val_accuracy: 0.6277\n",
            "Epoch 175/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8041 - accuracy: 0.6275 - val_loss: 0.8069 - val_accuracy: 0.6286\n",
            "Epoch 176/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8040 - accuracy: 0.6267 - val_loss: 0.8070 - val_accuracy: 0.6292\n",
            "Epoch 177/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8039 - accuracy: 0.6274 - val_loss: 0.8067 - val_accuracy: 0.6275\n",
            "Epoch 178/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8038 - accuracy: 0.6275 - val_loss: 0.8067 - val_accuracy: 0.6292\n",
            "Epoch 179/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8037 - accuracy: 0.6278 - val_loss: 0.8066 - val_accuracy: 0.6289\n",
            "Epoch 180/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8036 - accuracy: 0.6270 - val_loss: 0.8065 - val_accuracy: 0.6271\n",
            "Epoch 181/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8035 - accuracy: 0.6264 - val_loss: 0.8064 - val_accuracy: 0.6278\n",
            "Epoch 182/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8034 - accuracy: 0.6276 - val_loss: 0.8064 - val_accuracy: 0.6281\n",
            "Epoch 183/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8034 - accuracy: 0.6270 - val_loss: 0.8064 - val_accuracy: 0.6290\n",
            "Epoch 184/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8033 - accuracy: 0.6271 - val_loss: 0.8062 - val_accuracy: 0.6280\n",
            "Epoch 185/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8032 - accuracy: 0.6278 - val_loss: 0.8062 - val_accuracy: 0.6283\n",
            "Epoch 186/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8031 - accuracy: 0.6268 - val_loss: 0.8061 - val_accuracy: 0.6281\n",
            "Epoch 187/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8030 - accuracy: 0.6275 - val_loss: 0.8060 - val_accuracy: 0.6283\n",
            "Epoch 188/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8029 - accuracy: 0.6273 - val_loss: 0.8059 - val_accuracy: 0.6280\n",
            "Epoch 189/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8028 - accuracy: 0.6282 - val_loss: 0.8060 - val_accuracy: 0.6290\n",
            "Epoch 190/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8027 - accuracy: 0.6273 - val_loss: 0.8058 - val_accuracy: 0.6283\n",
            "Epoch 191/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8027 - accuracy: 0.6272 - val_loss: 0.8057 - val_accuracy: 0.6290\n",
            "Epoch 192/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8026 - accuracy: 0.6272 - val_loss: 0.8056 - val_accuracy: 0.6289\n",
            "Epoch 193/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8025 - accuracy: 0.6275 - val_loss: 0.8055 - val_accuracy: 0.6277\n",
            "Epoch 194/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8024 - accuracy: 0.6280 - val_loss: 0.8056 - val_accuracy: 0.6295\n",
            "Epoch 195/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8023 - accuracy: 0.6266 - val_loss: 0.8056 - val_accuracy: 0.6302\n",
            "Epoch 196/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8023 - accuracy: 0.6272 - val_loss: 0.8053 - val_accuracy: 0.6274\n",
            "Epoch 197/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8022 - accuracy: 0.6278 - val_loss: 0.8052 - val_accuracy: 0.6281\n",
            "Epoch 198/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8021 - accuracy: 0.6270 - val_loss: 0.8053 - val_accuracy: 0.6289\n",
            "Epoch 199/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8020 - accuracy: 0.6275 - val_loss: 0.8051 - val_accuracy: 0.6271\n",
            "Epoch 200/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8019 - accuracy: 0.6275 - val_loss: 0.8050 - val_accuracy: 0.6281\n",
            "Epoch 201/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8018 - accuracy: 0.6276 - val_loss: 0.8050 - val_accuracy: 0.6283\n",
            "Epoch 202/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8018 - accuracy: 0.6276 - val_loss: 0.8049 - val_accuracy: 0.6284\n",
            "Epoch 203/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8017 - accuracy: 0.6269 - val_loss: 0.8048 - val_accuracy: 0.6275\n",
            "Epoch 204/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8015 - accuracy: 0.6276 - val_loss: 0.8048 - val_accuracy: 0.6289\n",
            "Epoch 205/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8015 - accuracy: 0.6276 - val_loss: 0.8047 - val_accuracy: 0.6277\n",
            "Epoch 206/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8015 - accuracy: 0.6273 - val_loss: 0.8046 - val_accuracy: 0.6292\n",
            "Epoch 207/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8014 - accuracy: 0.6278 - val_loss: 0.8046 - val_accuracy: 0.6286\n",
            "Epoch 208/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8013 - accuracy: 0.6280 - val_loss: 0.8047 - val_accuracy: 0.6302\n",
            "Epoch 209/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8012 - accuracy: 0.6267 - val_loss: 0.8045 - val_accuracy: 0.6289\n",
            "Epoch 210/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8011 - accuracy: 0.6276 - val_loss: 0.8045 - val_accuracy: 0.6296\n",
            "Epoch 211/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8011 - accuracy: 0.6276 - val_loss: 0.8044 - val_accuracy: 0.6296\n",
            "Epoch 212/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.8010 - accuracy: 0.6275 - val_loss: 0.8042 - val_accuracy: 0.6295\n",
            "Epoch 213/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8010 - accuracy: 0.6278 - val_loss: 0.8043 - val_accuracy: 0.6298\n",
            "Epoch 214/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8008 - accuracy: 0.6274 - val_loss: 0.8044 - val_accuracy: 0.6298\n",
            "Epoch 215/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8009 - accuracy: 0.6282 - val_loss: 0.8041 - val_accuracy: 0.6292\n",
            "Epoch 216/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8007 - accuracy: 0.6275 - val_loss: 0.8041 - val_accuracy: 0.6299\n",
            "Epoch 217/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8007 - accuracy: 0.6278 - val_loss: 0.8039 - val_accuracy: 0.6301\n",
            "Epoch 218/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8006 - accuracy: 0.6277 - val_loss: 0.8039 - val_accuracy: 0.6298\n",
            "Epoch 219/600\n",
            "237/237 [==============================] - 2s 6ms/step - loss: 0.8005 - accuracy: 0.6282 - val_loss: 0.8040 - val_accuracy: 0.6296\n",
            "Epoch 220/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8005 - accuracy: 0.6285 - val_loss: 0.8038 - val_accuracy: 0.6299\n",
            "Epoch 221/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8004 - accuracy: 0.6280 - val_loss: 0.8039 - val_accuracy: 0.6301\n",
            "Epoch 222/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8004 - accuracy: 0.6285 - val_loss: 0.8036 - val_accuracy: 0.6305\n",
            "Epoch 223/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8002 - accuracy: 0.6285 - val_loss: 0.8036 - val_accuracy: 0.6302\n",
            "Epoch 224/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8002 - accuracy: 0.6282 - val_loss: 0.8035 - val_accuracy: 0.6304\n",
            "Epoch 225/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8001 - accuracy: 0.6281 - val_loss: 0.8035 - val_accuracy: 0.6298\n",
            "Epoch 226/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8001 - accuracy: 0.6284 - val_loss: 0.8034 - val_accuracy: 0.6307\n",
            "Epoch 227/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8000 - accuracy: 0.6293 - val_loss: 0.8035 - val_accuracy: 0.6302\n",
            "Epoch 228/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.8000 - accuracy: 0.6283 - val_loss: 0.8033 - val_accuracy: 0.6298\n",
            "Epoch 229/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7999 - accuracy: 0.6283 - val_loss: 0.8033 - val_accuracy: 0.6307\n",
            "Epoch 230/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7999 - accuracy: 0.6282 - val_loss: 0.8032 - val_accuracy: 0.6312\n",
            "Epoch 231/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7997 - accuracy: 0.6287 - val_loss: 0.8032 - val_accuracy: 0.6301\n",
            "Epoch 232/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7997 - accuracy: 0.6289 - val_loss: 0.8031 - val_accuracy: 0.6304\n",
            "Epoch 233/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7996 - accuracy: 0.6282 - val_loss: 0.8030 - val_accuracy: 0.6301\n",
            "Epoch 234/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7996 - accuracy: 0.6286 - val_loss: 0.8030 - val_accuracy: 0.6313\n",
            "Epoch 235/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7995 - accuracy: 0.6289 - val_loss: 0.8029 - val_accuracy: 0.6304\n",
            "Epoch 236/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7995 - accuracy: 0.6289 - val_loss: 0.8029 - val_accuracy: 0.6305\n",
            "Epoch 237/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7994 - accuracy: 0.6296 - val_loss: 0.8028 - val_accuracy: 0.6307\n",
            "Epoch 238/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7993 - accuracy: 0.6289 - val_loss: 0.8028 - val_accuracy: 0.6310\n",
            "Epoch 239/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7993 - accuracy: 0.6291 - val_loss: 0.8027 - val_accuracy: 0.6307\n",
            "Epoch 240/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7993 - accuracy: 0.6295 - val_loss: 0.8027 - val_accuracy: 0.6310\n",
            "Epoch 241/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7992 - accuracy: 0.6291 - val_loss: 0.8027 - val_accuracy: 0.6319\n",
            "Epoch 242/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7991 - accuracy: 0.6289 - val_loss: 0.8026 - val_accuracy: 0.6310\n",
            "Epoch 243/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7991 - accuracy: 0.6301 - val_loss: 0.8025 - val_accuracy: 0.6312\n",
            "Epoch 244/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7990 - accuracy: 0.6292 - val_loss: 0.8024 - val_accuracy: 0.6310\n",
            "Epoch 245/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7990 - accuracy: 0.6293 - val_loss: 0.8025 - val_accuracy: 0.6318\n",
            "Epoch 246/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7989 - accuracy: 0.6293 - val_loss: 0.8024 - val_accuracy: 0.6313\n",
            "Epoch 247/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7989 - accuracy: 0.6289 - val_loss: 0.8023 - val_accuracy: 0.6312\n",
            "Epoch 248/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7988 - accuracy: 0.6293 - val_loss: 0.8023 - val_accuracy: 0.6302\n",
            "Epoch 249/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7987 - accuracy: 0.6295 - val_loss: 0.8022 - val_accuracy: 0.6305\n",
            "Epoch 250/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7987 - accuracy: 0.6296 - val_loss: 0.8022 - val_accuracy: 0.6318\n",
            "Epoch 251/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7986 - accuracy: 0.6296 - val_loss: 0.8022 - val_accuracy: 0.6315\n",
            "Epoch 252/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7986 - accuracy: 0.6295 - val_loss: 0.8021 - val_accuracy: 0.6315\n",
            "Epoch 253/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7985 - accuracy: 0.6293 - val_loss: 0.8020 - val_accuracy: 0.6313\n",
            "Epoch 254/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7985 - accuracy: 0.6297 - val_loss: 0.8020 - val_accuracy: 0.6319\n",
            "Epoch 255/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7984 - accuracy: 0.6295 - val_loss: 0.8020 - val_accuracy: 0.6318\n",
            "Epoch 256/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7984 - accuracy: 0.6297 - val_loss: 0.8019 - val_accuracy: 0.6316\n",
            "Epoch 257/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7983 - accuracy: 0.6295 - val_loss: 0.8021 - val_accuracy: 0.6318\n",
            "Epoch 258/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7983 - accuracy: 0.6294 - val_loss: 0.8018 - val_accuracy: 0.6324\n",
            "Epoch 259/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7982 - accuracy: 0.6295 - val_loss: 0.8018 - val_accuracy: 0.6310\n",
            "Epoch 260/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7982 - accuracy: 0.6293 - val_loss: 0.8018 - val_accuracy: 0.6321\n",
            "Epoch 261/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7980 - accuracy: 0.6295 - val_loss: 0.8019 - val_accuracy: 0.6319\n",
            "Epoch 262/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7981 - accuracy: 0.6293 - val_loss: 0.8017 - val_accuracy: 0.6322\n",
            "Epoch 263/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7980 - accuracy: 0.6294 - val_loss: 0.8016 - val_accuracy: 0.6324\n",
            "Epoch 264/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7979 - accuracy: 0.6295 - val_loss: 0.8016 - val_accuracy: 0.6322\n",
            "Epoch 265/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7979 - accuracy: 0.6295 - val_loss: 0.8015 - val_accuracy: 0.6321\n",
            "Epoch 266/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7978 - accuracy: 0.6294 - val_loss: 0.8015 - val_accuracy: 0.6319\n",
            "Epoch 267/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7978 - accuracy: 0.6293 - val_loss: 0.8014 - val_accuracy: 0.6331\n",
            "Epoch 268/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7977 - accuracy: 0.6295 - val_loss: 0.8014 - val_accuracy: 0.6312\n",
            "Epoch 269/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7977 - accuracy: 0.6297 - val_loss: 0.8013 - val_accuracy: 0.6328\n",
            "Epoch 270/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7977 - accuracy: 0.6301 - val_loss: 0.8013 - val_accuracy: 0.6319\n",
            "Epoch 271/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7976 - accuracy: 0.6296 - val_loss: 0.8013 - val_accuracy: 0.6331\n",
            "Epoch 272/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7976 - accuracy: 0.6295 - val_loss: 0.8012 - val_accuracy: 0.6319\n",
            "Epoch 273/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7975 - accuracy: 0.6305 - val_loss: 0.8011 - val_accuracy: 0.6313\n",
            "Epoch 274/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7975 - accuracy: 0.6299 - val_loss: 0.8011 - val_accuracy: 0.6318\n",
            "Epoch 275/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7974 - accuracy: 0.6302 - val_loss: 0.8011 - val_accuracy: 0.6324\n",
            "Epoch 276/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7974 - accuracy: 0.6299 - val_loss: 0.8011 - val_accuracy: 0.6325\n",
            "Epoch 277/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7974 - accuracy: 0.6297 - val_loss: 0.8010 - val_accuracy: 0.6322\n",
            "Epoch 278/600\n",
            "237/237 [==============================] - 2s 6ms/step - loss: 0.7973 - accuracy: 0.6292 - val_loss: 0.8010 - val_accuracy: 0.6318\n",
            "Epoch 279/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7972 - accuracy: 0.6301 - val_loss: 0.8010 - val_accuracy: 0.6333\n",
            "Epoch 280/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7972 - accuracy: 0.6293 - val_loss: 0.8010 - val_accuracy: 0.6330\n",
            "Epoch 281/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7972 - accuracy: 0.6294 - val_loss: 0.8008 - val_accuracy: 0.6322\n",
            "Epoch 282/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7971 - accuracy: 0.6291 - val_loss: 0.8008 - val_accuracy: 0.6322\n",
            "Epoch 283/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7971 - accuracy: 0.6298 - val_loss: 0.8008 - val_accuracy: 0.6324\n",
            "Epoch 284/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7970 - accuracy: 0.6295 - val_loss: 0.8007 - val_accuracy: 0.6318\n",
            "Epoch 285/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7970 - accuracy: 0.6301 - val_loss: 0.8007 - val_accuracy: 0.6316\n",
            "Epoch 286/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7970 - accuracy: 0.6299 - val_loss: 0.8006 - val_accuracy: 0.6324\n",
            "Epoch 287/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7969 - accuracy: 0.6299 - val_loss: 0.8006 - val_accuracy: 0.6327\n",
            "Epoch 288/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7969 - accuracy: 0.6306 - val_loss: 0.8007 - val_accuracy: 0.6328\n",
            "Epoch 289/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7968 - accuracy: 0.6303 - val_loss: 0.8006 - val_accuracy: 0.6327\n",
            "Epoch 290/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7968 - accuracy: 0.6296 - val_loss: 0.8005 - val_accuracy: 0.6318\n",
            "Epoch 291/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7967 - accuracy: 0.6306 - val_loss: 0.8005 - val_accuracy: 0.6322\n",
            "Epoch 292/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7967 - accuracy: 0.6304 - val_loss: 0.8006 - val_accuracy: 0.6330\n",
            "Epoch 293/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7967 - accuracy: 0.6293 - val_loss: 0.8005 - val_accuracy: 0.6315\n",
            "Epoch 294/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7965 - accuracy: 0.6308 - val_loss: 0.8007 - val_accuracy: 0.6321\n",
            "Epoch 295/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7966 - accuracy: 0.6300 - val_loss: 0.8003 - val_accuracy: 0.6322\n",
            "Epoch 296/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7966 - accuracy: 0.6296 - val_loss: 0.8003 - val_accuracy: 0.6324\n",
            "Epoch 297/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7965 - accuracy: 0.6303 - val_loss: 0.8003 - val_accuracy: 0.6322\n",
            "Epoch 298/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7965 - accuracy: 0.6297 - val_loss: 0.8002 - val_accuracy: 0.6325\n",
            "Epoch 299/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7965 - accuracy: 0.6298 - val_loss: 0.8002 - val_accuracy: 0.6325\n",
            "Epoch 300/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7964 - accuracy: 0.6297 - val_loss: 0.8002 - val_accuracy: 0.6328\n",
            "Epoch 301/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7964 - accuracy: 0.6304 - val_loss: 0.8002 - val_accuracy: 0.6324\n",
            "Epoch 302/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7963 - accuracy: 0.6296 - val_loss: 0.8002 - val_accuracy: 0.6328\n",
            "Epoch 303/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7963 - accuracy: 0.6305 - val_loss: 0.8002 - val_accuracy: 0.6319\n",
            "Epoch 304/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7963 - accuracy: 0.6298 - val_loss: 0.8001 - val_accuracy: 0.6331\n",
            "Epoch 305/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7962 - accuracy: 0.6308 - val_loss: 0.8001 - val_accuracy: 0.6331\n",
            "Epoch 306/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7962 - accuracy: 0.6300 - val_loss: 0.8000 - val_accuracy: 0.6322\n",
            "Epoch 307/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7962 - accuracy: 0.6303 - val_loss: 0.8000 - val_accuracy: 0.6328\n",
            "Epoch 308/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7961 - accuracy: 0.6304 - val_loss: 0.8001 - val_accuracy: 0.6325\n",
            "Epoch 309/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7960 - accuracy: 0.6301 - val_loss: 0.7999 - val_accuracy: 0.6321\n",
            "Epoch 310/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7960 - accuracy: 0.6306 - val_loss: 0.7999 - val_accuracy: 0.6328\n",
            "Epoch 311/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7960 - accuracy: 0.6305 - val_loss: 0.7999 - val_accuracy: 0.6334\n",
            "Epoch 312/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7960 - accuracy: 0.6301 - val_loss: 0.7998 - val_accuracy: 0.6328\n",
            "Epoch 313/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7959 - accuracy: 0.6308 - val_loss: 0.7998 - val_accuracy: 0.6328\n",
            "Epoch 314/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7959 - accuracy: 0.6298 - val_loss: 0.7998 - val_accuracy: 0.6325\n",
            "Epoch 315/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7959 - accuracy: 0.6300 - val_loss: 0.7997 - val_accuracy: 0.6325\n",
            "Epoch 316/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7958 - accuracy: 0.6294 - val_loss: 0.7997 - val_accuracy: 0.6333\n",
            "Epoch 317/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7958 - accuracy: 0.6300 - val_loss: 0.7997 - val_accuracy: 0.6330\n",
            "Epoch 318/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7957 - accuracy: 0.6311 - val_loss: 0.8002 - val_accuracy: 0.6331\n",
            "Epoch 319/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7958 - accuracy: 0.6304 - val_loss: 0.7997 - val_accuracy: 0.6339\n",
            "Epoch 320/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7957 - accuracy: 0.6297 - val_loss: 0.7996 - val_accuracy: 0.6330\n",
            "Epoch 321/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7956 - accuracy: 0.6306 - val_loss: 0.7996 - val_accuracy: 0.6327\n",
            "Epoch 322/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7956 - accuracy: 0.6304 - val_loss: 0.7995 - val_accuracy: 0.6331\n",
            "Epoch 323/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7956 - accuracy: 0.6299 - val_loss: 0.7995 - val_accuracy: 0.6325\n",
            "Epoch 324/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7956 - accuracy: 0.6303 - val_loss: 0.7995 - val_accuracy: 0.6328\n",
            "Epoch 325/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7955 - accuracy: 0.6304 - val_loss: 0.7996 - val_accuracy: 0.6331\n",
            "Epoch 326/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7955 - accuracy: 0.6315 - val_loss: 0.7994 - val_accuracy: 0.6327\n",
            "Epoch 327/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7955 - accuracy: 0.6305 - val_loss: 0.7994 - val_accuracy: 0.6339\n",
            "Epoch 328/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7954 - accuracy: 0.6297 - val_loss: 0.7994 - val_accuracy: 0.6334\n",
            "Epoch 329/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7954 - accuracy: 0.6301 - val_loss: 0.7993 - val_accuracy: 0.6321\n",
            "Epoch 330/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7954 - accuracy: 0.6308 - val_loss: 0.7993 - val_accuracy: 0.6321\n",
            "Epoch 331/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7953 - accuracy: 0.6308 - val_loss: 0.7993 - val_accuracy: 0.6324\n",
            "Epoch 332/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7953 - accuracy: 0.6302 - val_loss: 0.7993 - val_accuracy: 0.6325\n",
            "Epoch 333/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7952 - accuracy: 0.6306 - val_loss: 0.7992 - val_accuracy: 0.6327\n",
            "Epoch 334/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7952 - accuracy: 0.6305 - val_loss: 0.7992 - val_accuracy: 0.6328\n",
            "Epoch 335/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7952 - accuracy: 0.6299 - val_loss: 0.7992 - val_accuracy: 0.6328\n",
            "Epoch 336/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7952 - accuracy: 0.6306 - val_loss: 0.7993 - val_accuracy: 0.6331\n",
            "Epoch 337/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7952 - accuracy: 0.6313 - val_loss: 0.7992 - val_accuracy: 0.6328\n",
            "Epoch 338/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7951 - accuracy: 0.6304 - val_loss: 0.7992 - val_accuracy: 0.6331\n",
            "Epoch 339/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7951 - accuracy: 0.6304 - val_loss: 0.7991 - val_accuracy: 0.6331\n",
            "Epoch 340/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7950 - accuracy: 0.6302 - val_loss: 0.7991 - val_accuracy: 0.6328\n",
            "Epoch 341/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7950 - accuracy: 0.6302 - val_loss: 0.7991 - val_accuracy: 0.6333\n",
            "Epoch 342/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7950 - accuracy: 0.6302 - val_loss: 0.7990 - val_accuracy: 0.6328\n",
            "Epoch 343/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7950 - accuracy: 0.6308 - val_loss: 0.7990 - val_accuracy: 0.6333\n",
            "Epoch 344/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7949 - accuracy: 0.6310 - val_loss: 0.7990 - val_accuracy: 0.6334\n",
            "Epoch 345/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7949 - accuracy: 0.6300 - val_loss: 0.7990 - val_accuracy: 0.6328\n",
            "Epoch 346/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7949 - accuracy: 0.6316 - val_loss: 0.7991 - val_accuracy: 0.6324\n",
            "Epoch 347/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7949 - accuracy: 0.6308 - val_loss: 0.7989 - val_accuracy: 0.6330\n",
            "Epoch 348/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7948 - accuracy: 0.6303 - val_loss: 0.7989 - val_accuracy: 0.6325\n",
            "Epoch 349/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7948 - accuracy: 0.6308 - val_loss: 0.7989 - val_accuracy: 0.6330\n",
            "Epoch 350/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7948 - accuracy: 0.6312 - val_loss: 0.7990 - val_accuracy: 0.6325\n",
            "Epoch 351/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7948 - accuracy: 0.6309 - val_loss: 0.7988 - val_accuracy: 0.6331\n",
            "Epoch 352/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7947 - accuracy: 0.6306 - val_loss: 0.7988 - val_accuracy: 0.6330\n",
            "Epoch 353/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7947 - accuracy: 0.6314 - val_loss: 0.7989 - val_accuracy: 0.6325\n",
            "Epoch 354/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7947 - accuracy: 0.6299 - val_loss: 0.7989 - val_accuracy: 0.6325\n",
            "Epoch 355/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7946 - accuracy: 0.6313 - val_loss: 0.7988 - val_accuracy: 0.6324\n",
            "Epoch 356/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7946 - accuracy: 0.6312 - val_loss: 0.7987 - val_accuracy: 0.6330\n",
            "Epoch 357/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7946 - accuracy: 0.6308 - val_loss: 0.7987 - val_accuracy: 0.6333\n",
            "Epoch 358/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7945 - accuracy: 0.6310 - val_loss: 0.7988 - val_accuracy: 0.6322\n",
            "Epoch 359/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7945 - accuracy: 0.6303 - val_loss: 0.7986 - val_accuracy: 0.6331\n",
            "Epoch 360/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7945 - accuracy: 0.6309 - val_loss: 0.7986 - val_accuracy: 0.6330\n",
            "Epoch 361/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7945 - accuracy: 0.6306 - val_loss: 0.7987 - val_accuracy: 0.6331\n",
            "Epoch 362/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7945 - accuracy: 0.6311 - val_loss: 0.7986 - val_accuracy: 0.6330\n",
            "Epoch 363/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7944 - accuracy: 0.6304 - val_loss: 0.7986 - val_accuracy: 0.6330\n",
            "Epoch 364/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7943 - accuracy: 0.6310 - val_loss: 0.7989 - val_accuracy: 0.6321\n",
            "Epoch 365/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7944 - accuracy: 0.6302 - val_loss: 0.7985 - val_accuracy: 0.6331\n",
            "Epoch 366/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7943 - accuracy: 0.6315 - val_loss: 0.7986 - val_accuracy: 0.6325\n",
            "Epoch 367/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7943 - accuracy: 0.6311 - val_loss: 0.7986 - val_accuracy: 0.6324\n",
            "Epoch 368/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7943 - accuracy: 0.6309 - val_loss: 0.7987 - val_accuracy: 0.6319\n",
            "Epoch 369/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7943 - accuracy: 0.6305 - val_loss: 0.7985 - val_accuracy: 0.6327\n",
            "Epoch 370/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7942 - accuracy: 0.6312 - val_loss: 0.7986 - val_accuracy: 0.6325\n",
            "Epoch 371/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7942 - accuracy: 0.6308 - val_loss: 0.7984 - val_accuracy: 0.6328\n",
            "Epoch 372/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7942 - accuracy: 0.6306 - val_loss: 0.7984 - val_accuracy: 0.6327\n",
            "Epoch 373/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7942 - accuracy: 0.6304 - val_loss: 0.7984 - val_accuracy: 0.6325\n",
            "Epoch 374/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7941 - accuracy: 0.6318 - val_loss: 0.7984 - val_accuracy: 0.6325\n",
            "Epoch 375/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7941 - accuracy: 0.6313 - val_loss: 0.7984 - val_accuracy: 0.6328\n",
            "Epoch 376/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7941 - accuracy: 0.6312 - val_loss: 0.7984 - val_accuracy: 0.6324\n",
            "Epoch 377/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7941 - accuracy: 0.6308 - val_loss: 0.7983 - val_accuracy: 0.6325\n",
            "Epoch 378/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7940 - accuracy: 0.6312 - val_loss: 0.7983 - val_accuracy: 0.6330\n",
            "Epoch 379/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7940 - accuracy: 0.6319 - val_loss: 0.7983 - val_accuracy: 0.6334\n",
            "Epoch 380/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7940 - accuracy: 0.6307 - val_loss: 0.7983 - val_accuracy: 0.6333\n",
            "Epoch 381/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7940 - accuracy: 0.6314 - val_loss: 0.7982 - val_accuracy: 0.6330\n",
            "Epoch 382/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7939 - accuracy: 0.6308 - val_loss: 0.7983 - val_accuracy: 0.6334\n",
            "Epoch 383/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7939 - accuracy: 0.6303 - val_loss: 0.7982 - val_accuracy: 0.6330\n",
            "Epoch 384/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7939 - accuracy: 0.6314 - val_loss: 0.7982 - val_accuracy: 0.6331\n",
            "Epoch 385/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7939 - accuracy: 0.6313 - val_loss: 0.7982 - val_accuracy: 0.6327\n",
            "Epoch 386/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7938 - accuracy: 0.6304 - val_loss: 0.7982 - val_accuracy: 0.6328\n",
            "Epoch 387/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7938 - accuracy: 0.6312 - val_loss: 0.7983 - val_accuracy: 0.6325\n",
            "Epoch 388/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7938 - accuracy: 0.6314 - val_loss: 0.7981 - val_accuracy: 0.6331\n",
            "Epoch 389/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7938 - accuracy: 0.6313 - val_loss: 0.7982 - val_accuracy: 0.6327\n",
            "Epoch 390/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7938 - accuracy: 0.6311 - val_loss: 0.7981 - val_accuracy: 0.6327\n",
            "Epoch 391/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7938 - accuracy: 0.6312 - val_loss: 0.7981 - val_accuracy: 0.6331\n",
            "Epoch 392/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7937 - accuracy: 0.6316 - val_loss: 0.7980 - val_accuracy: 0.6319\n",
            "Epoch 393/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7937 - accuracy: 0.6310 - val_loss: 0.7980 - val_accuracy: 0.6321\n",
            "Epoch 394/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7937 - accuracy: 0.6315 - val_loss: 0.7980 - val_accuracy: 0.6316\n",
            "Epoch 395/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7936 - accuracy: 0.6316 - val_loss: 0.7980 - val_accuracy: 0.6328\n",
            "Epoch 396/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7936 - accuracy: 0.6316 - val_loss: 0.7980 - val_accuracy: 0.6340\n",
            "Epoch 397/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7936 - accuracy: 0.6310 - val_loss: 0.7980 - val_accuracy: 0.6318\n",
            "Epoch 398/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7936 - accuracy: 0.6310 - val_loss: 0.7981 - val_accuracy: 0.6328\n",
            "Epoch 399/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7936 - accuracy: 0.6316 - val_loss: 0.7979 - val_accuracy: 0.6327\n",
            "Epoch 400/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7935 - accuracy: 0.6314 - val_loss: 0.7979 - val_accuracy: 0.6322\n",
            "Epoch 401/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7936 - accuracy: 0.6312 - val_loss: 0.7979 - val_accuracy: 0.6330\n",
            "Epoch 402/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7935 - accuracy: 0.6318 - val_loss: 0.7979 - val_accuracy: 0.6321\n",
            "Epoch 403/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7933 - accuracy: 0.6320 - val_loss: 0.7980 - val_accuracy: 0.6324\n",
            "Epoch 404/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7935 - accuracy: 0.6311 - val_loss: 0.7979 - val_accuracy: 0.6315\n",
            "Epoch 405/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7934 - accuracy: 0.6312 - val_loss: 0.7979 - val_accuracy: 0.6330\n",
            "Epoch 406/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7934 - accuracy: 0.6314 - val_loss: 0.7979 - val_accuracy: 0.6310\n",
            "Epoch 407/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7934 - accuracy: 0.6310 - val_loss: 0.7979 - val_accuracy: 0.6327\n",
            "Epoch 408/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7934 - accuracy: 0.6316 - val_loss: 0.7979 - val_accuracy: 0.6333\n",
            "Epoch 409/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7933 - accuracy: 0.6312 - val_loss: 0.7978 - val_accuracy: 0.6328\n",
            "Epoch 410/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7933 - accuracy: 0.6310 - val_loss: 0.7978 - val_accuracy: 0.6333\n",
            "Epoch 411/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7933 - accuracy: 0.6319 - val_loss: 0.7978 - val_accuracy: 0.6328\n",
            "Epoch 412/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7933 - accuracy: 0.6318 - val_loss: 0.7978 - val_accuracy: 0.6330\n",
            "Epoch 413/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7932 - accuracy: 0.6320 - val_loss: 0.7979 - val_accuracy: 0.6325\n",
            "Epoch 414/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7933 - accuracy: 0.6311 - val_loss: 0.7977 - val_accuracy: 0.6327\n",
            "Epoch 415/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7933 - accuracy: 0.6310 - val_loss: 0.7977 - val_accuracy: 0.6315\n",
            "Epoch 416/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7932 - accuracy: 0.6316 - val_loss: 0.7977 - val_accuracy: 0.6325\n",
            "Epoch 417/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7932 - accuracy: 0.6319 - val_loss: 0.7977 - val_accuracy: 0.6310\n",
            "Epoch 418/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7932 - accuracy: 0.6318 - val_loss: 0.7977 - val_accuracy: 0.6325\n",
            "Epoch 419/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7932 - accuracy: 0.6321 - val_loss: 0.7976 - val_accuracy: 0.6324\n",
            "Epoch 420/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7932 - accuracy: 0.6316 - val_loss: 0.7977 - val_accuracy: 0.6325\n",
            "Epoch 421/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7931 - accuracy: 0.6316 - val_loss: 0.7976 - val_accuracy: 0.6315\n",
            "Epoch 422/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7931 - accuracy: 0.6318 - val_loss: 0.7976 - val_accuracy: 0.6327\n",
            "Epoch 423/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7931 - accuracy: 0.6315 - val_loss: 0.7976 - val_accuracy: 0.6324\n",
            "Epoch 424/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7931 - accuracy: 0.6318 - val_loss: 0.7976 - val_accuracy: 0.6324\n",
            "Epoch 425/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7930 - accuracy: 0.6318 - val_loss: 0.7977 - val_accuracy: 0.6322\n",
            "Epoch 426/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7930 - accuracy: 0.6316 - val_loss: 0.7977 - val_accuracy: 0.6325\n",
            "Epoch 427/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7930 - accuracy: 0.6314 - val_loss: 0.7976 - val_accuracy: 0.6321\n",
            "Epoch 428/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7930 - accuracy: 0.6316 - val_loss: 0.7975 - val_accuracy: 0.6322\n",
            "Epoch 429/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7930 - accuracy: 0.6319 - val_loss: 0.7975 - val_accuracy: 0.6325\n",
            "Epoch 430/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7930 - accuracy: 0.6317 - val_loss: 0.7975 - val_accuracy: 0.6324\n",
            "Epoch 431/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7929 - accuracy: 0.6324 - val_loss: 0.7977 - val_accuracy: 0.6328\n",
            "Epoch 432/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7929 - accuracy: 0.6324 - val_loss: 0.7976 - val_accuracy: 0.6328\n",
            "Epoch 433/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7929 - accuracy: 0.6320 - val_loss: 0.7975 - val_accuracy: 0.6321\n",
            "Epoch 434/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7929 - accuracy: 0.6318 - val_loss: 0.7975 - val_accuracy: 0.6319\n",
            "Epoch 435/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7928 - accuracy: 0.6321 - val_loss: 0.7975 - val_accuracy: 0.6315\n",
            "Epoch 436/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7929 - accuracy: 0.6324 - val_loss: 0.7975 - val_accuracy: 0.6322\n",
            "Epoch 437/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7928 - accuracy: 0.6325 - val_loss: 0.7975 - val_accuracy: 0.6318\n",
            "Epoch 438/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7928 - accuracy: 0.6318 - val_loss: 0.7977 - val_accuracy: 0.6333\n",
            "Epoch 439/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7928 - accuracy: 0.6316 - val_loss: 0.7975 - val_accuracy: 0.6327\n",
            "Epoch 440/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7928 - accuracy: 0.6321 - val_loss: 0.7974 - val_accuracy: 0.6322\n",
            "Epoch 441/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7928 - accuracy: 0.6321 - val_loss: 0.7974 - val_accuracy: 0.6325\n",
            "Epoch 442/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7928 - accuracy: 0.6316 - val_loss: 0.7975 - val_accuracy: 0.6330\n",
            "Epoch 443/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7928 - accuracy: 0.6318 - val_loss: 0.7974 - val_accuracy: 0.6327\n",
            "Epoch 444/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7927 - accuracy: 0.6319 - val_loss: 0.7973 - val_accuracy: 0.6318\n",
            "Epoch 445/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7927 - accuracy: 0.6323 - val_loss: 0.7973 - val_accuracy: 0.6318\n",
            "Epoch 446/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7927 - accuracy: 0.6319 - val_loss: 0.7974 - val_accuracy: 0.6313\n",
            "Epoch 447/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7927 - accuracy: 0.6323 - val_loss: 0.7973 - val_accuracy: 0.6322\n",
            "Epoch 448/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7926 - accuracy: 0.6318 - val_loss: 0.7973 - val_accuracy: 0.6319\n",
            "Epoch 449/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7926 - accuracy: 0.6329 - val_loss: 0.7973 - val_accuracy: 0.6319\n",
            "Epoch 450/600\n",
            "237/237 [==============================] - 2s 6ms/step - loss: 0.7926 - accuracy: 0.6321 - val_loss: 0.7973 - val_accuracy: 0.6321\n",
            "Epoch 451/600\n",
            "237/237 [==============================] - 2s 6ms/step - loss: 0.7926 - accuracy: 0.6321 - val_loss: 0.7975 - val_accuracy: 0.6327\n",
            "Epoch 452/600\n",
            "237/237 [==============================] - 2s 6ms/step - loss: 0.7926 - accuracy: 0.6329 - val_loss: 0.7973 - val_accuracy: 0.6324\n",
            "Epoch 453/600\n",
            "237/237 [==============================] - 2s 6ms/step - loss: 0.7925 - accuracy: 0.6319 - val_loss: 0.7973 - val_accuracy: 0.6318\n",
            "Epoch 454/600\n",
            "237/237 [==============================] - 2s 6ms/step - loss: 0.7925 - accuracy: 0.6320 - val_loss: 0.7972 - val_accuracy: 0.6315\n",
            "Epoch 455/600\n",
            "237/237 [==============================] - 2s 6ms/step - loss: 0.7925 - accuracy: 0.6331 - val_loss: 0.7972 - val_accuracy: 0.6315\n",
            "Epoch 456/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7925 - accuracy: 0.6325 - val_loss: 0.7972 - val_accuracy: 0.6324\n",
            "Epoch 457/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7925 - accuracy: 0.6321 - val_loss: 0.7972 - val_accuracy: 0.6321\n",
            "Epoch 458/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7925 - accuracy: 0.6317 - val_loss: 0.7972 - val_accuracy: 0.6321\n",
            "Epoch 459/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7925 - accuracy: 0.6325 - val_loss: 0.7972 - val_accuracy: 0.6318\n",
            "Epoch 460/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7924 - accuracy: 0.6319 - val_loss: 0.7972 - val_accuracy: 0.6324\n",
            "Epoch 461/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7924 - accuracy: 0.6319 - val_loss: 0.7972 - val_accuracy: 0.6325\n",
            "Epoch 462/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7924 - accuracy: 0.6322 - val_loss: 0.7972 - val_accuracy: 0.6322\n",
            "Epoch 463/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7924 - accuracy: 0.6323 - val_loss: 0.7971 - val_accuracy: 0.6309\n",
            "Epoch 464/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7924 - accuracy: 0.6319 - val_loss: 0.7971 - val_accuracy: 0.6319\n",
            "Epoch 465/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7924 - accuracy: 0.6324 - val_loss: 0.7972 - val_accuracy: 0.6319\n",
            "Epoch 466/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7923 - accuracy: 0.6323 - val_loss: 0.7971 - val_accuracy: 0.6321\n",
            "Epoch 467/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7923 - accuracy: 0.6324 - val_loss: 0.7971 - val_accuracy: 0.6318\n",
            "Epoch 468/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7923 - accuracy: 0.6322 - val_loss: 0.7972 - val_accuracy: 0.6319\n",
            "Epoch 469/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7923 - accuracy: 0.6321 - val_loss: 0.7971 - val_accuracy: 0.6327\n",
            "Epoch 470/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7923 - accuracy: 0.6319 - val_loss: 0.7970 - val_accuracy: 0.6325\n",
            "Epoch 471/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7923 - accuracy: 0.6323 - val_loss: 0.7971 - val_accuracy: 0.6322\n",
            "Epoch 472/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7922 - accuracy: 0.6314 - val_loss: 0.7970 - val_accuracy: 0.6327\n",
            "Epoch 473/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7922 - accuracy: 0.6330 - val_loss: 0.7971 - val_accuracy: 0.6316\n",
            "Epoch 474/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7922 - accuracy: 0.6325 - val_loss: 0.7971 - val_accuracy: 0.6319\n",
            "Epoch 475/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7922 - accuracy: 0.6318 - val_loss: 0.7970 - val_accuracy: 0.6327\n",
            "Epoch 476/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7922 - accuracy: 0.6321 - val_loss: 0.7970 - val_accuracy: 0.6325\n",
            "Epoch 477/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7922 - accuracy: 0.6323 - val_loss: 0.7970 - val_accuracy: 0.6309\n",
            "Epoch 478/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7922 - accuracy: 0.6321 - val_loss: 0.7970 - val_accuracy: 0.6324\n",
            "Epoch 479/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7921 - accuracy: 0.6325 - val_loss: 0.7969 - val_accuracy: 0.6324\n",
            "Epoch 480/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7921 - accuracy: 0.6319 - val_loss: 0.7970 - val_accuracy: 0.6327\n",
            "Epoch 481/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7921 - accuracy: 0.6326 - val_loss: 0.7970 - val_accuracy: 0.6318\n",
            "Epoch 482/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7920 - accuracy: 0.6323 - val_loss: 0.7969 - val_accuracy: 0.6327\n",
            "Epoch 483/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7921 - accuracy: 0.6330 - val_loss: 0.7970 - val_accuracy: 0.6318\n",
            "Epoch 484/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7920 - accuracy: 0.6325 - val_loss: 0.7970 - val_accuracy: 0.6319\n",
            "Epoch 485/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7920 - accuracy: 0.6323 - val_loss: 0.7969 - val_accuracy: 0.6330\n",
            "Epoch 486/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7920 - accuracy: 0.6330 - val_loss: 0.7969 - val_accuracy: 0.6319\n",
            "Epoch 487/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7920 - accuracy: 0.6327 - val_loss: 0.7970 - val_accuracy: 0.6319\n",
            "Epoch 488/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7920 - accuracy: 0.6318 - val_loss: 0.7969 - val_accuracy: 0.6325\n",
            "Epoch 489/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7920 - accuracy: 0.6321 - val_loss: 0.7969 - val_accuracy: 0.6319\n",
            "Epoch 490/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7919 - accuracy: 0.6324 - val_loss: 0.7968 - val_accuracy: 0.6315\n",
            "Epoch 491/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7919 - accuracy: 0.6319 - val_loss: 0.7968 - val_accuracy: 0.6324\n",
            "Epoch 492/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7919 - accuracy: 0.6325 - val_loss: 0.7969 - val_accuracy: 0.6322\n",
            "Epoch 493/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7919 - accuracy: 0.6322 - val_loss: 0.7968 - val_accuracy: 0.6319\n",
            "Epoch 494/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7919 - accuracy: 0.6325 - val_loss: 0.7968 - val_accuracy: 0.6319\n",
            "Epoch 495/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7919 - accuracy: 0.6326 - val_loss: 0.7969 - val_accuracy: 0.6318\n",
            "Epoch 496/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7919 - accuracy: 0.6321 - val_loss: 0.7968 - val_accuracy: 0.6324\n",
            "Epoch 497/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7919 - accuracy: 0.6321 - val_loss: 0.7968 - val_accuracy: 0.6321\n",
            "Epoch 498/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7918 - accuracy: 0.6324 - val_loss: 0.7970 - val_accuracy: 0.6318\n",
            "Epoch 499/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7918 - accuracy: 0.6319 - val_loss: 0.7969 - val_accuracy: 0.6321\n",
            "Epoch 500/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7918 - accuracy: 0.6314 - val_loss: 0.7969 - val_accuracy: 0.6316\n",
            "Epoch 501/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7918 - accuracy: 0.6319 - val_loss: 0.7967 - val_accuracy: 0.6322\n",
            "Epoch 502/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7917 - accuracy: 0.6329 - val_loss: 0.7969 - val_accuracy: 0.6321\n",
            "Epoch 503/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7918 - accuracy: 0.6316 - val_loss: 0.7968 - val_accuracy: 0.6315\n",
            "Epoch 504/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7917 - accuracy: 0.6324 - val_loss: 0.7967 - val_accuracy: 0.6310\n",
            "Epoch 505/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7917 - accuracy: 0.6326 - val_loss: 0.7968 - val_accuracy: 0.6325\n",
            "Epoch 506/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7917 - accuracy: 0.6323 - val_loss: 0.7969 - val_accuracy: 0.6322\n",
            "Epoch 507/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7917 - accuracy: 0.6323 - val_loss: 0.7968 - val_accuracy: 0.6325\n",
            "Epoch 508/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7917 - accuracy: 0.6328 - val_loss: 0.7968 - val_accuracy: 0.6316\n",
            "Epoch 509/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7917 - accuracy: 0.6327 - val_loss: 0.7967 - val_accuracy: 0.6325\n",
            "Epoch 510/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7917 - accuracy: 0.6319 - val_loss: 0.7967 - val_accuracy: 0.6324\n",
            "Epoch 511/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7916 - accuracy: 0.6324 - val_loss: 0.7967 - val_accuracy: 0.6321\n",
            "Epoch 512/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7916 - accuracy: 0.6324 - val_loss: 0.7967 - val_accuracy: 0.6325\n",
            "Epoch 513/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7916 - accuracy: 0.6326 - val_loss: 0.7967 - val_accuracy: 0.6315\n",
            "Epoch 514/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7916 - accuracy: 0.6325 - val_loss: 0.7967 - val_accuracy: 0.6316\n",
            "Epoch 515/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7916 - accuracy: 0.6321 - val_loss: 0.7966 - val_accuracy: 0.6324\n",
            "Epoch 516/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7916 - accuracy: 0.6328 - val_loss: 0.7967 - val_accuracy: 0.6316\n",
            "Epoch 517/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7915 - accuracy: 0.6325 - val_loss: 0.7966 - val_accuracy: 0.6315\n",
            "Epoch 518/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7916 - accuracy: 0.6323 - val_loss: 0.7966 - val_accuracy: 0.6312\n",
            "Epoch 519/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7915 - accuracy: 0.6329 - val_loss: 0.7966 - val_accuracy: 0.6322\n",
            "Epoch 520/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7916 - accuracy: 0.6325 - val_loss: 0.7966 - val_accuracy: 0.6312\n",
            "Epoch 521/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7915 - accuracy: 0.6325 - val_loss: 0.7966 - val_accuracy: 0.6318\n",
            "Epoch 522/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7915 - accuracy: 0.6331 - val_loss: 0.7966 - val_accuracy: 0.6310\n",
            "Epoch 523/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7914 - accuracy: 0.6326 - val_loss: 0.7967 - val_accuracy: 0.6324\n",
            "Epoch 524/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7915 - accuracy: 0.6332 - val_loss: 0.7966 - val_accuracy: 0.6313\n",
            "Epoch 525/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7915 - accuracy: 0.6321 - val_loss: 0.7966 - val_accuracy: 0.6315\n",
            "Epoch 526/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7914 - accuracy: 0.6329 - val_loss: 0.7965 - val_accuracy: 0.6324\n",
            "Epoch 527/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7914 - accuracy: 0.6327 - val_loss: 0.7965 - val_accuracy: 0.6322\n",
            "Epoch 528/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7914 - accuracy: 0.6324 - val_loss: 0.7966 - val_accuracy: 0.6309\n",
            "Epoch 529/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7914 - accuracy: 0.6325 - val_loss: 0.7966 - val_accuracy: 0.6313\n",
            "Epoch 530/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7914 - accuracy: 0.6321 - val_loss: 0.7965 - val_accuracy: 0.6318\n",
            "Epoch 531/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7914 - accuracy: 0.6324 - val_loss: 0.7965 - val_accuracy: 0.6318\n",
            "Epoch 532/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7914 - accuracy: 0.6332 - val_loss: 0.7965 - val_accuracy: 0.6319\n",
            "Epoch 533/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7914 - accuracy: 0.6331 - val_loss: 0.7965 - val_accuracy: 0.6324\n",
            "Epoch 534/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7913 - accuracy: 0.6323 - val_loss: 0.7965 - val_accuracy: 0.6319\n",
            "Epoch 535/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7913 - accuracy: 0.6329 - val_loss: 0.7966 - val_accuracy: 0.6316\n",
            "Epoch 536/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7913 - accuracy: 0.6329 - val_loss: 0.7965 - val_accuracy: 0.6321\n",
            "Epoch 537/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7913 - accuracy: 0.6327 - val_loss: 0.7964 - val_accuracy: 0.6324\n",
            "Epoch 538/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7912 - accuracy: 0.6332 - val_loss: 0.7964 - val_accuracy: 0.6321\n",
            "Epoch 539/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7913 - accuracy: 0.6324 - val_loss: 0.7964 - val_accuracy: 0.6324\n",
            "Epoch 540/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7913 - accuracy: 0.6327 - val_loss: 0.7965 - val_accuracy: 0.6310\n",
            "Epoch 541/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7913 - accuracy: 0.6329 - val_loss: 0.7964 - val_accuracy: 0.6319\n",
            "Epoch 542/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7912 - accuracy: 0.6338 - val_loss: 0.7965 - val_accuracy: 0.6318\n",
            "Epoch 543/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7912 - accuracy: 0.6331 - val_loss: 0.7964 - val_accuracy: 0.6321\n",
            "Epoch 544/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7912 - accuracy: 0.6327 - val_loss: 0.7964 - val_accuracy: 0.6316\n",
            "Epoch 545/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7912 - accuracy: 0.6332 - val_loss: 0.7964 - val_accuracy: 0.6321\n",
            "Epoch 546/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7911 - accuracy: 0.6332 - val_loss: 0.7964 - val_accuracy: 0.6319\n",
            "Epoch 547/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7912 - accuracy: 0.6328 - val_loss: 0.7964 - val_accuracy: 0.6322\n",
            "Epoch 548/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7912 - accuracy: 0.6329 - val_loss: 0.7964 - val_accuracy: 0.6319\n",
            "Epoch 549/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7912 - accuracy: 0.6329 - val_loss: 0.7964 - val_accuracy: 0.6322\n",
            "Epoch 550/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7911 - accuracy: 0.6323 - val_loss: 0.7964 - val_accuracy: 0.6315\n",
            "Epoch 551/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7912 - accuracy: 0.6327 - val_loss: 0.7963 - val_accuracy: 0.6319\n",
            "Epoch 552/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7911 - accuracy: 0.6332 - val_loss: 0.7964 - val_accuracy: 0.6318\n",
            "Epoch 553/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7911 - accuracy: 0.6328 - val_loss: 0.7964 - val_accuracy: 0.6313\n",
            "Epoch 554/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7911 - accuracy: 0.6334 - val_loss: 0.7963 - val_accuracy: 0.6322\n",
            "Epoch 555/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7911 - accuracy: 0.6336 - val_loss: 0.7963 - val_accuracy: 0.6322\n",
            "Epoch 556/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7911 - accuracy: 0.6326 - val_loss: 0.7963 - val_accuracy: 0.6321\n",
            "Epoch 557/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7911 - accuracy: 0.6327 - val_loss: 0.7963 - val_accuracy: 0.6318\n",
            "Epoch 558/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7911 - accuracy: 0.6332 - val_loss: 0.7964 - val_accuracy: 0.6312\n",
            "Epoch 559/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7910 - accuracy: 0.6334 - val_loss: 0.7963 - val_accuracy: 0.6315\n",
            "Epoch 560/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7910 - accuracy: 0.6326 - val_loss: 0.7964 - val_accuracy: 0.6318\n",
            "Epoch 561/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7910 - accuracy: 0.6337 - val_loss: 0.7963 - val_accuracy: 0.6322\n",
            "Epoch 562/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7910 - accuracy: 0.6332 - val_loss: 0.7963 - val_accuracy: 0.6319\n",
            "Epoch 563/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7910 - accuracy: 0.6335 - val_loss: 0.7963 - val_accuracy: 0.6316\n",
            "Epoch 564/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7910 - accuracy: 0.6336 - val_loss: 0.7963 - val_accuracy: 0.6316\n",
            "Epoch 565/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7910 - accuracy: 0.6331 - val_loss: 0.7963 - val_accuracy: 0.6316\n",
            "Epoch 566/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7910 - accuracy: 0.6329 - val_loss: 0.7963 - val_accuracy: 0.6318\n",
            "Epoch 567/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7909 - accuracy: 0.6332 - val_loss: 0.7963 - val_accuracy: 0.6316\n",
            "Epoch 568/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7909 - accuracy: 0.6332 - val_loss: 0.7963 - val_accuracy: 0.6316\n",
            "Epoch 569/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7909 - accuracy: 0.6329 - val_loss: 0.7962 - val_accuracy: 0.6325\n",
            "Epoch 570/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7909 - accuracy: 0.6330 - val_loss: 0.7962 - val_accuracy: 0.6327\n",
            "Epoch 571/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7909 - accuracy: 0.6334 - val_loss: 0.7962 - val_accuracy: 0.6325\n",
            "Epoch 572/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7909 - accuracy: 0.6323 - val_loss: 0.7962 - val_accuracy: 0.6325\n",
            "Epoch 573/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7909 - accuracy: 0.6331 - val_loss: 0.7963 - val_accuracy: 0.6316\n",
            "Epoch 574/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7909 - accuracy: 0.6332 - val_loss: 0.7962 - val_accuracy: 0.6318\n",
            "Epoch 575/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7909 - accuracy: 0.6332 - val_loss: 0.7962 - val_accuracy: 0.6312\n",
            "Epoch 576/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7909 - accuracy: 0.6334 - val_loss: 0.7963 - val_accuracy: 0.6316\n",
            "Epoch 577/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7908 - accuracy: 0.6336 - val_loss: 0.7963 - val_accuracy: 0.6316\n",
            "Epoch 578/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7908 - accuracy: 0.6329 - val_loss: 0.7962 - val_accuracy: 0.6310\n",
            "Epoch 579/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7908 - accuracy: 0.6337 - val_loss: 0.7962 - val_accuracy: 0.6310\n",
            "Epoch 580/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7908 - accuracy: 0.6333 - val_loss: 0.7962 - val_accuracy: 0.6309\n",
            "Epoch 581/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7908 - accuracy: 0.6330 - val_loss: 0.7961 - val_accuracy: 0.6319\n",
            "Epoch 582/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7908 - accuracy: 0.6334 - val_loss: 0.7962 - val_accuracy: 0.6319\n",
            "Epoch 583/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7908 - accuracy: 0.6336 - val_loss: 0.7961 - val_accuracy: 0.6319\n",
            "Epoch 584/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7908 - accuracy: 0.6330 - val_loss: 0.7961 - val_accuracy: 0.6322\n",
            "Epoch 585/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7908 - accuracy: 0.6335 - val_loss: 0.7961 - val_accuracy: 0.6315\n",
            "Epoch 586/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7907 - accuracy: 0.6334 - val_loss: 0.7961 - val_accuracy: 0.6310\n",
            "Epoch 587/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7907 - accuracy: 0.6332 - val_loss: 0.7963 - val_accuracy: 0.6318\n",
            "Epoch 588/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7907 - accuracy: 0.6328 - val_loss: 0.7961 - val_accuracy: 0.6324\n",
            "Epoch 589/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7907 - accuracy: 0.6330 - val_loss: 0.7961 - val_accuracy: 0.6325\n",
            "Epoch 590/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7907 - accuracy: 0.6331 - val_loss: 0.7961 - val_accuracy: 0.6327\n",
            "Epoch 591/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7907 - accuracy: 0.6334 - val_loss: 0.7961 - val_accuracy: 0.6312\n",
            "Epoch 592/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7907 - accuracy: 0.6329 - val_loss: 0.7961 - val_accuracy: 0.6316\n",
            "Epoch 593/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7907 - accuracy: 0.6332 - val_loss: 0.7961 - val_accuracy: 0.6325\n",
            "Epoch 594/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7907 - accuracy: 0.6329 - val_loss: 0.7961 - val_accuracy: 0.6315\n",
            "Epoch 595/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7907 - accuracy: 0.6333 - val_loss: 0.7961 - val_accuracy: 0.6316\n",
            "Epoch 596/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7906 - accuracy: 0.6330 - val_loss: 0.7960 - val_accuracy: 0.6318\n",
            "Epoch 597/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7906 - accuracy: 0.6344 - val_loss: 0.7961 - val_accuracy: 0.6316\n",
            "Epoch 598/600\n",
            "237/237 [==============================] - 1s 6ms/step - loss: 0.7906 - accuracy: 0.6329 - val_loss: 0.7961 - val_accuracy: 0.6305\n",
            "Epoch 599/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7906 - accuracy: 0.6331 - val_loss: 0.7960 - val_accuracy: 0.6325\n",
            "Epoch 600/600\n",
            "237/237 [==============================] - 1s 5ms/step - loss: 0.7906 - accuracy: 0.6330 - val_loss: 0.7960 - val_accuracy: 0.6315\n",
            "74/74 [==============================] - 0s 3ms/step - loss: 0.7960 - accuracy: 0.6315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMz0D0D0iQ74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb06ab49-96d1-4523-fb44-80bc6acc642c"
      },
      "source": [
        "create_model2()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_16 (Dense)             (None, 13)                182       \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 460)               6440      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 370)               170570    \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 280)               103880    \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 190)               53390     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 100)               19100     \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 190)               19190     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 280)               53480     \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 370)               103970    \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 460)               170660    \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 3)                 1383      \n",
            "=================================================================\n",
            "Total params: 704,355\n",
            "Trainable params: 704,355\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 1.0928 - accuracy: 0.4305 - val_loss: 1.0886 - val_accuracy: 0.4307\n",
            "Epoch 2/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0858 - accuracy: 0.4307 - val_loss: 1.0833 - val_accuracy: 0.4307\n",
            "Epoch 3/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0817 - accuracy: 0.4307 - val_loss: 1.0802 - val_accuracy: 0.4307\n",
            "Epoch 4/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0791 - accuracy: 0.4307 - val_loss: 1.0781 - val_accuracy: 0.4307\n",
            "Epoch 5/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0774 - accuracy: 0.4307 - val_loss: 1.0766 - val_accuracy: 0.4307\n",
            "Epoch 6/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0761 - accuracy: 0.4307 - val_loss: 1.0756 - val_accuracy: 0.4307\n",
            "Epoch 7/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0752 - accuracy: 0.4307 - val_loss: 1.0747 - val_accuracy: 0.4307\n",
            "Epoch 8/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0743 - accuracy: 0.4307 - val_loss: 1.0739 - val_accuracy: 0.4307\n",
            "Epoch 9/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0737 - accuracy: 0.4307 - val_loss: 1.0733 - val_accuracy: 0.4307\n",
            "Epoch 10/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0730 - accuracy: 0.4307 - val_loss: 1.0726 - val_accuracy: 0.4307\n",
            "Epoch 11/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0722 - accuracy: 0.4307 - val_loss: 1.0718 - val_accuracy: 0.4307\n",
            "Epoch 12/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0714 - accuracy: 0.4307 - val_loss: 1.0710 - val_accuracy: 0.4307\n",
            "Epoch 13/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0706 - accuracy: 0.4307 - val_loss: 1.0701 - val_accuracy: 0.4307\n",
            "Epoch 14/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0697 - accuracy: 0.4307 - val_loss: 1.0692 - val_accuracy: 0.4307\n",
            "Epoch 15/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0687 - accuracy: 0.4307 - val_loss: 1.0680 - val_accuracy: 0.4307\n",
            "Epoch 16/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0674 - accuracy: 0.4307 - val_loss: 1.0666 - val_accuracy: 0.4307\n",
            "Epoch 17/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0658 - accuracy: 0.4307 - val_loss: 1.0649 - val_accuracy: 0.4307\n",
            "Epoch 18/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0639 - accuracy: 0.4307 - val_loss: 1.0628 - val_accuracy: 0.4307\n",
            "Epoch 19/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0617 - accuracy: 0.4307 - val_loss: 1.0604 - val_accuracy: 0.4307\n",
            "Epoch 20/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0591 - accuracy: 0.4307 - val_loss: 1.0574 - val_accuracy: 0.4307\n",
            "Epoch 21/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0559 - accuracy: 0.4307 - val_loss: 1.0539 - val_accuracy: 0.4307\n",
            "Epoch 22/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0519 - accuracy: 0.4307 - val_loss: 1.0494 - val_accuracy: 0.4307\n",
            "Epoch 23/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0469 - accuracy: 0.4307 - val_loss: 1.0439 - val_accuracy: 0.4307\n",
            "Epoch 24/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0408 - accuracy: 0.4307 - val_loss: 1.0371 - val_accuracy: 0.4307\n",
            "Epoch 25/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0334 - accuracy: 0.4307 - val_loss: 1.0288 - val_accuracy: 0.4307\n",
            "Epoch 26/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0241 - accuracy: 0.4307 - val_loss: 1.0184 - val_accuracy: 0.4307\n",
            "Epoch 27/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 1.0126 - accuracy: 0.4804 - val_loss: 1.0055 - val_accuracy: 0.5404\n",
            "Epoch 28/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.9985 - accuracy: 0.5424 - val_loss: 0.9900 - val_accuracy: 0.5565\n",
            "Epoch 29/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.9814 - accuracy: 0.5555 - val_loss: 0.9712 - val_accuracy: 0.5652\n",
            "Epoch 30/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.9616 - accuracy: 0.5691 - val_loss: 0.9509 - val_accuracy: 0.5719\n",
            "Epoch 31/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.9402 - accuracy: 0.5919 - val_loss: 0.9284 - val_accuracy: 0.5994\n",
            "Epoch 32/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.9188 - accuracy: 0.6028 - val_loss: 0.9080 - val_accuracy: 0.6052\n",
            "Epoch 33/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8990 - accuracy: 0.6073 - val_loss: 0.8899 - val_accuracy: 0.6091\n",
            "Epoch 34/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8820 - accuracy: 0.6107 - val_loss: 0.8744 - val_accuracy: 0.6111\n",
            "Epoch 35/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8686 - accuracy: 0.6130 - val_loss: 0.8653 - val_accuracy: 0.6046\n",
            "Epoch 36/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8574 - accuracy: 0.6129 - val_loss: 0.8568 - val_accuracy: 0.6052\n",
            "Epoch 37/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8488 - accuracy: 0.6141 - val_loss: 0.8465 - val_accuracy: 0.6169\n",
            "Epoch 38/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8414 - accuracy: 0.6189 - val_loss: 0.8449 - val_accuracy: 0.6068\n",
            "Epoch 39/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8354 - accuracy: 0.6211 - val_loss: 0.8438 - val_accuracy: 0.6157\n",
            "Epoch 40/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8301 - accuracy: 0.6245 - val_loss: 0.8370 - val_accuracy: 0.6112\n",
            "Epoch 41/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8263 - accuracy: 0.6252 - val_loss: 0.8266 - val_accuracy: 0.6228\n",
            "Epoch 42/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8229 - accuracy: 0.6268 - val_loss: 0.8293 - val_accuracy: 0.6155\n",
            "Epoch 43/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8199 - accuracy: 0.6258 - val_loss: 0.8216 - val_accuracy: 0.6198\n",
            "Epoch 44/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8174 - accuracy: 0.6285 - val_loss: 0.8195 - val_accuracy: 0.6211\n",
            "Epoch 45/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8153 - accuracy: 0.6288 - val_loss: 0.8174 - val_accuracy: 0.6226\n",
            "Epoch 46/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8131 - accuracy: 0.6274 - val_loss: 0.8159 - val_accuracy: 0.6223\n",
            "Epoch 47/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8122 - accuracy: 0.6269 - val_loss: 0.8186 - val_accuracy: 0.6217\n",
            "Epoch 48/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8107 - accuracy: 0.6279 - val_loss: 0.8202 - val_accuracy: 0.6210\n",
            "Epoch 49/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8098 - accuracy: 0.6297 - val_loss: 0.8144 - val_accuracy: 0.6228\n",
            "Epoch 50/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8088 - accuracy: 0.6297 - val_loss: 0.8154 - val_accuracy: 0.6219\n",
            "Epoch 51/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8076 - accuracy: 0.6304 - val_loss: 0.8133 - val_accuracy: 0.6228\n",
            "Epoch 52/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8070 - accuracy: 0.6300 - val_loss: 0.8102 - val_accuracy: 0.6219\n",
            "Epoch 53/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8063 - accuracy: 0.6302 - val_loss: 0.8099 - val_accuracy: 0.6231\n",
            "Epoch 54/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8055 - accuracy: 0.6288 - val_loss: 0.8137 - val_accuracy: 0.6207\n",
            "Epoch 55/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8048 - accuracy: 0.6306 - val_loss: 0.8091 - val_accuracy: 0.6245\n",
            "Epoch 56/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8044 - accuracy: 0.6298 - val_loss: 0.8085 - val_accuracy: 0.6233\n",
            "Epoch 57/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.8035 - accuracy: 0.6295 - val_loss: 0.8096 - val_accuracy: 0.6248\n",
            "Epoch 58/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.8032 - accuracy: 0.6288 - val_loss: 0.8087 - val_accuracy: 0.6229\n",
            "Epoch 59/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.8025 - accuracy: 0.6294 - val_loss: 0.8077 - val_accuracy: 0.6219\n",
            "Epoch 60/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.8021 - accuracy: 0.6294 - val_loss: 0.8072 - val_accuracy: 0.6225\n",
            "Epoch 61/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.8013 - accuracy: 0.6313 - val_loss: 0.8066 - val_accuracy: 0.6214\n",
            "Epoch 62/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8009 - accuracy: 0.6306 - val_loss: 0.8092 - val_accuracy: 0.6234\n",
            "Epoch 63/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8007 - accuracy: 0.6299 - val_loss: 0.8102 - val_accuracy: 0.6254\n",
            "Epoch 64/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8006 - accuracy: 0.6310 - val_loss: 0.8052 - val_accuracy: 0.6249\n",
            "Epoch 65/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.8001 - accuracy: 0.6321 - val_loss: 0.8091 - val_accuracy: 0.6264\n",
            "Epoch 66/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7995 - accuracy: 0.6319 - val_loss: 0.8049 - val_accuracy: 0.6245\n",
            "Epoch 67/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7991 - accuracy: 0.6319 - val_loss: 0.8051 - val_accuracy: 0.6242\n",
            "Epoch 68/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7988 - accuracy: 0.6303 - val_loss: 0.8047 - val_accuracy: 0.6216\n",
            "Epoch 69/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7987 - accuracy: 0.6307 - val_loss: 0.8145 - val_accuracy: 0.6234\n",
            "Epoch 70/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7983 - accuracy: 0.6308 - val_loss: 0.8045 - val_accuracy: 0.6272\n",
            "Epoch 71/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7978 - accuracy: 0.6310 - val_loss: 0.8047 - val_accuracy: 0.6236\n",
            "Epoch 72/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7978 - accuracy: 0.6315 - val_loss: 0.8050 - val_accuracy: 0.6240\n",
            "Epoch 73/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7974 - accuracy: 0.6314 - val_loss: 0.8046 - val_accuracy: 0.6258\n",
            "Epoch 74/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7972 - accuracy: 0.6322 - val_loss: 0.8092 - val_accuracy: 0.6222\n",
            "Epoch 75/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7967 - accuracy: 0.6334 - val_loss: 0.8052 - val_accuracy: 0.6267\n",
            "Epoch 76/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7964 - accuracy: 0.6303 - val_loss: 0.8028 - val_accuracy: 0.6225\n",
            "Epoch 77/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7965 - accuracy: 0.6306 - val_loss: 0.8047 - val_accuracy: 0.6255\n",
            "Epoch 78/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7959 - accuracy: 0.6329 - val_loss: 0.8022 - val_accuracy: 0.6240\n",
            "Epoch 79/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7956 - accuracy: 0.6312 - val_loss: 0.8019 - val_accuracy: 0.6255\n",
            "Epoch 80/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7953 - accuracy: 0.6324 - val_loss: 0.8031 - val_accuracy: 0.6243\n",
            "Epoch 81/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7951 - accuracy: 0.6330 - val_loss: 0.8062 - val_accuracy: 0.6248\n",
            "Epoch 82/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7947 - accuracy: 0.6311 - val_loss: 0.8028 - val_accuracy: 0.6248\n",
            "Epoch 83/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7946 - accuracy: 0.6316 - val_loss: 0.8024 - val_accuracy: 0.6229\n",
            "Epoch 84/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7939 - accuracy: 0.6345 - val_loss: 0.8048 - val_accuracy: 0.6258\n",
            "Epoch 85/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7942 - accuracy: 0.6306 - val_loss: 0.8013 - val_accuracy: 0.6246\n",
            "Epoch 86/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7940 - accuracy: 0.6329 - val_loss: 0.8020 - val_accuracy: 0.6234\n",
            "Epoch 87/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7938 - accuracy: 0.6318 - val_loss: 0.8006 - val_accuracy: 0.6252\n",
            "Epoch 88/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7935 - accuracy: 0.6310 - val_loss: 0.8097 - val_accuracy: 0.6226\n",
            "Epoch 89/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7934 - accuracy: 0.6331 - val_loss: 0.8027 - val_accuracy: 0.6239\n",
            "Epoch 90/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7928 - accuracy: 0.6320 - val_loss: 0.8002 - val_accuracy: 0.6263\n",
            "Epoch 91/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7926 - accuracy: 0.6329 - val_loss: 0.8009 - val_accuracy: 0.6237\n",
            "Epoch 92/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7922 - accuracy: 0.6314 - val_loss: 0.8003 - val_accuracy: 0.6231\n",
            "Epoch 93/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7927 - accuracy: 0.6310 - val_loss: 0.8008 - val_accuracy: 0.6254\n",
            "Epoch 94/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7924 - accuracy: 0.6340 - val_loss: 0.8015 - val_accuracy: 0.6243\n",
            "Epoch 95/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7920 - accuracy: 0.6334 - val_loss: 0.7997 - val_accuracy: 0.6249\n",
            "Epoch 96/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7920 - accuracy: 0.6328 - val_loss: 0.8023 - val_accuracy: 0.6248\n",
            "Epoch 97/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7918 - accuracy: 0.6333 - val_loss: 0.8031 - val_accuracy: 0.6261\n",
            "Epoch 98/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7911 - accuracy: 0.6316 - val_loss: 0.8001 - val_accuracy: 0.6252\n",
            "Epoch 99/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7913 - accuracy: 0.6339 - val_loss: 0.7991 - val_accuracy: 0.6260\n",
            "Epoch 100/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7909 - accuracy: 0.6338 - val_loss: 0.8000 - val_accuracy: 0.6263\n",
            "Epoch 101/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7908 - accuracy: 0.6323 - val_loss: 0.7993 - val_accuracy: 0.6261\n",
            "Epoch 102/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7906 - accuracy: 0.6338 - val_loss: 0.7988 - val_accuracy: 0.6248\n",
            "Epoch 103/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7902 - accuracy: 0.6334 - val_loss: 0.7993 - val_accuracy: 0.6258\n",
            "Epoch 104/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7899 - accuracy: 0.6323 - val_loss: 0.7990 - val_accuracy: 0.6249\n",
            "Epoch 105/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7902 - accuracy: 0.6339 - val_loss: 0.7988 - val_accuracy: 0.6251\n",
            "Epoch 106/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7901 - accuracy: 0.6326 - val_loss: 0.7994 - val_accuracy: 0.6258\n",
            "Epoch 107/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7897 - accuracy: 0.6343 - val_loss: 0.7985 - val_accuracy: 0.6263\n",
            "Epoch 108/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7898 - accuracy: 0.6336 - val_loss: 0.7982 - val_accuracy: 0.6252\n",
            "Epoch 109/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7894 - accuracy: 0.6340 - val_loss: 0.7988 - val_accuracy: 0.6257\n",
            "Epoch 110/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7892 - accuracy: 0.6332 - val_loss: 0.7998 - val_accuracy: 0.6248\n",
            "Epoch 111/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7888 - accuracy: 0.6347 - val_loss: 0.7984 - val_accuracy: 0.6237\n",
            "Epoch 112/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7890 - accuracy: 0.6353 - val_loss: 0.7983 - val_accuracy: 0.6255\n",
            "Epoch 113/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7891 - accuracy: 0.6345 - val_loss: 0.7986 - val_accuracy: 0.6261\n",
            "Epoch 114/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7888 - accuracy: 0.6335 - val_loss: 0.7981 - val_accuracy: 0.6258\n",
            "Epoch 115/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7886 - accuracy: 0.6341 - val_loss: 0.7986 - val_accuracy: 0.6269\n",
            "Epoch 116/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7883 - accuracy: 0.6329 - val_loss: 0.7981 - val_accuracy: 0.6249\n",
            "Epoch 117/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7884 - accuracy: 0.6352 - val_loss: 0.8010 - val_accuracy: 0.6271\n",
            "Epoch 118/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7883 - accuracy: 0.6334 - val_loss: 0.7996 - val_accuracy: 0.6255\n",
            "Epoch 119/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7879 - accuracy: 0.6336 - val_loss: 0.7974 - val_accuracy: 0.6252\n",
            "Epoch 120/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7874 - accuracy: 0.6327 - val_loss: 0.8056 - val_accuracy: 0.6217\n",
            "Epoch 121/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7877 - accuracy: 0.6337 - val_loss: 0.7997 - val_accuracy: 0.6264\n",
            "Epoch 122/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7874 - accuracy: 0.6330 - val_loss: 0.8017 - val_accuracy: 0.6260\n",
            "Epoch 123/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7873 - accuracy: 0.6360 - val_loss: 0.7971 - val_accuracy: 0.6261\n",
            "Epoch 124/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7870 - accuracy: 0.6355 - val_loss: 0.7980 - val_accuracy: 0.6263\n",
            "Epoch 125/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7870 - accuracy: 0.6344 - val_loss: 0.7996 - val_accuracy: 0.6251\n",
            "Epoch 126/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7866 - accuracy: 0.6342 - val_loss: 0.7972 - val_accuracy: 0.6267\n",
            "Epoch 127/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7867 - accuracy: 0.6344 - val_loss: 0.7994 - val_accuracy: 0.6267\n",
            "Epoch 128/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7865 - accuracy: 0.6335 - val_loss: 0.8034 - val_accuracy: 0.6274\n",
            "Epoch 129/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7866 - accuracy: 0.6364 - val_loss: 0.7986 - val_accuracy: 0.6229\n",
            "Epoch 130/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7862 - accuracy: 0.6355 - val_loss: 0.7972 - val_accuracy: 0.6257\n",
            "Epoch 131/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7860 - accuracy: 0.6355 - val_loss: 0.7967 - val_accuracy: 0.6267\n",
            "Epoch 132/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7862 - accuracy: 0.6357 - val_loss: 0.7990 - val_accuracy: 0.6271\n",
            "Epoch 133/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7860 - accuracy: 0.6349 - val_loss: 0.7965 - val_accuracy: 0.6258\n",
            "Epoch 134/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7856 - accuracy: 0.6351 - val_loss: 0.7978 - val_accuracy: 0.6269\n",
            "Epoch 135/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7856 - accuracy: 0.6348 - val_loss: 0.7964 - val_accuracy: 0.6252\n",
            "Epoch 136/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7854 - accuracy: 0.6359 - val_loss: 0.7965 - val_accuracy: 0.6257\n",
            "Epoch 137/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7857 - accuracy: 0.6356 - val_loss: 0.7966 - val_accuracy: 0.6252\n",
            "Epoch 138/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7852 - accuracy: 0.6356 - val_loss: 0.7969 - val_accuracy: 0.6278\n",
            "Epoch 139/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7846 - accuracy: 0.6344 - val_loss: 0.7966 - val_accuracy: 0.6257\n",
            "Epoch 140/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7851 - accuracy: 0.6366 - val_loss: 0.7961 - val_accuracy: 0.6254\n",
            "Epoch 141/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7849 - accuracy: 0.6355 - val_loss: 0.7970 - val_accuracy: 0.6249\n",
            "Epoch 142/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7848 - accuracy: 0.6357 - val_loss: 0.7974 - val_accuracy: 0.6260\n",
            "Epoch 143/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7846 - accuracy: 0.6360 - val_loss: 0.7983 - val_accuracy: 0.6246\n",
            "Epoch 144/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7846 - accuracy: 0.6347 - val_loss: 0.7971 - val_accuracy: 0.6267\n",
            "Epoch 145/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7842 - accuracy: 0.6372 - val_loss: 0.7959 - val_accuracy: 0.6267\n",
            "Epoch 146/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7843 - accuracy: 0.6351 - val_loss: 0.7958 - val_accuracy: 0.6274\n",
            "Epoch 147/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7836 - accuracy: 0.6375 - val_loss: 0.7992 - val_accuracy: 0.6283\n",
            "Epoch 148/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7838 - accuracy: 0.6374 - val_loss: 0.7956 - val_accuracy: 0.6283\n",
            "Epoch 149/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7837 - accuracy: 0.6355 - val_loss: 0.8056 - val_accuracy: 0.6254\n",
            "Epoch 150/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7835 - accuracy: 0.6362 - val_loss: 0.7971 - val_accuracy: 0.6248\n",
            "Epoch 151/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7832 - accuracy: 0.6364 - val_loss: 0.8009 - val_accuracy: 0.6255\n",
            "Epoch 152/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7829 - accuracy: 0.6372 - val_loss: 0.7962 - val_accuracy: 0.6255\n",
            "Epoch 153/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7831 - accuracy: 0.6360 - val_loss: 0.7955 - val_accuracy: 0.6263\n",
            "Epoch 154/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7830 - accuracy: 0.6357 - val_loss: 0.7965 - val_accuracy: 0.6261\n",
            "Epoch 155/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7833 - accuracy: 0.6376 - val_loss: 0.7981 - val_accuracy: 0.6283\n",
            "Epoch 156/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7826 - accuracy: 0.6376 - val_loss: 0.7987 - val_accuracy: 0.6263\n",
            "Epoch 157/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7824 - accuracy: 0.6374 - val_loss: 0.8028 - val_accuracy: 0.6272\n",
            "Epoch 158/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7827 - accuracy: 0.6347 - val_loss: 0.7960 - val_accuracy: 0.6261\n",
            "Epoch 159/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7829 - accuracy: 0.6368 - val_loss: 0.7994 - val_accuracy: 0.6264\n",
            "Epoch 160/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7825 - accuracy: 0.6369 - val_loss: 0.7961 - val_accuracy: 0.6255\n",
            "Epoch 161/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7822 - accuracy: 0.6376 - val_loss: 0.7954 - val_accuracy: 0.6260\n",
            "Epoch 162/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7824 - accuracy: 0.6371 - val_loss: 0.8006 - val_accuracy: 0.6269\n",
            "Epoch 163/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7825 - accuracy: 0.6377 - val_loss: 0.7978 - val_accuracy: 0.6272\n",
            "Epoch 164/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7822 - accuracy: 0.6367 - val_loss: 0.7991 - val_accuracy: 0.6254\n",
            "Epoch 165/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7822 - accuracy: 0.6390 - val_loss: 0.7969 - val_accuracy: 0.6257\n",
            "Epoch 166/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7816 - accuracy: 0.6368 - val_loss: 0.7973 - val_accuracy: 0.6292\n",
            "Epoch 167/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7819 - accuracy: 0.6368 - val_loss: 0.7954 - val_accuracy: 0.6278\n",
            "Epoch 168/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7816 - accuracy: 0.6369 - val_loss: 0.7960 - val_accuracy: 0.6280\n",
            "Epoch 169/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7815 - accuracy: 0.6361 - val_loss: 0.7958 - val_accuracy: 0.6267\n",
            "Epoch 170/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7814 - accuracy: 0.6361 - val_loss: 0.7952 - val_accuracy: 0.6263\n",
            "Epoch 171/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7811 - accuracy: 0.6381 - val_loss: 0.7952 - val_accuracy: 0.6277\n",
            "Epoch 172/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7810 - accuracy: 0.6372 - val_loss: 0.8041 - val_accuracy: 0.6267\n",
            "Epoch 173/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7810 - accuracy: 0.6374 - val_loss: 0.7962 - val_accuracy: 0.6274\n",
            "Epoch 174/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7810 - accuracy: 0.6372 - val_loss: 0.7957 - val_accuracy: 0.6266\n",
            "Epoch 175/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7811 - accuracy: 0.6377 - val_loss: 0.7948 - val_accuracy: 0.6258\n",
            "Epoch 176/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7803 - accuracy: 0.6384 - val_loss: 0.7961 - val_accuracy: 0.6263\n",
            "Epoch 177/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7806 - accuracy: 0.6387 - val_loss: 0.7952 - val_accuracy: 0.6292\n",
            "Epoch 178/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7804 - accuracy: 0.6365 - val_loss: 0.8036 - val_accuracy: 0.6267\n",
            "Epoch 179/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7806 - accuracy: 0.6362 - val_loss: 0.8186 - val_accuracy: 0.6176\n",
            "Epoch 180/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7804 - accuracy: 0.6383 - val_loss: 0.7969 - val_accuracy: 0.6274\n",
            "Epoch 181/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7798 - accuracy: 0.6364 - val_loss: 0.7949 - val_accuracy: 0.6257\n",
            "Epoch 182/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7799 - accuracy: 0.6383 - val_loss: 0.7947 - val_accuracy: 0.6275\n",
            "Epoch 183/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7797 - accuracy: 0.6375 - val_loss: 0.7972 - val_accuracy: 0.6277\n",
            "Epoch 184/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7795 - accuracy: 0.6383 - val_loss: 0.7947 - val_accuracy: 0.6272\n",
            "Epoch 185/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7795 - accuracy: 0.6390 - val_loss: 0.8019 - val_accuracy: 0.6267\n",
            "Epoch 186/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7796 - accuracy: 0.6390 - val_loss: 0.7949 - val_accuracy: 0.6313\n",
            "Epoch 187/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7790 - accuracy: 0.6382 - val_loss: 0.7953 - val_accuracy: 0.6267\n",
            "Epoch 188/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7791 - accuracy: 0.6379 - val_loss: 0.7948 - val_accuracy: 0.6267\n",
            "Epoch 189/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7793 - accuracy: 0.6381 - val_loss: 0.7954 - val_accuracy: 0.6267\n",
            "Epoch 190/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7789 - accuracy: 0.6392 - val_loss: 0.7981 - val_accuracy: 0.6275\n",
            "Epoch 191/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7793 - accuracy: 0.6368 - val_loss: 0.7967 - val_accuracy: 0.6284\n",
            "Epoch 192/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7787 - accuracy: 0.6393 - val_loss: 0.7946 - val_accuracy: 0.6261\n",
            "Epoch 193/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7789 - accuracy: 0.6373 - val_loss: 0.7950 - val_accuracy: 0.6278\n",
            "Epoch 194/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7789 - accuracy: 0.6377 - val_loss: 0.7997 - val_accuracy: 0.6280\n",
            "Epoch 195/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7784 - accuracy: 0.6390 - val_loss: 0.7943 - val_accuracy: 0.6277\n",
            "Epoch 196/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7787 - accuracy: 0.6382 - val_loss: 0.7961 - val_accuracy: 0.6277\n",
            "Epoch 197/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7782 - accuracy: 0.6385 - val_loss: 0.7961 - val_accuracy: 0.6278\n",
            "Epoch 198/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7780 - accuracy: 0.6367 - val_loss: 0.7949 - val_accuracy: 0.6274\n",
            "Epoch 199/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7784 - accuracy: 0.6386 - val_loss: 0.7958 - val_accuracy: 0.6280\n",
            "Epoch 200/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7780 - accuracy: 0.6388 - val_loss: 0.7994 - val_accuracy: 0.6267\n",
            "Epoch 201/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7779 - accuracy: 0.6394 - val_loss: 0.7941 - val_accuracy: 0.6281\n",
            "Epoch 202/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7776 - accuracy: 0.6387 - val_loss: 0.7947 - val_accuracy: 0.6269\n",
            "Epoch 203/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7779 - accuracy: 0.6392 - val_loss: 0.7994 - val_accuracy: 0.6267\n",
            "Epoch 204/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7779 - accuracy: 0.6403 - val_loss: 0.7955 - val_accuracy: 0.6295\n",
            "Epoch 205/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7776 - accuracy: 0.6389 - val_loss: 0.7941 - val_accuracy: 0.6274\n",
            "Epoch 206/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7777 - accuracy: 0.6384 - val_loss: 0.7951 - val_accuracy: 0.6271\n",
            "Epoch 207/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7772 - accuracy: 0.6385 - val_loss: 0.7957 - val_accuracy: 0.6267\n",
            "Epoch 208/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7772 - accuracy: 0.6393 - val_loss: 0.7940 - val_accuracy: 0.6274\n",
            "Epoch 209/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7770 - accuracy: 0.6385 - val_loss: 0.7943 - val_accuracy: 0.6286\n",
            "Epoch 210/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7772 - accuracy: 0.6383 - val_loss: 0.7942 - val_accuracy: 0.6277\n",
            "Epoch 211/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7769 - accuracy: 0.6412 - val_loss: 0.7952 - val_accuracy: 0.6286\n",
            "Epoch 212/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7769 - accuracy: 0.6405 - val_loss: 0.8062 - val_accuracy: 0.6283\n",
            "Epoch 213/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7766 - accuracy: 0.6418 - val_loss: 0.8028 - val_accuracy: 0.6246\n",
            "Epoch 214/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7770 - accuracy: 0.6394 - val_loss: 0.8015 - val_accuracy: 0.6290\n",
            "Epoch 215/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7764 - accuracy: 0.6386 - val_loss: 0.7940 - val_accuracy: 0.6301\n",
            "Epoch 216/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7764 - accuracy: 0.6406 - val_loss: 0.7942 - val_accuracy: 0.6312\n",
            "Epoch 217/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7763 - accuracy: 0.6402 - val_loss: 0.7950 - val_accuracy: 0.6271\n",
            "Epoch 218/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7761 - accuracy: 0.6400 - val_loss: 0.8058 - val_accuracy: 0.6231\n",
            "Epoch 219/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7761 - accuracy: 0.6391 - val_loss: 0.7979 - val_accuracy: 0.6274\n",
            "Epoch 220/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7759 - accuracy: 0.6394 - val_loss: 0.8044 - val_accuracy: 0.6266\n",
            "Epoch 221/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7764 - accuracy: 0.6403 - val_loss: 0.7938 - val_accuracy: 0.6298\n",
            "Epoch 222/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7760 - accuracy: 0.6402 - val_loss: 0.7948 - val_accuracy: 0.6298\n",
            "Epoch 223/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7752 - accuracy: 0.6394 - val_loss: 0.8067 - val_accuracy: 0.6264\n",
            "Epoch 224/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7757 - accuracy: 0.6400 - val_loss: 0.7942 - val_accuracy: 0.6286\n",
            "Epoch 225/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7756 - accuracy: 0.6392 - val_loss: 0.7995 - val_accuracy: 0.6302\n",
            "Epoch 226/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7753 - accuracy: 0.6403 - val_loss: 0.7947 - val_accuracy: 0.6292\n",
            "Epoch 227/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7753 - accuracy: 0.6400 - val_loss: 0.7947 - val_accuracy: 0.6290\n",
            "Epoch 228/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7750 - accuracy: 0.6418 - val_loss: 0.7980 - val_accuracy: 0.6272\n",
            "Epoch 229/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7751 - accuracy: 0.6388 - val_loss: 0.7961 - val_accuracy: 0.6277\n",
            "Epoch 230/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7749 - accuracy: 0.6399 - val_loss: 0.7941 - val_accuracy: 0.6284\n",
            "Epoch 231/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7747 - accuracy: 0.6411 - val_loss: 0.7998 - val_accuracy: 0.6261\n",
            "Epoch 232/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7747 - accuracy: 0.6407 - val_loss: 0.7976 - val_accuracy: 0.6274\n",
            "Epoch 233/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7745 - accuracy: 0.6411 - val_loss: 0.8004 - val_accuracy: 0.6252\n",
            "Epoch 234/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7746 - accuracy: 0.6409 - val_loss: 0.7942 - val_accuracy: 0.6298\n",
            "Epoch 235/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7747 - accuracy: 0.6398 - val_loss: 0.8082 - val_accuracy: 0.6269\n",
            "Epoch 236/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7750 - accuracy: 0.6403 - val_loss: 0.7960 - val_accuracy: 0.6290\n",
            "Epoch 237/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7742 - accuracy: 0.6417 - val_loss: 0.7952 - val_accuracy: 0.6298\n",
            "Epoch 238/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7741 - accuracy: 0.6401 - val_loss: 0.7968 - val_accuracy: 0.6274\n",
            "Epoch 239/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7744 - accuracy: 0.6407 - val_loss: 0.7968 - val_accuracy: 0.6301\n",
            "Epoch 240/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7740 - accuracy: 0.6417 - val_loss: 0.7945 - val_accuracy: 0.6290\n",
            "Epoch 241/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7736 - accuracy: 0.6417 - val_loss: 0.7939 - val_accuracy: 0.6307\n",
            "Epoch 242/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7737 - accuracy: 0.6398 - val_loss: 0.7936 - val_accuracy: 0.6310\n",
            "Epoch 243/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7736 - accuracy: 0.6415 - val_loss: 0.7944 - val_accuracy: 0.6284\n",
            "Epoch 244/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7737 - accuracy: 0.6411 - val_loss: 0.7956 - val_accuracy: 0.6295\n",
            "Epoch 245/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7739 - accuracy: 0.6422 - val_loss: 0.7947 - val_accuracy: 0.6292\n",
            "Epoch 246/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7734 - accuracy: 0.6411 - val_loss: 0.7958 - val_accuracy: 0.6275\n",
            "Epoch 247/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7732 - accuracy: 0.6412 - val_loss: 0.7942 - val_accuracy: 0.6296\n",
            "Epoch 248/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7736 - accuracy: 0.6414 - val_loss: 0.7940 - val_accuracy: 0.6299\n",
            "Epoch 249/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7732 - accuracy: 0.6413 - val_loss: 0.7987 - val_accuracy: 0.6280\n",
            "Epoch 250/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7730 - accuracy: 0.6400 - val_loss: 0.7941 - val_accuracy: 0.6304\n",
            "Epoch 251/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7730 - accuracy: 0.6406 - val_loss: 0.8011 - val_accuracy: 0.6243\n",
            "Epoch 252/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7726 - accuracy: 0.6418 - val_loss: 0.7949 - val_accuracy: 0.6302\n",
            "Epoch 253/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7727 - accuracy: 0.6412 - val_loss: 0.7941 - val_accuracy: 0.6293\n",
            "Epoch 254/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7729 - accuracy: 0.6407 - val_loss: 0.7936 - val_accuracy: 0.6296\n",
            "Epoch 255/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7722 - accuracy: 0.6410 - val_loss: 0.7943 - val_accuracy: 0.6290\n",
            "Epoch 256/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7724 - accuracy: 0.6408 - val_loss: 0.7942 - val_accuracy: 0.6280\n",
            "Epoch 257/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7724 - accuracy: 0.6415 - val_loss: 0.8006 - val_accuracy: 0.6258\n",
            "Epoch 258/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7719 - accuracy: 0.6422 - val_loss: 0.7948 - val_accuracy: 0.6290\n",
            "Epoch 259/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7723 - accuracy: 0.6413 - val_loss: 0.8017 - val_accuracy: 0.6298\n",
            "Epoch 260/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7720 - accuracy: 0.6413 - val_loss: 0.7938 - val_accuracy: 0.6310\n",
            "Epoch 261/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7720 - accuracy: 0.6409 - val_loss: 0.7953 - val_accuracy: 0.6249\n",
            "Epoch 262/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7719 - accuracy: 0.6411 - val_loss: 0.7946 - val_accuracy: 0.6309\n",
            "Epoch 263/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7719 - accuracy: 0.6430 - val_loss: 0.7944 - val_accuracy: 0.6301\n",
            "Epoch 264/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7713 - accuracy: 0.6415 - val_loss: 0.7938 - val_accuracy: 0.6304\n",
            "Epoch 265/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7717 - accuracy: 0.6420 - val_loss: 0.7975 - val_accuracy: 0.6283\n",
            "Epoch 266/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7715 - accuracy: 0.6411 - val_loss: 0.7972 - val_accuracy: 0.6305\n",
            "Epoch 267/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7716 - accuracy: 0.6394 - val_loss: 0.7945 - val_accuracy: 0.6305\n",
            "Epoch 268/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7713 - accuracy: 0.6402 - val_loss: 0.7989 - val_accuracy: 0.6257\n",
            "Epoch 269/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7708 - accuracy: 0.6433 - val_loss: 0.7990 - val_accuracy: 0.6280\n",
            "Epoch 270/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7709 - accuracy: 0.6413 - val_loss: 0.7975 - val_accuracy: 0.6269\n",
            "Epoch 271/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7706 - accuracy: 0.6417 - val_loss: 0.8080 - val_accuracy: 0.6277\n",
            "Epoch 272/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7710 - accuracy: 0.6428 - val_loss: 0.8009 - val_accuracy: 0.6248\n",
            "Epoch 273/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7706 - accuracy: 0.6403 - val_loss: 0.7940 - val_accuracy: 0.6296\n",
            "Epoch 274/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7707 - accuracy: 0.6441 - val_loss: 0.7959 - val_accuracy: 0.6269\n",
            "Epoch 275/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7704 - accuracy: 0.6426 - val_loss: 0.7997 - val_accuracy: 0.6237\n",
            "Epoch 276/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7710 - accuracy: 0.6409 - val_loss: 0.7960 - val_accuracy: 0.6296\n",
            "Epoch 277/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7702 - accuracy: 0.6424 - val_loss: 0.7936 - val_accuracy: 0.6328\n",
            "Epoch 278/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7699 - accuracy: 0.6426 - val_loss: 0.7950 - val_accuracy: 0.6307\n",
            "Epoch 279/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7705 - accuracy: 0.6419 - val_loss: 0.7997 - val_accuracy: 0.6292\n",
            "Epoch 280/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7699 - accuracy: 0.6413 - val_loss: 0.7979 - val_accuracy: 0.6248\n",
            "Epoch 281/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7698 - accuracy: 0.6424 - val_loss: 0.7983 - val_accuracy: 0.6295\n",
            "Epoch 282/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7698 - accuracy: 0.6427 - val_loss: 0.7966 - val_accuracy: 0.6296\n",
            "Epoch 283/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7696 - accuracy: 0.6429 - val_loss: 0.7937 - val_accuracy: 0.6319\n",
            "Epoch 284/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7692 - accuracy: 0.6434 - val_loss: 0.7977 - val_accuracy: 0.6269\n",
            "Epoch 285/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7697 - accuracy: 0.6430 - val_loss: 0.7941 - val_accuracy: 0.6302\n",
            "Epoch 286/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7697 - accuracy: 0.6416 - val_loss: 0.7944 - val_accuracy: 0.6295\n",
            "Epoch 287/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7693 - accuracy: 0.6433 - val_loss: 0.7976 - val_accuracy: 0.6274\n",
            "Epoch 288/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7692 - accuracy: 0.6438 - val_loss: 0.7949 - val_accuracy: 0.6292\n",
            "Epoch 289/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7693 - accuracy: 0.6428 - val_loss: 0.7938 - val_accuracy: 0.6305\n",
            "Epoch 290/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7692 - accuracy: 0.6430 - val_loss: 0.7987 - val_accuracy: 0.6261\n",
            "Epoch 291/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7691 - accuracy: 0.6426 - val_loss: 0.8006 - val_accuracy: 0.6309\n",
            "Epoch 292/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7691 - accuracy: 0.6417 - val_loss: 0.7959 - val_accuracy: 0.6301\n",
            "Epoch 293/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7690 - accuracy: 0.6425 - val_loss: 0.7951 - val_accuracy: 0.6302\n",
            "Epoch 294/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7687 - accuracy: 0.6433 - val_loss: 0.7993 - val_accuracy: 0.6304\n",
            "Epoch 295/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7687 - accuracy: 0.6450 - val_loss: 0.7942 - val_accuracy: 0.6313\n",
            "Epoch 296/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7683 - accuracy: 0.6446 - val_loss: 0.7954 - val_accuracy: 0.6284\n",
            "Epoch 297/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7685 - accuracy: 0.6435 - val_loss: 0.7948 - val_accuracy: 0.6298\n",
            "Epoch 298/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7683 - accuracy: 0.6430 - val_loss: 0.7940 - val_accuracy: 0.6316\n",
            "Epoch 299/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7685 - accuracy: 0.6428 - val_loss: 0.7939 - val_accuracy: 0.6298\n",
            "Epoch 300/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7677 - accuracy: 0.6430 - val_loss: 0.7954 - val_accuracy: 0.6302\n",
            "Epoch 301/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7682 - accuracy: 0.6428 - val_loss: 0.7954 - val_accuracy: 0.6264\n",
            "Epoch 302/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7678 - accuracy: 0.6435 - val_loss: 0.7951 - val_accuracy: 0.6293\n",
            "Epoch 303/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7679 - accuracy: 0.6440 - val_loss: 0.7973 - val_accuracy: 0.6310\n",
            "Epoch 304/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7678 - accuracy: 0.6426 - val_loss: 0.7950 - val_accuracy: 0.6313\n",
            "Epoch 305/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7677 - accuracy: 0.6416 - val_loss: 0.7954 - val_accuracy: 0.6299\n",
            "Epoch 306/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7672 - accuracy: 0.6428 - val_loss: 0.8060 - val_accuracy: 0.6266\n",
            "Epoch 307/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7678 - accuracy: 0.6425 - val_loss: 0.8055 - val_accuracy: 0.6272\n",
            "Epoch 308/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7676 - accuracy: 0.6437 - val_loss: 0.7992 - val_accuracy: 0.6299\n",
            "Epoch 309/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7673 - accuracy: 0.6445 - val_loss: 0.7949 - val_accuracy: 0.6302\n",
            "Epoch 310/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7674 - accuracy: 0.6430 - val_loss: 0.7947 - val_accuracy: 0.6310\n",
            "Epoch 311/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7672 - accuracy: 0.6415 - val_loss: 0.8006 - val_accuracy: 0.6287\n",
            "Epoch 312/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7669 - accuracy: 0.6443 - val_loss: 0.7958 - val_accuracy: 0.6316\n",
            "Epoch 313/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7671 - accuracy: 0.6429 - val_loss: 0.7953 - val_accuracy: 0.6293\n",
            "Epoch 314/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7666 - accuracy: 0.6458 - val_loss: 0.7950 - val_accuracy: 0.6293\n",
            "Epoch 315/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7662 - accuracy: 0.6425 - val_loss: 0.7945 - val_accuracy: 0.6305\n",
            "Epoch 316/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7668 - accuracy: 0.6441 - val_loss: 0.7980 - val_accuracy: 0.6277\n",
            "Epoch 317/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7664 - accuracy: 0.6445 - val_loss: 0.7949 - val_accuracy: 0.6325\n",
            "Epoch 318/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7659 - accuracy: 0.6435 - val_loss: 0.8048 - val_accuracy: 0.6251\n",
            "Epoch 319/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7662 - accuracy: 0.6452 - val_loss: 0.7981 - val_accuracy: 0.6269\n",
            "Epoch 320/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7668 - accuracy: 0.6426 - val_loss: 0.7948 - val_accuracy: 0.6307\n",
            "Epoch 321/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7658 - accuracy: 0.6456 - val_loss: 0.7950 - val_accuracy: 0.6284\n",
            "Epoch 322/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7658 - accuracy: 0.6435 - val_loss: 0.7971 - val_accuracy: 0.6301\n",
            "Epoch 323/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7658 - accuracy: 0.6432 - val_loss: 0.7964 - val_accuracy: 0.6287\n",
            "Epoch 324/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7656 - accuracy: 0.6426 - val_loss: 0.8006 - val_accuracy: 0.6302\n",
            "Epoch 325/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7661 - accuracy: 0.6426 - val_loss: 0.8053 - val_accuracy: 0.6240\n",
            "Epoch 326/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7660 - accuracy: 0.6444 - val_loss: 0.7988 - val_accuracy: 0.6287\n",
            "Epoch 327/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7657 - accuracy: 0.6433 - val_loss: 0.7990 - val_accuracy: 0.6287\n",
            "Epoch 328/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7654 - accuracy: 0.6446 - val_loss: 0.7955 - val_accuracy: 0.6307\n",
            "Epoch 329/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7653 - accuracy: 0.6444 - val_loss: 0.7954 - val_accuracy: 0.6278\n",
            "Epoch 330/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7651 - accuracy: 0.6436 - val_loss: 0.7951 - val_accuracy: 0.6309\n",
            "Epoch 331/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7648 - accuracy: 0.6447 - val_loss: 0.7965 - val_accuracy: 0.6264\n",
            "Epoch 332/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7647 - accuracy: 0.6442 - val_loss: 0.8050 - val_accuracy: 0.6286\n",
            "Epoch 333/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7647 - accuracy: 0.6441 - val_loss: 0.7947 - val_accuracy: 0.6304\n",
            "Epoch 334/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7650 - accuracy: 0.6428 - val_loss: 0.7980 - val_accuracy: 0.6275\n",
            "Epoch 335/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7642 - accuracy: 0.6446 - val_loss: 0.7972 - val_accuracy: 0.6286\n",
            "Epoch 336/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7644 - accuracy: 0.6448 - val_loss: 0.7963 - val_accuracy: 0.6299\n",
            "Epoch 337/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7641 - accuracy: 0.6435 - val_loss: 0.7987 - val_accuracy: 0.6278\n",
            "Epoch 338/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7649 - accuracy: 0.6435 - val_loss: 0.8092 - val_accuracy: 0.6242\n",
            "Epoch 339/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7645 - accuracy: 0.6445 - val_loss: 0.7961 - val_accuracy: 0.6280\n",
            "Epoch 340/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7640 - accuracy: 0.6452 - val_loss: 0.7974 - val_accuracy: 0.6315\n",
            "Epoch 341/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7642 - accuracy: 0.6454 - val_loss: 0.7973 - val_accuracy: 0.6249\n",
            "Epoch 342/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7642 - accuracy: 0.6449 - val_loss: 0.7949 - val_accuracy: 0.6286\n",
            "Epoch 343/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7640 - accuracy: 0.6452 - val_loss: 0.8063 - val_accuracy: 0.6260\n",
            "Epoch 344/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7642 - accuracy: 0.6432 - val_loss: 0.7970 - val_accuracy: 0.6312\n",
            "Epoch 345/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7637 - accuracy: 0.6442 - val_loss: 0.8021 - val_accuracy: 0.6289\n",
            "Epoch 346/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7634 - accuracy: 0.6466 - val_loss: 0.7969 - val_accuracy: 0.6287\n",
            "Epoch 347/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7637 - accuracy: 0.6469 - val_loss: 0.7964 - val_accuracy: 0.6267\n",
            "Epoch 348/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7633 - accuracy: 0.6463 - val_loss: 0.7953 - val_accuracy: 0.6299\n",
            "Epoch 349/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7633 - accuracy: 0.6449 - val_loss: 0.7974 - val_accuracy: 0.6309\n",
            "Epoch 350/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7629 - accuracy: 0.6445 - val_loss: 0.7976 - val_accuracy: 0.6271\n",
            "Epoch 351/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7629 - accuracy: 0.6469 - val_loss: 0.8011 - val_accuracy: 0.6248\n",
            "Epoch 352/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7627 - accuracy: 0.6464 - val_loss: 0.7948 - val_accuracy: 0.6296\n",
            "Epoch 353/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7630 - accuracy: 0.6452 - val_loss: 0.7952 - val_accuracy: 0.6307\n",
            "Epoch 354/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7627 - accuracy: 0.6452 - val_loss: 0.7961 - val_accuracy: 0.6298\n",
            "Epoch 355/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7625 - accuracy: 0.6462 - val_loss: 0.7991 - val_accuracy: 0.6271\n",
            "Epoch 356/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7628 - accuracy: 0.6480 - val_loss: 0.8015 - val_accuracy: 0.6269\n",
            "Epoch 357/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7620 - accuracy: 0.6456 - val_loss: 0.7956 - val_accuracy: 0.6298\n",
            "Epoch 358/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7621 - accuracy: 0.6455 - val_loss: 0.8063 - val_accuracy: 0.6228\n",
            "Epoch 359/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7622 - accuracy: 0.6474 - val_loss: 0.8001 - val_accuracy: 0.6258\n",
            "Epoch 360/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7618 - accuracy: 0.6471 - val_loss: 0.7970 - val_accuracy: 0.6278\n",
            "Epoch 361/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7618 - accuracy: 0.6461 - val_loss: 0.8091 - val_accuracy: 0.6240\n",
            "Epoch 362/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7618 - accuracy: 0.6468 - val_loss: 0.8021 - val_accuracy: 0.6261\n",
            "Epoch 363/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7615 - accuracy: 0.6450 - val_loss: 0.8040 - val_accuracy: 0.6266\n",
            "Epoch 364/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7615 - accuracy: 0.6453 - val_loss: 0.7975 - val_accuracy: 0.6274\n",
            "Epoch 365/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7618 - accuracy: 0.6447 - val_loss: 0.8105 - val_accuracy: 0.6242\n",
            "Epoch 366/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7616 - accuracy: 0.6457 - val_loss: 0.7986 - val_accuracy: 0.6257\n",
            "Epoch 367/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7610 - accuracy: 0.6469 - val_loss: 0.8008 - val_accuracy: 0.6283\n",
            "Epoch 368/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7615 - accuracy: 0.6468 - val_loss: 0.7960 - val_accuracy: 0.6298\n",
            "Epoch 369/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7606 - accuracy: 0.6441 - val_loss: 0.7981 - val_accuracy: 0.6287\n",
            "Epoch 370/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7612 - accuracy: 0.6458 - val_loss: 0.7965 - val_accuracy: 0.6304\n",
            "Epoch 371/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7606 - accuracy: 0.6463 - val_loss: 0.7958 - val_accuracy: 0.6295\n",
            "Epoch 372/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7607 - accuracy: 0.6468 - val_loss: 0.7987 - val_accuracy: 0.6272\n",
            "Epoch 373/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7603 - accuracy: 0.6471 - val_loss: 0.7994 - val_accuracy: 0.6299\n",
            "Epoch 374/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7606 - accuracy: 0.6455 - val_loss: 0.7990 - val_accuracy: 0.6267\n",
            "Epoch 375/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7600 - accuracy: 0.6480 - val_loss: 0.7958 - val_accuracy: 0.6302\n",
            "Epoch 376/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7601 - accuracy: 0.6478 - val_loss: 0.8007 - val_accuracy: 0.6284\n",
            "Epoch 377/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7602 - accuracy: 0.6462 - val_loss: 0.7998 - val_accuracy: 0.6257\n",
            "Epoch 378/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7595 - accuracy: 0.6471 - val_loss: 0.7978 - val_accuracy: 0.6281\n",
            "Epoch 379/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7599 - accuracy: 0.6494 - val_loss: 0.7967 - val_accuracy: 0.6296\n",
            "Epoch 380/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7603 - accuracy: 0.6456 - val_loss: 0.8000 - val_accuracy: 0.6284\n",
            "Epoch 381/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7599 - accuracy: 0.6467 - val_loss: 0.8011 - val_accuracy: 0.6283\n",
            "Epoch 382/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7589 - accuracy: 0.6480 - val_loss: 0.7983 - val_accuracy: 0.6281\n",
            "Epoch 383/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7591 - accuracy: 0.6469 - val_loss: 0.7973 - val_accuracy: 0.6296\n",
            "Epoch 384/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7596 - accuracy: 0.6465 - val_loss: 0.7983 - val_accuracy: 0.6302\n",
            "Epoch 385/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7586 - accuracy: 0.6469 - val_loss: 0.7998 - val_accuracy: 0.6271\n",
            "Epoch 386/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7593 - accuracy: 0.6482 - val_loss: 0.7977 - val_accuracy: 0.6305\n",
            "Epoch 387/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7588 - accuracy: 0.6469 - val_loss: 0.7973 - val_accuracy: 0.6312\n",
            "Epoch 388/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7587 - accuracy: 0.6464 - val_loss: 0.8116 - val_accuracy: 0.6248\n",
            "Epoch 389/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7590 - accuracy: 0.6473 - val_loss: 0.7968 - val_accuracy: 0.6296\n",
            "Epoch 390/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7585 - accuracy: 0.6469 - val_loss: 0.7963 - val_accuracy: 0.6304\n",
            "Epoch 391/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7584 - accuracy: 0.6493 - val_loss: 0.7990 - val_accuracy: 0.6298\n",
            "Epoch 392/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7580 - accuracy: 0.6484 - val_loss: 0.7982 - val_accuracy: 0.6315\n",
            "Epoch 393/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7584 - accuracy: 0.6482 - val_loss: 0.7997 - val_accuracy: 0.6258\n",
            "Epoch 394/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7583 - accuracy: 0.6482 - val_loss: 0.8131 - val_accuracy: 0.6254\n",
            "Epoch 395/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7582 - accuracy: 0.6472 - val_loss: 0.7970 - val_accuracy: 0.6307\n",
            "Epoch 396/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7581 - accuracy: 0.6472 - val_loss: 0.7969 - val_accuracy: 0.6304\n",
            "Epoch 397/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7574 - accuracy: 0.6481 - val_loss: 0.7997 - val_accuracy: 0.6293\n",
            "Epoch 398/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7580 - accuracy: 0.6469 - val_loss: 0.8079 - val_accuracy: 0.6249\n",
            "Epoch 399/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7577 - accuracy: 0.6490 - val_loss: 0.7978 - val_accuracy: 0.6275\n",
            "Epoch 400/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7572 - accuracy: 0.6470 - val_loss: 0.8007 - val_accuracy: 0.6274\n",
            "Epoch 401/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7570 - accuracy: 0.6482 - val_loss: 0.8041 - val_accuracy: 0.6258\n",
            "Epoch 402/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7571 - accuracy: 0.6483 - val_loss: 0.7973 - val_accuracy: 0.6298\n",
            "Epoch 403/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7566 - accuracy: 0.6474 - val_loss: 0.7989 - val_accuracy: 0.6302\n",
            "Epoch 404/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7567 - accuracy: 0.6492 - val_loss: 0.7994 - val_accuracy: 0.6274\n",
            "Epoch 405/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7570 - accuracy: 0.6486 - val_loss: 0.7969 - val_accuracy: 0.6284\n",
            "Epoch 406/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7569 - accuracy: 0.6485 - val_loss: 0.7975 - val_accuracy: 0.6293\n",
            "Epoch 407/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7565 - accuracy: 0.6473 - val_loss: 0.8006 - val_accuracy: 0.6269\n",
            "Epoch 408/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7560 - accuracy: 0.6508 - val_loss: 0.7980 - val_accuracy: 0.6287\n",
            "Epoch 409/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7566 - accuracy: 0.6514 - val_loss: 0.8002 - val_accuracy: 0.6286\n",
            "Epoch 410/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7565 - accuracy: 0.6494 - val_loss: 0.8056 - val_accuracy: 0.6249\n",
            "Epoch 411/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7563 - accuracy: 0.6492 - val_loss: 0.8000 - val_accuracy: 0.6295\n",
            "Epoch 412/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7563 - accuracy: 0.6477 - val_loss: 0.8049 - val_accuracy: 0.6252\n",
            "Epoch 413/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7558 - accuracy: 0.6485 - val_loss: 0.7975 - val_accuracy: 0.6292\n",
            "Epoch 414/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7555 - accuracy: 0.6477 - val_loss: 0.7976 - val_accuracy: 0.6302\n",
            "Epoch 415/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7554 - accuracy: 0.6499 - val_loss: 0.8016 - val_accuracy: 0.6284\n",
            "Epoch 416/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7557 - accuracy: 0.6505 - val_loss: 0.8059 - val_accuracy: 0.6281\n",
            "Epoch 417/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7554 - accuracy: 0.6493 - val_loss: 0.8005 - val_accuracy: 0.6310\n",
            "Epoch 418/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7555 - accuracy: 0.6488 - val_loss: 0.8029 - val_accuracy: 0.6267\n",
            "Epoch 419/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7551 - accuracy: 0.6501 - val_loss: 0.8110 - val_accuracy: 0.6274\n",
            "Epoch 420/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7552 - accuracy: 0.6503 - val_loss: 0.8069 - val_accuracy: 0.6272\n",
            "Epoch 421/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7548 - accuracy: 0.6485 - val_loss: 0.8037 - val_accuracy: 0.6277\n",
            "Epoch 422/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7549 - accuracy: 0.6517 - val_loss: 0.8029 - val_accuracy: 0.6251\n",
            "Epoch 423/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7543 - accuracy: 0.6508 - val_loss: 0.8170 - val_accuracy: 0.6222\n",
            "Epoch 424/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7548 - accuracy: 0.6491 - val_loss: 0.7989 - val_accuracy: 0.6292\n",
            "Epoch 425/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7543 - accuracy: 0.6471 - val_loss: 0.7981 - val_accuracy: 0.6287\n",
            "Epoch 426/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7543 - accuracy: 0.6497 - val_loss: 0.8045 - val_accuracy: 0.6260\n",
            "Epoch 427/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7544 - accuracy: 0.6512 - val_loss: 0.7996 - val_accuracy: 0.6301\n",
            "Epoch 428/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7537 - accuracy: 0.6498 - val_loss: 0.7987 - val_accuracy: 0.6281\n",
            "Epoch 429/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7540 - accuracy: 0.6514 - val_loss: 0.8165 - val_accuracy: 0.6195\n",
            "Epoch 430/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7539 - accuracy: 0.6506 - val_loss: 0.8019 - val_accuracy: 0.6287\n",
            "Epoch 431/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7531 - accuracy: 0.6512 - val_loss: 0.8098 - val_accuracy: 0.6255\n",
            "Epoch 432/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7536 - accuracy: 0.6503 - val_loss: 0.7984 - val_accuracy: 0.6278\n",
            "Epoch 433/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7533 - accuracy: 0.6508 - val_loss: 0.7996 - val_accuracy: 0.6296\n",
            "Epoch 434/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7530 - accuracy: 0.6498 - val_loss: 0.7984 - val_accuracy: 0.6309\n",
            "Epoch 435/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7534 - accuracy: 0.6501 - val_loss: 0.8141 - val_accuracy: 0.6195\n",
            "Epoch 436/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7531 - accuracy: 0.6503 - val_loss: 0.7995 - val_accuracy: 0.6292\n",
            "Epoch 437/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7531 - accuracy: 0.6503 - val_loss: 0.8011 - val_accuracy: 0.6263\n",
            "Epoch 438/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7522 - accuracy: 0.6517 - val_loss: 0.8005 - val_accuracy: 0.6277\n",
            "Epoch 439/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7530 - accuracy: 0.6511 - val_loss: 0.7988 - val_accuracy: 0.6281\n",
            "Epoch 440/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7531 - accuracy: 0.6520 - val_loss: 0.8061 - val_accuracy: 0.6229\n",
            "Epoch 441/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7526 - accuracy: 0.6498 - val_loss: 0.7985 - val_accuracy: 0.6280\n",
            "Epoch 442/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7520 - accuracy: 0.6510 - val_loss: 0.7989 - val_accuracy: 0.6290\n",
            "Epoch 443/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7526 - accuracy: 0.6496 - val_loss: 0.8005 - val_accuracy: 0.6287\n",
            "Epoch 444/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7514 - accuracy: 0.6514 - val_loss: 0.8025 - val_accuracy: 0.6296\n",
            "Epoch 445/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7517 - accuracy: 0.6512 - val_loss: 0.7999 - val_accuracy: 0.6283\n",
            "Epoch 446/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7516 - accuracy: 0.6499 - val_loss: 0.8015 - val_accuracy: 0.6284\n",
            "Epoch 447/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7517 - accuracy: 0.6493 - val_loss: 0.8048 - val_accuracy: 0.6280\n",
            "Epoch 448/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7514 - accuracy: 0.6491 - val_loss: 0.7997 - val_accuracy: 0.6287\n",
            "Epoch 449/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7513 - accuracy: 0.6515 - val_loss: 0.8093 - val_accuracy: 0.6248\n",
            "Epoch 450/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7515 - accuracy: 0.6510 - val_loss: 0.8007 - val_accuracy: 0.6280\n",
            "Epoch 451/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7509 - accuracy: 0.6524 - val_loss: 0.7998 - val_accuracy: 0.6295\n",
            "Epoch 452/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7512 - accuracy: 0.6502 - val_loss: 0.8023 - val_accuracy: 0.6277\n",
            "Epoch 453/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7504 - accuracy: 0.6516 - val_loss: 0.8140 - val_accuracy: 0.6223\n",
            "Epoch 454/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7504 - accuracy: 0.6506 - val_loss: 0.8315 - val_accuracy: 0.6179\n",
            "Epoch 455/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7506 - accuracy: 0.6506 - val_loss: 0.7990 - val_accuracy: 0.6298\n",
            "Epoch 456/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7503 - accuracy: 0.6517 - val_loss: 0.8166 - val_accuracy: 0.6258\n",
            "Epoch 457/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7502 - accuracy: 0.6519 - val_loss: 0.8000 - val_accuracy: 0.6278\n",
            "Epoch 458/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7511 - accuracy: 0.6491 - val_loss: 0.8061 - val_accuracy: 0.6261\n",
            "Epoch 459/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7501 - accuracy: 0.6515 - val_loss: 0.8039 - val_accuracy: 0.6275\n",
            "Epoch 460/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7500 - accuracy: 0.6521 - val_loss: 0.8009 - val_accuracy: 0.6307\n",
            "Epoch 461/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7497 - accuracy: 0.6496 - val_loss: 0.8087 - val_accuracy: 0.6272\n",
            "Epoch 462/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7495 - accuracy: 0.6521 - val_loss: 0.8148 - val_accuracy: 0.6234\n",
            "Epoch 463/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7492 - accuracy: 0.6505 - val_loss: 0.8017 - val_accuracy: 0.6280\n",
            "Epoch 464/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7492 - accuracy: 0.6527 - val_loss: 0.8049 - val_accuracy: 0.6287\n",
            "Epoch 465/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7491 - accuracy: 0.6525 - val_loss: 0.8079 - val_accuracy: 0.6252\n",
            "Epoch 466/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7495 - accuracy: 0.6530 - val_loss: 0.8003 - val_accuracy: 0.6284\n",
            "Epoch 467/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7489 - accuracy: 0.6523 - val_loss: 0.7999 - val_accuracy: 0.6312\n",
            "Epoch 468/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7486 - accuracy: 0.6533 - val_loss: 0.8004 - val_accuracy: 0.6292\n",
            "Epoch 469/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7483 - accuracy: 0.6532 - val_loss: 0.8213 - val_accuracy: 0.6175\n",
            "Epoch 470/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7492 - accuracy: 0.6542 - val_loss: 0.8027 - val_accuracy: 0.6304\n",
            "Epoch 471/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7490 - accuracy: 0.6543 - val_loss: 0.8007 - val_accuracy: 0.6292\n",
            "Epoch 472/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7485 - accuracy: 0.6516 - val_loss: 0.8087 - val_accuracy: 0.6261\n",
            "Epoch 473/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7489 - accuracy: 0.6512 - val_loss: 0.8047 - val_accuracy: 0.6271\n",
            "Epoch 474/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7477 - accuracy: 0.6520 - val_loss: 0.8078 - val_accuracy: 0.6292\n",
            "Epoch 475/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7488 - accuracy: 0.6519 - val_loss: 0.8017 - val_accuracy: 0.6304\n",
            "Epoch 476/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7477 - accuracy: 0.6516 - val_loss: 0.8004 - val_accuracy: 0.6269\n",
            "Epoch 477/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7475 - accuracy: 0.6533 - val_loss: 0.8090 - val_accuracy: 0.6249\n",
            "Epoch 478/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7473 - accuracy: 0.6527 - val_loss: 0.8192 - val_accuracy: 0.6193\n",
            "Epoch 479/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7475 - accuracy: 0.6511 - val_loss: 0.8260 - val_accuracy: 0.6167\n",
            "Epoch 480/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7470 - accuracy: 0.6543 - val_loss: 0.8137 - val_accuracy: 0.6261\n",
            "Epoch 481/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7467 - accuracy: 0.6527 - val_loss: 0.8025 - val_accuracy: 0.6271\n",
            "Epoch 482/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7475 - accuracy: 0.6529 - val_loss: 0.8012 - val_accuracy: 0.6272\n",
            "Epoch 483/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7467 - accuracy: 0.6519 - val_loss: 0.8105 - val_accuracy: 0.6261\n",
            "Epoch 484/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7470 - accuracy: 0.6532 - val_loss: 0.8022 - val_accuracy: 0.6275\n",
            "Epoch 485/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7456 - accuracy: 0.6534 - val_loss: 0.8060 - val_accuracy: 0.6299\n",
            "Epoch 486/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7464 - accuracy: 0.6531 - val_loss: 0.8353 - val_accuracy: 0.6134\n",
            "Epoch 487/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7465 - accuracy: 0.6540 - val_loss: 0.8397 - val_accuracy: 0.6097\n",
            "Epoch 488/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7466 - accuracy: 0.6519 - val_loss: 0.8038 - val_accuracy: 0.6301\n",
            "Epoch 489/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7461 - accuracy: 0.6518 - val_loss: 0.8148 - val_accuracy: 0.6198\n",
            "Epoch 490/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7457 - accuracy: 0.6537 - val_loss: 0.8032 - val_accuracy: 0.6278\n",
            "Epoch 491/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7454 - accuracy: 0.6535 - val_loss: 0.8195 - val_accuracy: 0.6167\n",
            "Epoch 492/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7456 - accuracy: 0.6551 - val_loss: 0.8033 - val_accuracy: 0.6283\n",
            "Epoch 493/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7454 - accuracy: 0.6542 - val_loss: 0.8069 - val_accuracy: 0.6266\n",
            "Epoch 494/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7446 - accuracy: 0.6523 - val_loss: 0.8045 - val_accuracy: 0.6264\n",
            "Epoch 495/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7448 - accuracy: 0.6541 - val_loss: 0.8053 - val_accuracy: 0.6239\n",
            "Epoch 496/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7447 - accuracy: 0.6547 - val_loss: 0.8158 - val_accuracy: 0.6229\n",
            "Epoch 497/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7455 - accuracy: 0.6520 - val_loss: 0.8050 - val_accuracy: 0.6272\n",
            "Epoch 498/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7448 - accuracy: 0.6539 - val_loss: 0.8035 - val_accuracy: 0.6277\n",
            "Epoch 499/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7446 - accuracy: 0.6559 - val_loss: 0.8069 - val_accuracy: 0.6255\n",
            "Epoch 500/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7447 - accuracy: 0.6544 - val_loss: 0.8399 - val_accuracy: 0.6116\n",
            "Epoch 501/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7440 - accuracy: 0.6547 - val_loss: 0.8072 - val_accuracy: 0.6274\n",
            "Epoch 502/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7442 - accuracy: 0.6532 - val_loss: 0.8105 - val_accuracy: 0.6239\n",
            "Epoch 503/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7443 - accuracy: 0.6560 - val_loss: 0.8071 - val_accuracy: 0.6278\n",
            "Epoch 504/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7430 - accuracy: 0.6546 - val_loss: 0.8048 - val_accuracy: 0.6281\n",
            "Epoch 505/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7435 - accuracy: 0.6538 - val_loss: 0.8027 - val_accuracy: 0.6280\n",
            "Epoch 506/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7430 - accuracy: 0.6570 - val_loss: 0.8032 - val_accuracy: 0.6261\n",
            "Epoch 507/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7427 - accuracy: 0.6549 - val_loss: 0.8154 - val_accuracy: 0.6242\n",
            "Epoch 508/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7433 - accuracy: 0.6559 - val_loss: 0.8095 - val_accuracy: 0.6281\n",
            "Epoch 509/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7425 - accuracy: 0.6542 - val_loss: 0.8119 - val_accuracy: 0.6231\n",
            "Epoch 510/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7431 - accuracy: 0.6557 - val_loss: 0.8105 - val_accuracy: 0.6251\n",
            "Epoch 511/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7427 - accuracy: 0.6542 - val_loss: 0.8044 - val_accuracy: 0.6269\n",
            "Epoch 512/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7428 - accuracy: 0.6543 - val_loss: 0.8135 - val_accuracy: 0.6228\n",
            "Epoch 513/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7421 - accuracy: 0.6551 - val_loss: 0.8076 - val_accuracy: 0.6242\n",
            "Epoch 514/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7425 - accuracy: 0.6528 - val_loss: 0.8114 - val_accuracy: 0.6254\n",
            "Epoch 515/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7413 - accuracy: 0.6555 - val_loss: 0.8051 - val_accuracy: 0.6295\n",
            "Epoch 516/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7419 - accuracy: 0.6563 - val_loss: 0.8307 - val_accuracy: 0.6137\n",
            "Epoch 517/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7419 - accuracy: 0.6547 - val_loss: 0.8782 - val_accuracy: 0.5953\n",
            "Epoch 518/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7418 - accuracy: 0.6534 - val_loss: 0.8037 - val_accuracy: 0.6277\n",
            "Epoch 519/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7415 - accuracy: 0.6556 - val_loss: 0.8092 - val_accuracy: 0.6248\n",
            "Epoch 520/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7411 - accuracy: 0.6569 - val_loss: 0.8054 - val_accuracy: 0.6284\n",
            "Epoch 521/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7410 - accuracy: 0.6562 - val_loss: 0.8227 - val_accuracy: 0.6181\n",
            "Epoch 522/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7420 - accuracy: 0.6538 - val_loss: 0.8080 - val_accuracy: 0.6263\n",
            "Epoch 523/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7407 - accuracy: 0.6577 - val_loss: 0.8111 - val_accuracy: 0.6318\n",
            "Epoch 524/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7404 - accuracy: 0.6554 - val_loss: 0.8055 - val_accuracy: 0.6278\n",
            "Epoch 525/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7403 - accuracy: 0.6565 - val_loss: 0.8097 - val_accuracy: 0.6287\n",
            "Epoch 526/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7403 - accuracy: 0.6590 - val_loss: 0.8067 - val_accuracy: 0.6248\n",
            "Epoch 527/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7402 - accuracy: 0.6579 - val_loss: 0.8104 - val_accuracy: 0.6243\n",
            "Epoch 528/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7401 - accuracy: 0.6558 - val_loss: 0.8173 - val_accuracy: 0.6234\n",
            "Epoch 529/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7402 - accuracy: 0.6557 - val_loss: 0.8114 - val_accuracy: 0.6264\n",
            "Epoch 530/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7399 - accuracy: 0.6558 - val_loss: 0.8066 - val_accuracy: 0.6269\n",
            "Epoch 531/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7394 - accuracy: 0.6562 - val_loss: 0.8154 - val_accuracy: 0.6254\n",
            "Epoch 532/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7395 - accuracy: 0.6568 - val_loss: 0.8065 - val_accuracy: 0.6267\n",
            "Epoch 533/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7392 - accuracy: 0.6556 - val_loss: 0.8161 - val_accuracy: 0.6233\n",
            "Epoch 534/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7385 - accuracy: 0.6582 - val_loss: 0.8069 - val_accuracy: 0.6296\n",
            "Epoch 535/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7385 - accuracy: 0.6576 - val_loss: 0.8151 - val_accuracy: 0.6219\n",
            "Epoch 536/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7387 - accuracy: 0.6567 - val_loss: 0.8087 - val_accuracy: 0.6251\n",
            "Epoch 537/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7384 - accuracy: 0.6579 - val_loss: 0.8119 - val_accuracy: 0.6298\n",
            "Epoch 538/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7383 - accuracy: 0.6554 - val_loss: 0.8107 - val_accuracy: 0.6226\n",
            "Epoch 539/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7386 - accuracy: 0.6576 - val_loss: 0.8274 - val_accuracy: 0.6193\n",
            "Epoch 540/600\n",
            "237/237 [==============================] - 2s 9ms/step - loss: 0.7382 - accuracy: 0.6581 - val_loss: 0.8435 - val_accuracy: 0.6082\n",
            "Epoch 541/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7392 - accuracy: 0.6556 - val_loss: 0.8091 - val_accuracy: 0.6269\n",
            "Epoch 542/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7373 - accuracy: 0.6568 - val_loss: 0.8089 - val_accuracy: 0.6287\n",
            "Epoch 543/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7378 - accuracy: 0.6582 - val_loss: 0.8077 - val_accuracy: 0.6257\n",
            "Epoch 544/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7368 - accuracy: 0.6574 - val_loss: 0.8370 - val_accuracy: 0.6140\n",
            "Epoch 545/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7372 - accuracy: 0.6579 - val_loss: 0.8081 - val_accuracy: 0.6246\n",
            "Epoch 546/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7368 - accuracy: 0.6561 - val_loss: 0.8068 - val_accuracy: 0.6286\n",
            "Epoch 547/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7370 - accuracy: 0.6573 - val_loss: 0.8215 - val_accuracy: 0.6240\n",
            "Epoch 548/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7380 - accuracy: 0.6579 - val_loss: 0.8296 - val_accuracy: 0.6167\n",
            "Epoch 549/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7366 - accuracy: 0.6578 - val_loss: 0.8069 - val_accuracy: 0.6249\n",
            "Epoch 550/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7357 - accuracy: 0.6600 - val_loss: 0.8085 - val_accuracy: 0.6264\n",
            "Epoch 551/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7359 - accuracy: 0.6603 - val_loss: 0.8093 - val_accuracy: 0.6236\n",
            "Epoch 552/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7364 - accuracy: 0.6583 - val_loss: 0.8091 - val_accuracy: 0.6233\n",
            "Epoch 553/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7358 - accuracy: 0.6611 - val_loss: 0.8104 - val_accuracy: 0.6299\n",
            "Epoch 554/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7359 - accuracy: 0.6591 - val_loss: 0.8082 - val_accuracy: 0.6309\n",
            "Epoch 555/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7359 - accuracy: 0.6579 - val_loss: 0.8113 - val_accuracy: 0.6266\n",
            "Epoch 556/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7351 - accuracy: 0.6615 - val_loss: 0.8087 - val_accuracy: 0.6255\n",
            "Epoch 557/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7361 - accuracy: 0.6581 - val_loss: 0.8113 - val_accuracy: 0.6278\n",
            "Epoch 558/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7352 - accuracy: 0.6588 - val_loss: 0.8084 - val_accuracy: 0.6295\n",
            "Epoch 559/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7351 - accuracy: 0.6581 - val_loss: 0.8110 - val_accuracy: 0.6278\n",
            "Epoch 560/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7346 - accuracy: 0.6588 - val_loss: 0.8190 - val_accuracy: 0.6220\n",
            "Epoch 561/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7346 - accuracy: 0.6587 - val_loss: 0.8105 - val_accuracy: 0.6233\n",
            "Epoch 562/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7343 - accuracy: 0.6603 - val_loss: 0.8114 - val_accuracy: 0.6239\n",
            "Epoch 563/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7341 - accuracy: 0.6611 - val_loss: 0.8134 - val_accuracy: 0.6263\n",
            "Epoch 564/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7340 - accuracy: 0.6612 - val_loss: 0.8208 - val_accuracy: 0.6214\n",
            "Epoch 565/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7338 - accuracy: 0.6592 - val_loss: 0.8149 - val_accuracy: 0.6222\n",
            "Epoch 566/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7333 - accuracy: 0.6609 - val_loss: 0.8242 - val_accuracy: 0.6211\n",
            "Epoch 567/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7338 - accuracy: 0.6604 - val_loss: 0.8129 - val_accuracy: 0.6271\n",
            "Epoch 568/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7333 - accuracy: 0.6588 - val_loss: 0.8088 - val_accuracy: 0.6254\n",
            "Epoch 569/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7332 - accuracy: 0.6604 - val_loss: 0.8175 - val_accuracy: 0.6249\n",
            "Epoch 570/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7338 - accuracy: 0.6598 - val_loss: 0.8133 - val_accuracy: 0.6274\n",
            "Epoch 571/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7328 - accuracy: 0.6572 - val_loss: 0.8497 - val_accuracy: 0.6035\n",
            "Epoch 572/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7332 - accuracy: 0.6605 - val_loss: 0.8109 - val_accuracy: 0.6264\n",
            "Epoch 573/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7325 - accuracy: 0.6593 - val_loss: 0.8118 - val_accuracy: 0.6246\n",
            "Epoch 574/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7322 - accuracy: 0.6619 - val_loss: 0.8106 - val_accuracy: 0.6246\n",
            "Epoch 575/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7319 - accuracy: 0.6588 - val_loss: 0.8350 - val_accuracy: 0.6116\n",
            "Epoch 576/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7319 - accuracy: 0.6614 - val_loss: 0.8128 - val_accuracy: 0.6269\n",
            "Epoch 577/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7314 - accuracy: 0.6621 - val_loss: 0.8178 - val_accuracy: 0.6243\n",
            "Epoch 578/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7323 - accuracy: 0.6623 - val_loss: 0.8247 - val_accuracy: 0.6211\n",
            "Epoch 579/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7314 - accuracy: 0.6609 - val_loss: 0.8384 - val_accuracy: 0.6138\n",
            "Epoch 580/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7313 - accuracy: 0.6611 - val_loss: 0.8300 - val_accuracy: 0.6161\n",
            "Epoch 581/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7314 - accuracy: 0.6622 - val_loss: 0.8248 - val_accuracy: 0.6198\n",
            "Epoch 582/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7317 - accuracy: 0.6601 - val_loss: 0.8150 - val_accuracy: 0.6249\n",
            "Epoch 583/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7308 - accuracy: 0.6626 - val_loss: 0.8272 - val_accuracy: 0.6207\n",
            "Epoch 584/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7309 - accuracy: 0.6607 - val_loss: 0.8110 - val_accuracy: 0.6237\n",
            "Epoch 585/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7316 - accuracy: 0.6610 - val_loss: 0.8253 - val_accuracy: 0.6195\n",
            "Epoch 586/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7301 - accuracy: 0.6603 - val_loss: 0.8361 - val_accuracy: 0.6152\n",
            "Epoch 587/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7298 - accuracy: 0.6630 - val_loss: 0.8434 - val_accuracy: 0.6126\n",
            "Epoch 588/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7301 - accuracy: 0.6611 - val_loss: 0.8118 - val_accuracy: 0.6277\n",
            "Epoch 589/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7296 - accuracy: 0.6638 - val_loss: 0.8381 - val_accuracy: 0.6117\n",
            "Epoch 590/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7301 - accuracy: 0.6620 - val_loss: 0.8416 - val_accuracy: 0.6141\n",
            "Epoch 591/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7290 - accuracy: 0.6652 - val_loss: 0.8441 - val_accuracy: 0.6108\n",
            "Epoch 592/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7296 - accuracy: 0.6599 - val_loss: 0.8125 - val_accuracy: 0.6278\n",
            "Epoch 593/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7291 - accuracy: 0.6617 - val_loss: 0.8127 - val_accuracy: 0.6255\n",
            "Epoch 594/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7287 - accuracy: 0.6636 - val_loss: 0.8400 - val_accuracy: 0.6128\n",
            "Epoch 595/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7289 - accuracy: 0.6624 - val_loss: 0.8333 - val_accuracy: 0.6160\n",
            "Epoch 596/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7281 - accuracy: 0.6648 - val_loss: 0.8192 - val_accuracy: 0.6190\n",
            "Epoch 597/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7282 - accuracy: 0.6638 - val_loss: 0.8256 - val_accuracy: 0.6214\n",
            "Epoch 598/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7286 - accuracy: 0.6629 - val_loss: 0.8159 - val_accuracy: 0.6263\n",
            "Epoch 599/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7276 - accuracy: 0.6623 - val_loss: 0.8383 - val_accuracy: 0.6103\n",
            "Epoch 600/600\n",
            "237/237 [==============================] - 2s 8ms/step - loss: 0.7278 - accuracy: 0.6656 - val_loss: 0.8282 - val_accuracy: 0.6195\n",
            "74/74 [==============================] - 0s 4ms/step - loss: 0.8282 - accuracy: 0.6195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihr4fTW9JBhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# from keras.models import load_model\n",
        "\n",
        "# path='gdrive/My Drive/Colab Notebooks/epl'\n",
        "# n_path = path + '/model.h5'\n",
        "\n",
        "# if (os.path.isfile(n_path)):\n",
        "#   print(\"LOAding model\")\n",
        "#   model = load_model(n_path)\n",
        "#   model.summary()\n",
        "# else:\n",
        "#   create_model()\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yUuRDQWMnmt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "9df68ffa-4c3a-41ca-ad10-f3b545f1d068"
      },
      "source": [
        "model_stats(History)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBH2j1suS98v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "f8aaa125-9b8f-45cf-f769-c724fb5a32ba"
      },
      "source": [
        "model_stats(History_1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sO2A4vtibfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "659cee43-0251-486f-b414-bd2eea411911"
      },
      "source": [
        "model_stats(History_2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3zU9f3A8dfn9l1y2TshhD0DIqBofwKiuKriKEVUrLbuCi3WVe2gSh1VO7Qufq2DFgdi/WndiihuGTIEwgoJ2fsud7l99/n9ccmRQMYFEo7xeT4eeZC773pfQr7v72cLKSWKoijK8UsT6wAURVGU2FKJQFEU5TinEoGiKMpxTiUCRVGU45xKBIqiKMc5XawD6K20tDRZUFAQ6zAURVGOKuvWrauXUqZ3tu2oSwQFBQWsXbs21mEoiqIcVYQQpV1tU1VDiqIoxzmVCBRFUY5zKhEoiqIc5/qtjUAI8SxwPlArpRzbyfaRwHPAicA9UspH+isWRVH6j9/vp7y8HI/HE+tQFMBkMpGXl4der4/6mP5sLH4e+DuwtIvtjcAC4KJ+jEFRlH5WXl6O1WqloKAAIUSswzmuSSlpaGigvLycQYMGRX1cv1UNSSlXE77Zd7W9Vkq5BvD3VwyKovQ/j8dDamqqSgJHACEEqampvS6dHRVtBEKI64UQa4UQa+vq6mIdjqIo+1FJ4MhxML+LoyIRSCmXSCknSSknpad3Oh6iRzubdvLwmofxBFQ9pqIoSntHRSLoC1UNpWx543k2122KdSiKoihHlOMmEQz7rpY7V4TY/tU7sQ5FUZSjVCAQiHUI/aLfEoEQ4iXgK2CEEKJcCPEzIcSNQogbW7dnCSHKgVuB37Tuk9Bf8WTMPI+QBlwfrEStyqYox56LLrqIiRMnMmbMGJYsWQLAe++9x4knnsj48eM544wzAHA6nVxzzTUUFhYybtw4XnvtNQDi4+Mj51qxYgVXX301AFdffTU33ngjJ598MnfccQfffvstp5xyChMmTODUU09l+/btAASDQW677TbGjh3LuHHjePzxx/n444+56KJ9HSM//PBDLr744sPx4+iVfus+KqWc28P2aiCvv66/P21SEo7JIznxqyI+2/QmU8fPOlyXVpTjxh/+u4Wtlc19es7ROQn8/oIxPe737LPPkpKSgtvtZvLkycyaNYvrrruO1atXM2jQIBobw50Y77vvPhITE9m8eTMATU1NPZ67vLycL7/8Eq1WS3NzM5999hk6nY6PPvqIu+++m9dee40lS5ZQUlLChg0b0Ol0NDY2kpyczM0330xdXR3p6ek899xz/PSnPz20H0g/OG6qhgDG3vMA+pAgcNOvee/txwmGgrEOSVGUPvLYY48xfvx4pkyZQllZGUuWLGHq1KmR/vQpKSkAfPTRR/z85z+PHJecnNzjuWfPno1WqwXAbrcze/Zsxo4dy8KFC9myZUvkvDfccAM6nS5yPSEE8+bN49///jc2m42vvvqKc889t08/d1846mYfPRTxw0eS8dTjeG9diPVXT/LO359FO/cips35FXHG+J5PoChKt6J5cu8Pn3zyCR999BFfffUVFouF6dOnc8IJJ1BUVBT1Odp3u9y/H35cXFzk+9/+9recfvrpvP7665SUlDB9+vRuz3vNNddwwQUXYDKZmD17diRRHEmOqxIBQPb/nMGED1djv/Yi0puCDLr/Zb6cOYX/LL+PQOjYbAhSlGOd3W4nOTkZi8VCUVERX3/9NR6Ph9WrV7Nnzx6ASNXQzJkzeeKJJyLHtlUNZWZmsm3bNkKhEK+//nq318rNzQXg+eefj7w/c+ZMnnnmmUiDctv1cnJyyMnJYfHixVxzzTV996H70HGXCAD0iUlMue0BTlq9Fv9vbyEhoGPU715k6e0X0OJriXV4iqL00jnnnEMgEGDUqFHcddddTJkyhfT0dJYsWcIll1zC+PHjmTNnDgC/+c1vaGpqYuzYsYwfP55Vq1YB8OCDD3L++edz6qmnkp2d3eW17rjjDn79618zYcKEDr2Irr32WvLz8xk3bhzjx4/nxRdfjGy74oorGDBgAKNGjeqnn8ChEUdbD5pJkybJvl6YJuR2882tPyVp1QY+/9EIfnbff9CI4zJHKkqvbdu27Yi9wR0pbrnlFiZMmMDPfvazw3K9zn4nQoh1UspJne2v7naAxmxmyhPLsJ00nCn/2c7/vfuXWIekKMoxYuLEiWzatIkrr7wy1qF0SSWCVkKjYfJjL+A36wg88TxNnp67lCmKovRk3bp1rF69GqPRGOtQuqQSQTu6pCTirplH4e4Ab7/x51iHoyiKclioRLCfYT+9Bb9Bg+utd9UIZEVRjgsqEexHY7HgPnks47a0sK32+1iHoyiK0u9UIuhE/sVzSXTBhg+WxToURVGUfqcSQScyp59FUCtwfvllrENRFEXpdyoRdEJjseAcmkVWUT1OnzPW4SiK0ofazzKqhKlE0AXjhBMYVCPZVLEu1qEoinIMOpLWNjjyZj86QuRMmkrjy++yd8NnnDpoWqzDUZSjw7t3QfXmvj1nViGc+2CXm++66y4GDBgQmVF00aJF6HQ6Vq1aRVNTE36/n8WLFzNrVs9TzzudTmbNmtXpcUuXLuWRRx5BCMG4ceP417/+RU1NDTfeeCPFxcUAPPXUU+Tk5HD++efz/ffhziaPPPIITqeTRYsWRSbD+/zzz5k7dy7Dhw9n8eLF+Hw+UlNTWbZsGZmZmTidTubPn8/atWsRQvD73/8eu93Opk2b+Otf/wrA//7v/7J161b+8pdDHwCrEkEXkk+YRCPg/H4THHnrSCiK0mrOnDn88pe/jCSC5cuX8/7777NgwQISEhKor69nypQpXHjhhT0u7G4ymXj99dcPOG7r1q0sXryYL7/8krS0tMiEcgsWLGDatGm8/vrrBINBnE5nj+sb+Hw+2qbJaWpq4uuvv0YIwT/+8Q/+9Kc/8eijj3a6ZoJer+ePf/wjDz/8MHq9nueee45nnnnmUH98gEoEXdLn5OA3aJAlZbEORVGOHt08ufeXCRMmUFtbS2VlJXV1dSQnJ5OVlcXChQtZvXo1Go2GiooKampqyMrK6vZcUkruvvvuA477+OOPmT17NmlpacC+tQ0+/vhjli5dCoBWqyUxMbHHRNA2+R2EF7yZM2cOVVVV+Hy+yNoJH330ES+//HJkv7Y1E2bMmMFbb73FqFGj8Pv9FBYW9vKn1TmVCLogNBpcuakkVDUQCAXQadSPSlGOVLNnz2bFihVUV1czZ84cli1bRl1dHevWrUOv11NQUHDAGgOdOdjj2tPpdIRCocjr7tY2mD9/PrfeeisXXnghn3zyCYsWLer23Ndeey33338/I0eO7NMprVVjcXcG5pJdH6LKWRXrSBRF6cacOXN4+eWXWbFiBbNnz8Zut5ORkYFer2fVqlWUlpZGdZ6ujpsxYwavvvoqDQ0NwL61Bs444wyeeuopILxmsd1uJzMzk9raWhoaGvB6vbz11lvdXq9tbYMXXngh8n5XayacfPLJlJWV8eKLLzJ3brerAfeKSgTdiMsvIMUBJU27Yx2KoijdGDNmDA6Hg9zcXLKzs7niiitYu3YthYWFLF26lJEjR0Z1nq6OGzNmDPfccw/Tpk1j/Pjx3HrrrQD87W9/Y9WqVRQWFjJx4kS2bt2KXq/nd7/7HSeddBIzZ87s9tqLFi1i9uzZTJw4MVLtBF2vmQDw4x//mB/84AdRLbEZLbUeQTfKlj2H874/8f0z85k97ebDck1FOdqo9QgOr/PPP5+FCxdyxhlndLnPEbMegRDiWSFErRCi0wl7RNhjQohdQohNQogT+yuWg5U0cCgALeXRFSsVRVH6i81mY/jw4ZjN5m6TwMHozxbQ54G/A0u72H4uMKz162TgqdZ/jxiGnDwAfJUVMY5EUZS+tHnzZubNm9fhPaPRyDfffBOjiHqWlJTEjh07+uXc/ZYIpJSrhRAF3ewyC1gqw3VTXwshkoQQ2VLKI6ZlVp/Tum5pdW1sA1EUpU8VFhayYcOGWIdxxIhlY3Eu0L6TfnnrewcQQlwvhFgrhFhbV1d3WIID0JhMuK0G9LW2w3ZNRVGUw+2o6DUkpVwipZwkpZyUnp5+WK/tTUvA0uBSi9QoinLMimUiqAAGtHud1/reESWUmUqKPYjda491KIqiKP0ilongTeCq1t5DUwD7kdQ+0EafkUmyE2pcNbEORVGULqippQ9NvzUWCyFeAqYDaUKIcuD3gB5ASvk08A5wHrALcAF9N166D5kzs9F5ocZWzoiUEbEOR1EUpc/1W4lASjlXSpktpdRLKfOklP+UUj7dmgSQYT+XUg6RUhZKKQ/PKLFeSsgIdyFtrNoT40gURemJlJLbb7+dsWPHUlhYyCuvvAJAVVUVU6dO5YQTTmDs2LF89tlnBINBrr766si+fTGd89FKzaTWg4SsAbiAltrKWIeiKEe8h759iKLGoj4958iUkdx50p1R7fuf//yHDRs2sHHjRurr65k8eTJTp07lxRdf5Oyzz+aee+4hGAzicrnYsGEDFRUVkXUDbLbjt3fgUdFrKJZM6ZkAeOrUWAJFOdK1Lfii1WrJzMxk2rRprFmzhsmTJ/Pcc8+xaNEiNm/ejNVqZfDgwRQXFzN//nzee+89EhISYh1+zKgSQQ+0qakABBrqYxyJohz5on1yP9ymTp3K6tWrefvtt7n66qu59dZbueqqq9i4cSPvv/8+Tz/9NMuXL+fZZ5+NdagxoUoEPdC1LkBB0/FbbFSUo8Vpp53GK6+8QjAYpK6ujtWrV3PSSSdRWlpKZmYm1113Hddeey3r16+nvr6eUCjEpZdeyuLFi1m/fn2sw48ZVSLogbBY8Os1iCZHrENRFKUHF198MV999RXjx49HCMGf/vQnsrKyeOGFFyJLPMbHx7N06VIqKiq45pprIovIPPDAAzGOPnbUNNRRWPs/k9icE+Ca5WpuEkXZn5qG+shzxExDfSwJJMZhdvgIhoKxDkVRFKXPqUQQjeREElwSu09NM6EoyrFHJYIoaFNSSGyBRndjrENRFEXpcyoRRMGQmkaCCxrdDbEORVEUpc+pRBAFc1omuhA0NarRxYqiHHtUIohCfHoOAM215TGORFEUpe+pRBCFtkTgalBTUSuKcuxRiSAK+tZpJrz1ar4hRTnadbd2QUlJCWPHjj2M0RwZVCKIgjY5GYBAo+o1pCjKsUdNMRGFtkQQsqlxBIrSner778e7rW+noTaOGknW3Xd3uf2uu+5iwIAB/PznPwdg0aJF6HQ6Vq1aRVNTE36/n8WLFzNr1qxeXdfj8XDTTTexdu1adDodf/7znzn99NPZsmUL11xzDT6fj1AoxGuvvUZOTg4//vGPKS8vJxgM8tvf/pY5c+Yc0uc+nFQiiIImLo6gToPGruYbUpQjzZw5c/jlL38ZSQTLly/n/fffZ8GCBSQkJFBfX8+UKVO48MILEUJEfd4nnngCIQSbN2+mqKiIs846ix07dvD000/zi1/8giuuuAKfz0cwGOSdd94hJyeHt99+GwC7/eh6aFSJIApCCHxWI7pmd6xDUZQjWndP7v1lwoQJ1NbWUllZSV1dHcnJyWRlZbFw4UJWr16NRqOhoqKCmpoasrKyoj7v559/zvz58wEYOXIkAwcOZMeOHZxyyin88Y9/pLy8nEsuuYRhw4ZRWFjIr371K+68807OP/98TjvttP76uP1CtRFEKZgQh6nFjzfojXUoiqLsZ/bs2axYsYJXXnmFOXPmsGzZMurq6li3bh0bNmwgMzMTj8fTJ9e6/PLLefPNNzGbzZx33nl8/PHHDB8+nPXr11NYWMhvfvMb7r333j651uGiEkG0khJIcEmaPE2xjkRRlP3MmTOHl19+mRUrVjB79mzsdjsZGRno9XpWrVpFaWlpr8952mmnsWzZMgB27NjB3r17GTFiBMXFxQwePJgFCxYwa9YsNm3aRGVlJRaLhSuvvJLbb7/9qFvboFdVQ0KIZGCAlHJTP8VzxNImJWPdCw2eBrLioi9eKorS/8aMGYPD4SA3N5fs7GyuuOIKLrjgAgoLC5k0aRIjR47s9TlvvvlmbrrpJgoLC9HpdDz//PMYjUaWL1/Ov/71L/R6PVlZWdx9992sWbOG22+/HY1Gg16v56mnnuqHT9l/elyPQAjxCXAh4aSxDqgFvpBS3trjyYU4B/gboAX+IaV8cL/tA4FngXSgEbhSStnt8N1YrEcA8P3dv8Dz9ge4317CaXlHV/2fovQntR7Bkac/1iNIlFI2A5cAS6WUJwNn9nSQEEILPAGcC4wG5gohRu+32yOt5xwH3AscsUsEmbNyifOCza5GFyuKcmyJpmpIJ4TIBn4M3NOLc58E7JJSFgMIIV4GZgFb2+0zGmgrWawC/q8X5z+s4rPzaQScVWUwJtbRKIpyKDZv3sy8efM6vGc0Gvnmm29iFFFsRZMI7gXeBz6XUq4RQgwGdkZxXC5Q1u51OXDyfvtsJFzS+BtwMWAVQqRKKTvM9yyEuB64HiA/Pz+KS/e9uOw8GgFPtZqBVFH2J6XsVR/9WCssLGTDhmNz6dmDWX64x6ohKeWrUspxUsqbW18XSykvPYj4OnMbME0I8R0wDagADlgPUkq5REo5SUo5KT09vY8u3Tu6jAwA/LVqviFFac9kMtHQ0HBQNyClb0kpaWhowGQy9eq4HksEQog/AYsBN/AeMA5YKKX8dw+HVgAD2r3Oa32vfdCVhEsECCHigUullLaooz+M9K2JQNarxWkUpb28vDzKy8upq6uLdSgK4cScl5fXq2OiqRo6S0p5hxDiYqCE8I17NdBTIlgDDBNCDCKcAC4DLm+/gxAiDWiUUoaAXxPuQXRE0iQmEtBp0DYcXUPHFaW/6fV6Bg0aFOswlEMQTa+htmTxQ+BVKWVUd0IpZQC4hXD7wjZguZRyixDiXiHEha27TQe2CyF2AJnAH3sT/OEkhMCdZMJgc8U6FEVRlD4VTYngLSFEEeGqoZuEEOlAVGO1pZTvAO/s997v2n2/AlgRfbixFUhJwGKrJRAKoNOoaZoU5VjQ/MEHxE+diqaX9erHkmgai+8CTgUmSSn9QAvhbqDHHW16GkmOEFUtVbEORVGUPuBa/x0VC35B7Z8ejnUoMdVjIhBC6IErgVeEECuAnwHHZYupKSeXtGYos/d+3hJFUY48QVu4b4q/8vjuFh5NG8FTwETgydavE1vfO+4kjSzEGIDanZtjHYqiKH1ABgPhb3Ta2AYSY9FUdE+WUo5v9/pjIcTG/groSJY+bjKlgHPr93B6rKNRFOWQBcPDloTm+E4E0ZQIgkKIIW0vWkcWHzDo63hgGjackAB2lcQ6FEVR+oBsTQRoj+8Z+aMpEdwOrBJCFAMCGAhc069RHaE0JhO2zDgsJWp0saIcE0IhAIT2+O4F2OOnl1KuFEIMA0a0vrUdOL9fozqCuQsySd+6R3UhVZRjgAy0Vg0d5yWCqD69lNIrpdzU+uUF/tLPcR2xzCNHktYs2bX3uGwmUZRjivS2Lj2r2ggOytEzzWAfyznhVAB2r18Z40gURTlUIY87/M1hKhFU3nkXe3927WG5Vm8c7Kc/bqcZzJsQXp1M89+PkX5/jKNRFOVQSE+4RBBtG4GUEtnarnAw7G+8QcsXX3R/fikJeTz4DmKd5YPVZSIQQmwWQmzq5Gsz4XmBjkv6zAzWz8ij4KtSGpcujXU4iqIcgpA3qtlyIprfepudp01F+nz9Ek/V3fewffwJ7Jw6jd1nnxNJDP2tuxLB+cAFnXydDwzv98iOYM4bfkRZGjR9uirWoSjKca/5/Q/YOf10Qq4DJ4Tcc8ml7P3pT7s8tq1EIAPRle49W7YQbGgg2NxMqKXl4AJux71xI83vvRd5bX/9daTPR6i5GYCiUaOp//sT+Kursf/3v4d8va50WR6SUqp5FLpwWt5pvFvwN3I3bkIGgwjt8d3QpCixVPGLXwDgr67GOHhwh22erVs7OyRCtpYIoq3mDdSG1yyvffTP2F9/neHffI02MbG3IePeuJFAYyPlN90MgHXrWfsarvdT/8QTtHzxBe4NG4g79VR0qam9vl5Pju8+UwdpVMooGgckovH68ZeXxzocRTlqSSlpemV5nzxdh9zu3h/TViKIMhH4a8JjiOyvvw5AoKGx19cEKJlzWSQJAPj37sVbXNzl/sGmJgC8u3Yf1PV6ohLBQRBCkD1mMkC4Hu8QGo8U5XjW8vnnVP/+99T++eB6pLevP5edVA31JOQOHxN9iaDjYFLH+++xffJJBJ2Hlshavv4G97r1Xe+gCd+qvTujWS6+91QiOEgzpl4V+d5XomrRFOVgBOrqAQg6mqM+puXbbym74UZkIID07Gvs3b+NIJpG1lCzI7xvFIlASkmgpqbDe3V/e4yQw4G/vKzb41zr1nXbwFy9aBE199/f5Xbfnj0ABBv7Z+Lng+k1tEkIsalfojmKjM6fxOqLwvWRTWu+Cnf5OoiiqaIcz9qeyDUWS6fb/dXVOD/7rMN7zW+/g/PTT/GVlRF0OPada/9EEEXPnmBro2zI4WTPJZfiWv9d17Ha7V2eM9Qujg4xSEnNAw9QesWVNK3ofA0ubVpaj3ECGEeMIH3Bgqj27a1oeg291/p1RevXAauOHa9OXXg/1UlQ/cjDFJ/3Q3aeNpWgXa1p3PTSS7R8+22sw1AOk6DTedDdKdtuoBpz54mg5PLLKbvu+g7Vr54tWwDwlZYScjoj7zs//5ztJ08h0Fqf3r7doav4gs3hv1fP5s14tm6lZvHiyLG+/dr/2toHOhNoXdcgsm91NeXz52N7+WWalv4rHN/Kjzs9NnnuZV2et8M1+qk0AN0kAillaWvPoZlSyjuklJtbv+4Czuq3iI4ihZnj2XznhRgdHnx79hByOtl11tmEXC78+xUh2/Ns3Yp3167DGOnhVf2He9l71U9iHYZymOyYNJmym38OwK4zZ1J5510A+MrL8dd2P0FjW9VQ43PPYX/7baSU2F77T+RmHqgMrwbYtoCM9Pnwbt8OgL+0tMOTuP21/xCy23F/t4G6xx6n4Z//jGwLtrQQdLZQ9dvfEWhsJOTzEXK7CdnDJYK2qiEZCK9PUL5wIbvPnEn1vfexbcxYSi6bS8uXX3b5OYL7JYL6J5/C8eFH1Nz/QOS9rgaSmce1m+VfHDhpQ/Llc8ObdPour3+oohlOJ4QQP5BSftH64lRU20LEdRcs4o6aLWRuKOPCDQb0NjvbT5wIwJAPP8AwYADQWmzV6dAYDOy55FIARhVti1nc/SXYRRFZ6X8yFMJfURH5P9cT1/r1hBwO4qdNo/LXd+Nat46hH7wf9fVc679Dl5EBhBt9Afzl5djLy8l56EF2nzkT6P7/eVt3TADbihXos3OouuceEmfNIuehB/ftV1ePLiUF765dkZu2d3cxwmQ+4JzS66H+ySc7vOcvL8e7eze2V1/F9uqraBITCXVSevfu2EHQ4aBldbg6qunFFwFwb9iAe8OGLj9H0GYjUFeHZ9s2/BUVuNasCcfSQ9tD3LSpmCdMiLwWJhPS7e4QX+ZvfoOhYBBxp0zp9lyHIpob+s+AJ4UQJUKIUsKrlHU9QqMdIcQ5QojtQohdQoi7OtmeL4RYJYT4rrXt4bzehR97Zp2Ze+b9g5JLJnPlDW6++Z999X27Z55F6byrqHvySbZPmsz2ceNp+Oezke2+8gpKLpuLr6TkgPMGGhvZNnIUjpVH15xG/qr+W89ZhkKEOini+2traXrppUMegdn2NHqoPUAOVcjrZe+11+He/H1U+7u3bKHslluo//vf2T3zrE7/P3Wm9PIrKLvhRiDcHdK/dy++srLITaw70uej9PLL2X3mmfvibtdwW3HHHd0eX//0M1Tfe1/HjhYhievbb4Bw75j2bQOVt99O3WOP4fz0UwC0ycnYli+n+ve/P+DcncVfMvvHHZ7aO0sCbXZMPqnb2Olk3FDdo39m52lTKbv+Bqr/cG+kcRcg/4UXSLzkEgAMgwZF3o8/8wxyH3kEbXwcw74KlzYybvsVcVNPY8BTT5K2YD6Zd/8aodGQctU8jMOGdR/XIRDR/vEIIRIBpJRRVYILIbTADmAmUA6sAeZKKbe222cJ8J2U8ikhxGjgHSllQXfnnTRpkly7dm1UMR9OwVCQN3e/yZJ1TzJwfSU/qE9hbI0Bc40d2cWNxTR+HJ6Nm9CmppLz0EM4P16Jr3Qv8dOno4mPp+rXv8Y4bBiD//tm5BgpJaKT4mNnav/yV4xDh5B4wQVA+AYjhEAYDN0eF/J4qLr7btJumY9x8KBu94Vw/23vzp2Yx43DuXo1ZdffABx8iSfocLDnRz8i47bbsE6bRvnCWzGNGY2veA/Nb73FyG1bO/wMSuddhWvNGoZ88D6G/PyDuiaEn/pKLptL4qwLyXnooYM+j2frVirvvIu0W24h4ex9taghr5eml14i+fLL0RgM2P/7FiGng+S5c5GBAPVPP0Py5XPxFRdTeuU8TKNHYz37bJqWLWPop590+nvfffY5B8xJk/f0U1inT+8xzm0jRwEwcttWikaN7rBt4LJ/Yxo9Go05/MTd8vXX1D/9DAnnnIMuK5PK227vUD8P4ZtY7SOPHnCdwe++Q/lNNzPgH//AkJdLyzffsvcn4apDYTD0un1BEx9P/LRpNL/9dq+Oa2MaPbrHgWYAwmjsMMjLes45ZP9hEU0vvkjd3x47YH99Xh7+8nKE0YihoIDsxYvR5+WiS04m0NBAzf0PkPnru9DExxNyOtHt10gsg0HQaKL+++4tIcQ6KeWkzrb1WDUkhDAClwIFgK4tSCnlvT0cehKwS0pZ3Hqel4FZQPvfgAQSWr9PBI7aFaS1Gi0XD7uYHw7+Ia+d8BovFb3En+x7sAS1XNQ4krEZ4xn89HtoWzyR/1yejeHOV8GGBsqu3TcjYfu6RO/Ondj+7//wFm3H/tZbaEwmCl55GW1yMkKzr0AnAwHQahFCEGppwfnppzQ88wwA+qwszJMmsfvcczEMHMjA557r9rO41q6j+Z13aX7nXUZs+A7p91M67yoybl2IZeJE/DW1HRJE9aJF2N94k0Sbz+kAACAASURBVKGffoq/oqLDuZqWL0efmUn8tGlR/yy9RUX4S/dSMX8BA555GufKlTjblYwCdXXoW6skpN+Pa324/7W/srLTRCB9Pprf/4CEH54HQuDesAHnqk9Ivf56tPFxkf18ZeEugO4N+6YY7yrxeov3oM/NQWM0HrDNufozvDt3Uvvww5jHjkEYjejS0rCtWEHtgw9BMEjKVVdRefvtACT9+Me0fPEF9X//O/6yMixTwlUAwmik7i/h/vVVd91FzkMPEWhowLNlC/FTp9Lw3POdTkzmePe9DonAV14OwSDa1FScn3xKwlkzOzzVBmrrDjhH6RVXknH7baT+7Gf4SkvZe3V4LSrX118fsG+bzpIAQOllcwna7dQsXoy/uhpvUVFk28E0MhuHDMFQUNDtPm0JRp+fT/6zz1Lz4AM4P1qJafRoch56kOILLiTn4Yexv/4fPFu3EbTZSL3pRiwnnohj5UqSLr0UfXY2rnXr0ZhN6PPyIqOW0266ibSbbgo3KJeVIQwGtElJ6FJSuoxHl5pK7qOPRF5rTKYDY47hDAU9lgiEEO8BdmAd7ZaolFJ2/lvfd9yPgHOklNe2vp4HnCylvKXdPtnAB0AyEAecKaVc1915j9QSwf6klKyrWcen5Z/yYemHVDjDN0idRsfolNEMTxjCsOThDIsrIOO1z5Gbt2HIzUOblETTsmVdDjdvo01ORhMfD0JgGDAA9/ffYywoQJeTjePd97o9Nv7005F+P6nXXotx+DAIhfDt3Yvjo5XET53K3quvjuybNPcyQvZmmt8JdxQTZjPS7cYwaBA5rTe6nVOnEaitJe/vj+P89FNsr4a7yQ1++y2Kfxhew2j4N19je/VVjKNGUT5/AYNeeRl0Ospv/jmGIYPRxsVH6oSbli+n+nfhIr9x2FC8Ow9sWM959BG8O3fS8PQzkfcybvsViZdc0uEP0l9Rwa4zwtUXuX9+FDRaKn75SwDSFy4k9frrcH78Me5NmyOJE0CblET6woU0PPtPDAMHkjLvKtAINBYLgbo6Khb8Am1KCoYBA8h/4fnIH7avpISaBx6MVGEAaOLiGLb6U3ZOm07I6SRx1oXoBw6k/rHHAbCeew6WCRMiDYuWKVM6veHmv/ACe6+9Fvx+shffR9Vvftvl79g6cyaGgoHY33jzgEFQXf1MO6XXQxf13KaxY8lefB+Vd94VacDtC9aZMwk2NeFq93cuTCZyHv4TFfMXkHbzTSTOmkXZ9TcQbG4m+4H7MY8bh3fXLkItLZhGjkSfnd16kzaiz8wg6HDQ+NzzxE+fhnncuA4JPuh0Empxoc/M6LPPcCTqrkQQTSL4Xko59iAuGk0iuLU1hkeFEKcA/wTGSilD+53reuB6gPz8/Imlh3F61r4gpaTOXcfOpp18W/0tm+s3s6NpB3bvvlo2s87M+PTxjE4dzZCkIeiDgmxdKjmNknhpQJgtBBsbkD4frjVrwl3ZRLiu01ceTjIy4IeQRAYCBOvDvTHaqp8g/McEdBiEc6g0cXGRbnoaq7XL/tS6nOxID5CumMePD5/P68WzZQv6vFx8vRxSL/R6Mu68E/f69bg3beowBUj86afj2V7UIY7ky+fS9OJLvbrGAde0WEi77lpca9bQ8uVXAOhzczuUjozDhh04KlSrRZeaSqC2ttufXTSs557T4wMAEB6h2slIeH1ODv7KfQVyw6BBHeq5DQMHkn7br6i49VcIvZ5By1/BkJ+PMBjwV1RQNn8+GrMFfW4OqddeS8UvfonQ6YifMQOh1eL46CN0WZmkzJuHLj0d767d6LMyETodIY8H98ZNWM+YETknEP79VVYSP3060uNBm5SEv7ISbWpqpyUxpXuHmgiWAI9LKTf38qKnAIuklGe3vv41gJTygXb7bCGcLMpaXxcDU6SUXfY5O1pKBD1pSw47mnZQYi+hpLmETXWb2GnbSSAU6LBvhjkDjUbD0KShDE0aSpw+joKEAgYlhqtnkoxJpFvS0YiObf9BpxNNXBz+vXvDVUkGAyGHA9eGDYQcToLNdoROj/T70SYmYp5wArZXluPbuxfzCSdgHleI/c3/En/6dIwFBdjfeANfRQWpP/kJMiRp+ve/AIGvtBRDQQG+khJCHg9pN95I81tvRZ6KNQkJaOLjMA4dimfjJoJ2O7rsbPQZGbg3HrjSmzCZSLrkYoReT+MLS7GefTbGIUMO6AkCkP6LBdT97TGEXo95wgR8JSUHPAFHQ5OQQN7jj6PPzWH3mTMjN0bzxIlk/fY37Lno4g77W889h2CTrcOTe1tPD21qKln33I3z009xfLQS09ixeHfvJlhfj3Xmmbi/34IuPZ2se+7GMHgw1b9fhK+8HI3BgDAYSDjvXAxDhuBe/x2ODz8k8dJLMA4dStO//o115pk0/nsZQqcjee5luDduIvnyueESotEIWi3OTz7Bu307QXszyVdeiW35clJ/9lM827ZhOflkQs3NaKxWAnX1uDdtRGu1EjdlCu7Nm9HExYUbNAMBZCCAr6QEXWZmpJQVamlBSjpUqSlHh0NNBFuBocAewEt4dTIppRzXw3E6wo3FZwAVhBuLL5dSbmm3z7vAK1LK54UQo4CVQK7sJqhjJRF0xRf0UeYoIyRDVLVUsbNpJ9sbt4OA3bbdFNuLD0gUAPH6eHLjcwmEAuRac0kwJDDAOoBEYyIZlgxy4nNIMCRg1BpJM6cdkDQOJxkKgRDh9gyfDwIB0OnA70cGg2jMZoRej/T5CDQ0oMvKQvr9hOx2NImJCL0eIUS4frmt6kKnQ2g0yEAA744d6DIy0KWl4a+qQhMXh8ZsjtSVGwoKIpONBW1NaCyWDtVJgaYmtElJ+HbvRp+fj8ZgwFdaGj5PQgJBmw1dcnI4xkAAhMC3Z0+Hp1lFOdIcaiIY2Nn70UxT3dod9K+AFnhWSvlHIcS9wFop5ZutPYX+F4gn3HB8h5Tyg+7Oeawngp6EZAhf0EdJcwkl9hKEEDR5mthl20VVSxU6oaPcWY7T56SqpQrZyWJyBo0Bo9ZIZlwmeo2egoQCNBoNGZYM0kxpZMRlkG5OJ14fT5IxCY3QkGJKQXucr+uqKEezQ0oE7U6SAUSauqWUe/smvN453hNBbzh8DrxBL3WuOiqdlbQEWnD73ZQ7y/EFfVS2VOL0Odll24VZZ6beXY8/1HnDYJw+Dl/QR5o5DYEgw5JBsimZeH08icZEDFoDJq2J/IR8koxJJBmTSDQmkmRMIk4f129d4hRFic6hdh+9EHgUyAFqgYHANmBMXwap9D2rwYoVK2nmNEaljupx/5AM0extpthejDfopcXfQpO3CW/AS0lzCSatiXpPPQJBpbOSPfY9eINenD4nnqCnyySiEzoSjAnoNXosegvx+nji9fGkmlPRa/TkWfPwBX0kGBLIistCq9FGEk5OfA4WnQWTzhTT6ixFOZZFM8XEfcAU4CMp5QQhxOnAlf0blhILGqEhyZTEiaYTD+p4d8DNjqYdhGQIm8eGzWvD7rVj99mxeW20+FsiScPmtbHbtpuADNDoiW5xD5PWRIopBZPORLw+njh9HPGG8L9aoSXPmkeqKRWL3oJZFx4IlRWXhV6jJ8WUgk6jw6wzq4SiKPuJJhH4pZQNQgiNEEIjpVwlhPhrv0emHHXausD2ljvgRqfRYffaaXA3EJRB6lx1OP1ObF4bnoAHb9BLg7uBalc1Jq0Jp9+J0++k1lWLw+/A7Xfj8Pfc/TLFlILVYEUrtFgNViw6CwnGBJKMSWTFZZFkTCIkQ+g1esw6M6nmVDItmVgNVsw6M0atUVVzKcecaBKBTQgRD6wGlgkhaoHYTsaiHFPant7TzGmkmVuH3R/Esqw2jy3SDuIKuHAFXDS6GwnIAM3eZnwhHzuaduANeAnIAO6AmwZPAyXNJbgCrg7jOroiEJh0Jsw68wFfWXFZpJnTMOlMWHQWPAEPqeZU8qx55MXn4Q/5yY7Lps5dR158nkooyhEjmkQwC3ADCwmvR5AI9DS9hKIcdkmmJJJIOujjXX5XpJrKFXBh89ho8jbh8rtwB9wHfHkCnsj3Td4m3tvzHgEZICR7Xro0Xh+PQWtAILAarAxMGEiaOQ1/yE9WXBYOn4PsuGwMWgMaoSE3PtwlONmUTIYlg5AMYdFZVDJR+kSPiUBK2fb0HwJe6N9wFCV2LHoLFn3nC6T0hifgodHTSJo5jQZ3A+XOcvY27yUog1S3VGPSmah1hQe9hWSIJk8TpY5Svq//Ptz47nf2cIUwndCRn5BPqjkVg8aAXqvHoDFg0BpIt6STackk2ZhMijklUq1l0BgYkDAArdCqthIlIpoSgaIovWDSmciJzwEgOz6b7PhsJmdNjupYKSWeoAed0EXaPAKhAKXNpTh9Thx+BzUtNQghqHPVUeooxe13Y/fb8YV8+EN+Wnwt1Lp7Hl2dakrFarCSa83ForOg1+hJMiZh1BrJjs8mNz4Xk9ZEkimJBEMCQRkk2ZjcJ8lSObKoRKAoRxAhRKTNJEW7b7RzhiX6CdGklDj8jkhVV6OnEafPiT/kp9HTSIWzApvHhhACd8BNhbMCT8BDSIaocFaER3x3Ub0lEORZ85BSYjVYSTGlkGpOjYxYTzOnYTWEuywnG5PRCA2ZcZm0+Ft69RmUwyuacQQXAG/vPxGcoihHJiEECYaEyLiM3nAH3Bi1xkjCaPG30OhpxOaxEaePo7qlml22XUgkTp+TOncd3zd8TzAU7LFKK9GYSJYli6y4rEiDe7w+nvEZ44nTxVHmKGNEyggGJgwk2ZisRrIfRtGUCOYAfxVCvEZ4moiing5QFOXo1GkPrij5gj7KneU0uBtw+sLdeyWSosYi0sxpfFfzHeXOcmpcNXgCHjxBD02eJv697d8HnEsrwoMKMywZ1Lhq8Af9CCEYnDiYGfkzyI7Lxhv0YtKaGJU6Ck/QQ5o5jQRDQieRKT2JaooJIUQCMBe4hvCcQM8BL0kpD/sCtWqKCUU5drj8LkqbS/EGvRi0Bqpaqqhz1VHrqo186TQ6MuMyWV2+OtLI3hmBIDMuk0RDImmWNJo8TYxKGRWprhqaNJR8az5GnRGj1ojVYD2uGsz7aq6hVGAe8EvCU0wMBR6TUj7eV4FGQyUCRTl+SSmxeW3UuGpw+pzUe+opaihiaPJQyh3l7LHviVRpWfQWyh3l2Ly2Tts82to1Mi2ZJBmTsBqseIIezhx4JhadBY3QMCRxCMmmZEy6A1cUO9oc6uyjFxIuCQwFlgIvSClrhRAWYGtPawz3NZUIFEXpjWAoyC7bLkqbS6l11eIOuNFqtOy27WZH0w4sOguNnkZKmks6PV4jNBQkFGDRWdBpdFS1VDEocRAz8mdgNVjZbdvNT0aH12BOMh38OJb+dqiJ4AXgn1LK1Z1sO0NKubKTw/qNSgSKovQXl98VmZ3XG/Syo2kHjZ5GihqK8If8uANutjZsxRPsfJU/ndARkAEGWAcwKmUU+Qn5FCQUkGxKjiyPmRefR0FiwWGvljrURDAIqJJSelpfm4FMKWVJXwcaDZUIFEWJtWZfMxWOivDYjaCfLyq/wKg14vA52OvYi9vvpqqlikpnJQF54EJSycZkBiUOosXfEhlzkmHJIMGQwGl5p2HUGsmKyyLRkNhnvacONRGsBU6VUvpaXxuAL6SU0Y2Q6WMqESiKcrTwh/xUOCqocdWgFVrq3fXUu+vZadtJsa2YZl8zIRnCE/RQ3VJ9wPFWg5VMSyYJhgTyrHmckX8GM/JnHFQsh7QeAaBrSwIAUkpfazJQFEVRuqHX6ClILKAgsaDHfUMyxG7bbsod4S62jZ5Galw1lNhLcAfcfLz3YwZYBxx0IuhONImgTghxoZTyTQAhxCygvs8jURRFOY5phIZhycMYljzssF87mkRwI+Hpp/9OeOH6MuCqfo1KURRFOWyimX10NzCldU0CpJTRTY2oKIqiHBWimnROCPFDwmsUm9rmP5dSqjUJFEVRjgE9dmQVQjxNeL6h+YSrhmYTXsC+R0KIc4QQ24UQu4QQd3Wy/S9CiA2tXzuEELZexq8oiqIcomhKBKdKKccJITZJKf8ghHgUeLeng4QQWuAJYCZQDqwRQrwppdzato+UcmG7/ecDE3r9CRRFUZRDEs3QtrYhdC4hRA7gB7KjOO4kYJeUsri1++nLhJe97Mpc4KUozqsoiqL0oWgSwX+FEEnAw8B6oAR4MYrjcgn3MGpT3vreAYQQA4FBwMddbL9eCLFWCLG2rq4uiksriqIo0eq2akgIoQFWSiltwGtCiLcAk5TS3sdxXAaskFIGO9sopVwCLIHwyOI+vraiKMpxrdsSQeuqZE+0e+3tRRKoAAa0e53X+l5nLkNVCymKosRENFVDK4UQl4q2fqPRWwMME0IMap2S4jLgzf13EkKMBJKBr3p5fkVRFKUPRJMIbgBeBbxCiGYhhEMI0dzTQVLKAHAL8D7hhWyWSym3CCHubV3joM1lwMsy2hVyFEVRlD4V9QplRwo1+6iiKErvHdLso0KIqZ2939lCNYqiKMrRJ5oBZbe3+95EeHzAOqDv50JVFEVRDrtoJp27oP1rIcQA4K/9FpGiKIpyWB3MopnlwKi+DkRRFEXpWr3T22/njqaN4HGgrUVZA5xAeISxoiiKcgjaFrSXUuIPSiSSrZXNPP3pbnyBEJedlM/zX5QweVAKz32+hyumDOSuc0f2eRzRtBG076ITAF6SUn7R55EoiqIcAaSUSAkaTXRDp7yBIO9urmZDmY28ZDPrSpt44JJCjDotT32yi00VdrISTAzPtJKVaGJrZTP/+roUty88kUJeipniuhYMOg2+QKjDuVdtD0+p81VxAwAzRmb04SfdJ5pEsALwtE3/IITQCiEsUkpXv0SkKIrSRzz+ICa9lhZvgEqbG7c/yPBMK3qtBq1GYHP58AZCePxBdtU62VLZzLrSJrZWNfPwj8aRGmfkrU2VbK9xEGfU0ej0odGAWa/FF5QEgiEqbG5KGzreDt/9/sCF6DszY2QGa0oaATokgVMGp/JVcQOFuYncOnM4VpOOmmYvJw1K6bsfTjvRJIKVwJlA28pkZuAD4NR+iUhRlOOClJKQBG0nT952t5/yJhdVNg9rShpJjjNww9TBOL0BNpTZKKpykBJnID/Vwq5aJw6Pn1HZCTy5ajcSSYJJz576FnbXOQl1MlQqO9GEwxPA6Q10Gd/Vz62J6nMMSovjJ6eEl2hxeAJ8V2ZjcFoc6VYj54zNIjvRTIsvwJYKO5/trKfC5ubGaUNIjTNw6tA0ANy+IGaDllBIRkoidpefBLOO3k/q0HvRJAJT++UppZROIYSlH2NSFKUfODx+rCb9Ae83e/xYjftuOG03o1BI4vYH2dvo4s8f7uD8cdlcOD6H9XttFNc5SYs38sn2WjaW2/nVWcNZvaOO97ZUMyLTSlq8kZFZVlz+IG9vqmJnrROrUUdOkpmpw9No8Qb578ZKGlp8nDYsjWq7h5213a+C+/jKnbT4Op2XslsGrQZfMPy0nW414guEsLv9uFrPlZtkJiQlJwxI4rOd9dx93iimj0hnZVEtcQYtHn+I6SPScfuDrCttItliYFCahbxkC1urmpkwICmqm/WJ+cnMO6Wg021mgxboWB2VaDnwd9VfehxZLIT4ApgvpVzf+noi8Hcp5SmHIb4DqJHFytHIHwyh14Y76bX9zfXmSe+b4gaqmz1MHJjMutImTh2SxmMrd5KfYsHu9pORYOT0ERnEGXV8uLWaSQUp7G108U1xI0XVzVTbPRRVOzh/XDapcQZCElp8AYIhyRsbKslNMjMiy0qlzU1RtSNyXa1GEOzskbobGkGHp/A4g7bHG7hOIwi0Oygt3sDPTx9KWryRkwelsGR1MbvqnKTEGTBoNdQ0ezh3bDb1LV6mD89g/d4mapo9/PQHg9hZ60QjYEtlM7Mn5aERAin33Wzb2Fw+WnxBshJM+AIhzAZtpPH2WNTdyOJoEsFkwovKVBJeqjILmCOlXNfXgUZDJQIlWj39UUsp2VnrZEh6PBoB60qbGJoRz4YyGzXNHs4Zm42U4afj78vt2N1+NBpBrcPL6OwEshNN/O6NLYSkZN6UgcSbdGzYa2NElpVki4H/21CBSa/hrU1VlDa4GJllxebyU93s4aRBKcybMpAPttZQ7/Ci12mw6LXUODzoNIIxOYms2l6L3e3H7Qvi3a8Rsa/NHJ3JmpJGbC4/AFaTjuxEExlWE5U2N9dNHUy9w8t7W6rJSjDxo4l5ODwBmj1+hmbE8+C7RZw6JI3rpg4iO9FMMCQprnOSYNaTHm/E6QvQ6PQRZ9Rhc/koqnZw1phMNEKg12qQUrJ6Zz2FuYn4gyEyE0z9+nmPR4eUCFpPoAdGtL7cLqX092F8vaISwZEhEAyh0/Y8DMUbCOILhLCa9Lhan0DN+vCTmU6rwe7y4/D6+Wp3A2eNySIUknyxu57//WwPKRY9Ewcmk58aR4bVyLaqZlLjjZQ1unh7UxX5KRZM+nCx32rUs6nCzoBkMyuLasmwGnF6AowbkIheq8EfDOH1h8hMNPF9hT38hKjXsr3GgUZAfoqFkoaD6//QWW+PXh2v1aDTCnQaQbMnEHminjQwmZQ4Ax9srWF8XiI//Z9BvLa+gjE5CfgDIU7IT6La7uHUIWnsqHGwrrQJly/ImJwE3txYyU3Th6DTCNKtRjStCdEbCDEwdV/NrpSQbNFHfpdSSoIhiVYjevVkfCw/SR8rDrVE8HNgWeviNAghkoG5Uson+zzSKKhE0DtSSmqavWQmGAH4vqKZASlm4o06AiFJk8tHICixu/1YDFo+LqplT30Lg1obu1q8QXbUOPAFQwxOi6Os0UVpo4tPttdx5qgMZozM5LOddRh0GrRC8HVxA0kWA9uqm4k36pASXL4AwzKsbK8JVzloNQKDVoPbH0SI8M0IDu6GatZrsZp01DrCg22sRh0ObwCTXkNesgWLQcue+hZCIUlOkhm3P0hOohmjXkOLN0CV3UNBalyke97ILCtF1Q7G5yWSbjVSkBpHiy/IuWOzcPkCbK1sZtqIDEobwg2RP5o4gAyrkc0VdnyBEPkpFt7YUIlWA+eMzWJVUR0mvYYxuYmkWAzkJJnZVtVMXrKZLZXNZCWayE0yE2fUIaVkS2UzI7KsODwBki16hBDUO70kmfVRJV5F6cqhJoINUsoT9nvvOyllTBaaP5oTQWd1w1srm9FowKjTotMIWnwB/vHZHi4cn8PIbCtljS4yE0y8saGStHgDACUNLoIhSbxRx/YaB0atBm8wxKZyG7lJZprdARxeP+cVZvPtnka+22tjTE4Cu2qdkSoGk16DPyh7Xf/bnWSLHq1G02EE5A/HZZOVEH4K/2ZPuJtcfoqF5DgDWyrs/GhiHgNSLCSa9exqbSx0+4LcMmMoO2sd1Dt9DEmPo8LmYVSWlbImFxZDuNri2z2NXDQhF40QNDi9pMUbCUmJ0xvAbNBi1IVLHsFQ+HMadJ3fSKWUrC1tYkxOAhaDbt/TraMa4jOhr550A15AgM7QN+dTlF44pNlHAa0QQrStFyCE0ALqf3I7bTeOnTUOvtzdQEhKah1ehmXEMzzTyrJv9gKSb/c0UtLg4sT8JHKSzJQ3uVlX2tTpOVesK+91HAWpFrZVObC7wzV3z3xaHNkWkuEqCK1GMH/GMErqW5BIBqXF89G2GkZmWbGa9BRVN3P+uBxCUjIkPR6XL4DFoMMbCPL5znoWnDEMo06D1x/E73FSbAejXsPIrITwz8LnYmudj5R4IylxBoz2Elj5B/jJ4wQNCQj29Yxo6+MNQHMlnDy6w013QGAvbPsbDJjPxNGDoPgThqUOhurNkHsOAye1LoBXu40MvRlKStFUbSSpdivkTYZhM6F+B1pnLdqAByypkDUOrNmw9p9QsQ4S8xDmZCZ7HZBxCxhSELXbIOiFJdNh7KVw8o3w5nzIHg+nzg8fV/09xKVDXRGE/NBcBeMvCyePuiKY8Vso+wbWLw1vH3kBrFoM6aOg8EdgSgRDHIw4F756Mvz5x14cHsOv0YbjfHM+/OAXsPElKJwN7iao3wH5p0DZ1zB+LjiqIGe/Z7K67VDyOSTlg84ELXUw4jyo3w6ZhbDzfajdBjXfw6BpULwKkgaG900ugHFzwOeALa/DsLPC71tSwF4BXkf489VsgSGnw8BTw0U6dxPYy8CcAkkDwtu3vhH+2Vla+757neH4s08AjQY8zRD0tW5rhsR8+OKv4c+VmAvFn8DOD8O/y4zR4f1qNod/J218LqhcD4b48O8jsdNl0btm2/v/7d15nBTVtcDx35kdGJgZdmRHQWURRERRMURDRIPirkSfS4wkRo1GTYKfvBiieVnM4haeippIXkzUGBeiRCRgDEYji4LK4MIOCrLIgCAM0zPn/XGqmWboaXqGKXq6+3w/n/509e3q6nu7q+rcpRZo083yU/kZ/GEcnHob9BlVO0/VTvtfI5W2/MLW8Ze1dj689zx8YSJs/wTKejYsLymUTIvgl0BP4MEg6RvAGlW9OeS8xdWcWgTrt+7ih8+9y5wPNzKoawlvra7Y68iHuo4vrWBgzy48U76NKwtnM7fd2XRnPQtWb+U97c7YgZ35dGeEjZs387kW0bm0BUcWbmR1TUc6l7bkoiHtef2thZzYGSpbdGBbfifyqj9nWPsIWtab4gIh56N5VKxcRFHLVqzpehb56+bTuayYFroTXbcIbX84OTs2wPYN0OcLtkOI7IJjrrSNdOUcWP4KtGoPZ94LK16BV++yneiAcyx97bzaDb3vadCqHeQWQIcj4NW7bQdU2gMiO2H1G7Bjg+3Itq2DVa/ajqygFSDw6XLIK4QN5dB/HER2w4p/WZ6it7Au6QFbV+/7g5b0gN3bYeenTfentjsMNi9tuuUdDIPHQ/fhsGOTBdJ//hxq6jk+Pq/IfttEWnW0k07MLAAAFIlJREFU/ywZQy6x54WPJZ6vqAR2BXe5LSi2/w0scGgN7KqA3EILwAB5LWz9iedrM2x57/wF5vx67/fadIN+p1nAfuMB+z83fQCflNtvU9YTijtD50G2Dj52vpVh2Wz7/WqC4c8LpkJxRwt6s38Cn1vXIUeMtd96wVQYeRO8dh9c9pwF5IdOtXU2WtYxv4ABZ4PkwLaPYPtGWLcQuh4DvU+27affGCgsrs1/TQ3MvsPy9MYDMPBcC7afrrDWaUHjj9w/0K6hHGACdlIZwEzgoeB+xgddcwkEry3bxKQ/zmTFzpa0K6iiSvM4oWsON47qQd6uLUQWPUm3Ta9S3vV88gdfSP853yLn4zehsAQt7oDE29kUtrGaUTyFbWzjrqozoJmTV/9G31itu9iKnYz8lvvmKVbsxh3VaaDl+7P1sD32DEyxoNC+H6x/25IGnGM106icfBh5Myz5G2xYvPdyS3tYTfqE623D3rDENsLyZ+39vl+22ty7f60t45BLoU0X+Pc9tbXTqENPhWWzbPqIsbD0H9DxSAto29fD9W9a7b38OdsZ//pwq0kOuhCe/SYM/wYcOdZqlBWrYfottcsePgHmTrHpfqfDB3+36bZ9oGoXfPbx3nnpM8paQtEd0jFXwIJH4//mhSUw4lpYO9fyHNW2D3QbDrn58MGL1lIAOPt+ePaafZeTWwCtO1veYx15JiydDVU74n9/fToNhO7HWeCO/U+TFV3XynrZjnZn/NZ0qBqzvbXtYy2+eAG4xwnw1SfgzxfbfEMvg0dGW7CKbhvfWwF39rZ19ezJ+y4jSQd81FCdhY0ELlbVaxudowPQHALBrqpqxt0zmxnbz6dGcpGcPKTuzi6ezkdZbbO+HWfbQ62mvvuz+O/3P9uav71PhlWvWa2ttKfVKNYusB3FqIlWi6n8DBb83j534R9gw3u2E5xxK4y9G4ra2AZeXWU1tPXvWK1s5C1wyBB47V7LS5ch0P1Yq6lvXWO1pk4DYPV/4KgLbUdY+Znla81c2LoWnplg33vD29DmEOs62LUVWpRZC6B939ouoMhuq0XlBL2UqtaH/t50a3UMvgjuHgQn3gin/shqkkVtaj+76lXL06AL6u/L37IS/v592+EVlVjXTqcBVps84dvQotQ21J0V9pusnAPl0+DMu6xmunYunPQdy5uIdRFs+8g23Fg7KyC/hQWzXVvtu2Lt3gGv/RaGX23dJa/80v7DM35Z+zvkt7Aukycuta6tyC7rVup4BFRHYP0iWPayBcPq3fCT4NozPU6AY6+CN6fC6XdawAL7jFbDqn9bN1BOzLH0771ggWXoZfD+3y3fuYX2u3c5Cg4Zav+Fqq0fCx612uzRl1hadZV1Nz1wknU5jZ4Efwy6ba6dB5OPtemzH7DAeOlf4bCgPln+HHz+qQX0ZbOs++crv7YKz31DrUusY394+upgHf4/OPQU+PAleOpKSzvtp9Cyfe36FtXzRCtvqw61wW7AOfabTT2zNoDEa3WU9bL15Pmb7D8cfJG1iJORWwBXzYRNH0LlVnghyU6TE2+wighASXfbzmKd/D341502ffP7FpwboSkOHz0aGA9cCKwAnlbV+xqVmwPUHALBc08+whMLN/Gngp9aQo8RtoGsfNVWwk/esQ37vIdthVz9utXoBpxjzcO5D8Jx19gOrfw520H2GWUbadVO2/gLiq27pP3htuK27tzwQct3nrJacvfh9lrVdnRh9l1GKmt3TpO2Ns0yd2y2HacfnrivLausu66gVerysPwV24GW9YRJJdCuL1w/36ZblMH3V0LFGhs7qOv1/7XKyfHXwphge6qOQG5QMZgUBNOJa2x7qdwOPwvGAb4+25b5q757L/Oix2yb3L0d7jnK0m7bYuMAYIFWa2xdfWQ0VKyq7a7qfTJc/jfbVmqqLR+7P7dWZWkPePQr8X+Di/5on40G/0ildV31GmlddZXb4Mt3wL0HeIzNiOvgtP9p1EcbNVgsIv2wnf94YBPwBBY4vtioXGSIJUuXM678JsbFDpePmwztDo3/gdKLbRAxqrgDnPLfta8Hnb/3/Pkt7AE2OAmQ36Vxma277GgfaZjyCq2G02XI/udNVqt2TbesTNMcBiT7fKF2+ub3a9ffW5ZaNxTEDwIAx1xuffgjvlWblhuzWxr/hO2Eo63AwmI46SaY+5C1evJiTjzrP84qVl0G2zrTMuYCbTkxR4xFlwVB19cq22F3OByO/bqli9Tmo6AlDPlq/Pwfdw28cT90P37vFmBeIRx9qU2fc789J6p0j77DWuJTz6xnBgHUehVCkOioofeAOcBYVV0KICLfCSUXaWLxzKl8Pue3e9/O5/zf1x8EstXo21OdA5cqsd0WxR32P39BKzgzwQ0PDx9jj1hf+hF88Qd7BwywAHHhH2pfJ9OCjOa3qARG/3j/80ed8StrcX9pEoz5WXLfFW+e0bdbi+q4b1rQjDem1vMk2LjEuvE6D0o+jw2QKBCcC1wMvCwiL2KXmWhQ21xExgD3ALnAw6r68zjzXAhMwg6cW6Sq9YTe1NKK1Qz497f3DgKDLrBRfefcwRUbBEbeAnN+Zf3rdY1/3Grn9SkOAkF9h4TW1fZQ+HSZjdcNvzr5/NZ1xQs2tjXgbGtBR313qR02O/93dpTQqIlQ0s2CzosTbXwtBPUGAlV9FnhWRFoB44AbgY4icj/wjKq+lGjBwfkGk4HR2O0t54nINFUtj5mnL3ArcKKqbhGRcO660ATWz36QvTpocgvh3IdSlR3nXNSpP4QTrrPxiLoOPz3xZ6MtAknyrO3/etoG2ZNp7STS66T46UVtoP9Z9og1ZLw9QpLMzet3AH8C/hRcXuIC4PvYPQkSGQ4sVdXlACLyOBZQymPmuRqYrKpbgu9K8uDlgy/ngxeYq/05+tiR5HcZYH2bzrnmIV4QSEZ0HCHZQ2HLetmhuY3V7/R9Dw1uBpI5s3iPYIc9JXjsT1cg9jiotcBxdebpB3sudZ0LTFLVF+suSEQmYOcy0KNHj4ZkuWns2EynXSuY0fprDB9758H/fudcOAqCk7l2N/CciMb66uMH53saKNVXscoD+gKjsKOTHhKR0rozqeoUVR2mqsM6dDjAJlljBGeu5pU28PR151zz1jro8C3ulNp8pFiDWgQN9BEQO3rTLUiLtRZ4I7is9QoR+QALDMndI+4g2bFtM62A0rYpCELOufD0OtGO/Os3Zv/zZrAwWwTzgL4i0ltECrAjkKbVmedZrDWAiLTHuoqW08xs27IJgOISP57duYwz8NwDuoZPJggtEKhqBLgOmAEsAZ5U1cUicruIRIfEZwCbRaQceBn4rqpuDitPjbVzm3UNFbVu5ICUc841Y2F2DaGq04HpddJui5lW4Kbg0WxVbrdrk7Rq0z7FOXHOuaaX6sHitFC1owKAkjLvGnLOZR4PBEmo3lnBbs2lpE2b/c/snHNpxgNBEmTnZj6jFcVF+anOinPONTkPBEnotPUdPszpvde9hp1zLlN4INifHZvpUrmcD1o24WWVnXOuGfFAsD/r3gJgS9ngFGfEOefC4YFgP2o+XmTPncK5IYRzzqVaqOcRZILKjxezWdvTsWOzvUK2c84dEG8R7EfVlrV8rO3oVpbdp6A75zKXB4L9yNm+jk+0jO5lLVKdFeecC4UHgkRUKdz5Ceu1LYeUeiBwzmUmDwSJ7Kogv6aS7QUdKMrPTXVunHMuFB4IEtm11Z7y/NISzrnM5YEgkZpqe871S0s45zKXB4JEaiIASI4fZeucy1weCBKJBoJcDwTOuczlgSCRIBDkeNeQcy6DeSBIxFsEzrks4IEgkWCwONcDgXMug3kgSCTaNZTnXUPOuczlgSCRPWME3iJwzmWuUAOBiIwRkfdFZKmITIzz/hUislFEFgaPr4eZnwbzwWLnXBYIraorIrnAZGA0sBaYJyLTVLW8zqxPqOp1YeXjgETHCPK8ReCcy1xhtgiGA0tVdbmq7gYeB8aF+H1NL2gR5OYWpDgjzjkXnjADQVdgTczrtUFaXeeJyNsi8pSIdI+3IBGZICLzRWT+xo0bw8hrfNFA4C0C51wGS/Vg8d+AXqp6FDATmBpvJlWdoqrDVHVYhw4dDlrmtDoIBPk+RuCcy1xhBoKPgNgafrcgbQ9V3ayqlcHLh4FjQsxPg1VFqgDI88NHnXMZLMxAMA/oKyK9RaQAuBiYFjuDiHSJeXkWsCTE/DRYdRAIcv2oIedcBgut81tVIyJyHTADyAV+p6qLReR2YL6qTgO+LSJnARHgU+CKsPLTGJGqoEWQ72MEzrnMFeoeTlWnA9PrpN0WM30rcGuYeTgQ1d415JzLAqkeLG7WooEg3weLnXMZzANBApEgEBQUFKY4J845Fx4PBAlEA0FhgbcInHOZywNBApGInUdQkO9nFjvnMpcHggSq93QNeSBwzmUuDwQJeCBwzmUDDwQJRANBkQcC51wG80CQQE11hIjmUFSQm+qsOOdcaDwQJFBTXUU1uRTmeSBwzmUuDwQJ1EQiRMihKN9/Judc5vI9XAI11RGqyaUo31sEzrnM5YEggZrqKiLkkJ/rP5NzLnP5Hi4BrYlQg7cGnHOZzQNBAlodoVo8EDjnMlvWXGj/7Vl/pturE1FN/jMDdQdbpDS8TDnnXDOQNYEgv6wri9uMJC9HECTpz+X0GkGX/c/mnHNpK2sCwZFDT4ahJ6c6G8451+z4GIFzzmU5DwTOOZflPBA451yW80DgnHNZLtRAICJjROR9EVkqIhMTzHeeiKiIDAszP8455/YVWiAQkVxgMnA60B8YLyL948zXGrgBeCOsvDjnnKtfmC2C4cBSVV2uqruBx4Fxcea7A/gFsCvEvDjnnKtHmIGgK7Am5vXaIG0PERkKdFfVFxItSEQmiMh8EZm/cePGps+pc85lsZSdUCYiOcBvgCv2N6+qTgGmBJ/bKCKrGvm17YFNjfxsc+NlaZ68LM1PppQDDqwsPet7I8xA8BHQPeZ1tyAtqjUwEPiniAB0BqaJyFmqOr++hapqh8ZmSETmq2pGDEh7WZonL0vzkynlgPDKEmbX0Dygr4j0FpEC4GJgWvRNVd2qqu1VtZeq9gL+AyQMAs4555peaIFAVSPAdcAMYAnwpKouFpHbReSssL7XOedcw4Q6RqCq04HpddJuq2feUWHmJTDlIHzHweJlaZ68LM1PppQDQiqLaEMu0O+ccy7j+CUmnHMuy3kgcM65LJc1gSDZ6x41FyLyOxHZICLvxqS1FZGZIvJh8FwWpIuI3BuU7e3gRL1mQUS6i8jLIlIuIotF5IYgPR3LUiQic0VkUVCWHwfpvUXkjSDPTwRHySEihcHrpcH7vVKZ/3hEJFdE3hKR54PXaVkWEVkpIu+IyEIRmR+kpd06BiAipSLylIi8JyJLRGRE2GXJikCQ7HWPmplHgTF10iYCs1S1LzAreA1Wrr7BYwJw/0HKYzIiwM2q2h84Hrg2+O3TsSyVwCmqOhgYAowRkeOxS6TcpaqHAVuAq4L5rwK2BOl3BfM1NzdgR/VFpXNZvqiqQ2KOs0/HdQzgHuBFVT0CGIz9P+GWRVUz/gGMAGbEvL4VuDXV+Uoi372Ad2Nevw90Caa7AO8H0w8C4+PN19wewHPA6HQvC9ASeBM4DjvTM6/uuoYdOj0imM4L5pNU5z2mDN2CncopwPOApHFZVgLt66Sl3ToGlAAr6v62YZclK1oEJHHdozTRSVXXBdPrgU7BdFqUL+hOOBq70mxaliXoSlkIbABmAsuACrXzZmDv/O4pS/D+VqDdwc1xQncD3wNqgtftSN+yKPCSiCwQkQlBWjquY72BjcDvgy67h0WkFSGXJVsCQcZRC/9pc+yviBQDfwVuVNVtse+lU1lUtVpVh2C16eHAESnOUqOIyFhgg6ouSHVemshJqjoU6yq5VkROjn0zjdaxPGAocL+qHg3soLYbCAinLNkSCPZ33aN08YmIdAEInjcE6c26fCKSjwWBx1T16SA5LcsSpaoVwMtY90mpiERPzozN756yBO+XAJsPclbrcyJwloisxC4RfwrWN52OZUFVPwqeNwDPYEE6HdextcBaVY3en+UpLDCEWpZsCQQJr3uURqYBlwfTl2P97dH0y4IjCI4HtsY0I1NKRAR4BFiiqr+JeSsdy9JBREqD6RbYWMcSLCCcH8xWtyzRMp4PzA5qcymnqreqaje163xdjOXtEtKwLCLSSuwGVwTdKF8G3iUN1zFVXQ+sEZHDg6RTgXLCLkuqB0cO4iDMGcAHWJ/uD1KdnyTy+2dgHVCF1RKuwvpkZwEfAv8A2gbzCnZU1DLgHWBYqvMfU46TsGbs28DC4HFGmpblKOCtoCzvArcF6X2AucBS4C9AYZBeFLxeGrzfJ9VlqKdco4Dn07UsQZ4XBY/F0e07HdexIH9DgPnBevYsUBZ2WfwSE845l+WypWvIOedcPTwQOOdclvNA4JxzWc4DgXPOZTkPBM45l+U8EDhXh4hUB1exjD6a7Gq1ItJLYq4o61xzEOqtKp1LUzvVLiPhXFbwFoFzSQqueX9ncN37uSJyWJDeS0RmB9eDnyUiPYL0TiLyjNj9CxaJyAnBonJF5CGxexq8FJyl7FzKeCBwbl8t6nQNXRTz3lZVHQT8Frt6J8B9wFRVPQp4DLg3SL8XeEXt/gVDsbNewa4dP1lVBwAVwHkhl8e5hPzMYufqEJHtqlocJ30ldmOa5cGF9NarajsR2YRdA74qSF+nqu1FZCPQTVUrY5bRC5ipdoMRROT7QL6q/iT8kjkXn7cInGsYrWe6ISpjpqvxsTqXYh4InGuYi2KeXw+mX8Ou4AlwCTAnmJ4FXAN7bmhTcrAy6VxDeE3EuX21CO5CFvWiqkYPIS0TkbexWv34IO167I5S38XuLnVlkH4DMEVErsJq/tdgV5R1rlnxMQLnkhSMEQxT1U2pzotzTcm7hpxzLst5i8A557Kctwiccy7LeSBwzrks54HAOeeynAcC55zLch4InHMuy/0/kNHMuXxEFYsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uJ-uLmHSuUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "98f77e3f-670e-4735-854a-188b6488c46a"
      },
      "source": [
        "pred_data.replace({\"HTR\":{'H':1,'A':2,'D':0}},inplace=True)\n",
        "pred_data = scaler.fit_transform(pred_data)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:6666: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  regex=regex,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU4PfnpTfG-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZPPh0iyPHF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pred_data = pd.DataFrame(pred_data,columns=['HTR','HS','AS','HST','HF','AF','HY','AY','HR','AR','HC','AC','AST'])\n",
        "result=model.predict(pred_data)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPcKVUIJd1OT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78d99bcb-842c-4df9-91f3-84aabc2d3301"
      },
      "source": [
        "type(result)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzI3lFU5XpSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c75442fe-e3ae-4d14-9a13-79ba149fcd41"
      },
      "source": [
        "model.predict_classes(pred_data[[2]])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-37-6ee8c6417924>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x9N3pnOoTwE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1fbaeddc-8934-46fd-8eda-6ec211ac6697"
      },
      "source": [
        "  Class_prediction=pd.DataFrame(model.predict_classes(pred_data))\n",
        "  Class_prediction.replace({0:{1:'H',2:'A',0:'D'}},inplace=True)\n",
        "  type(Class_prediction)\n",
        "  # result=[]\n",
        "  # for i in Class_prediction:\n",
        "  #   if Class_prediction[i]==1:\n",
        "  #     result.append('H')\n",
        "  #   elif Class_prediction[i]==2:\n",
        "  #     result.append('A')\n",
        "  #   else:\n",
        "  #     result.append('D')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0bMKiU3ru8m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "62d73e65-a215-4253-d31f-3fe63599f539"
      },
      "source": [
        "Class_prediction\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>H</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>H</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>H</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>H</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>H</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>313 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0\n",
              "0    H\n",
              "1    A\n",
              "2    H\n",
              "3    H\n",
              "4    H\n",
              "..  ..\n",
              "308  A\n",
              "309  H\n",
              "310  D\n",
              "311  A\n",
              "312  D\n",
              "\n",
              "[313 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLQnte33ssFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(Class_prediction)\n",
        "# print(tester,type(tester))\n",
        "# tester"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htbg-QAktHTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "3fd7c598-a7db-40c4-a8b7-4d8dedc53f5a"
      },
      "source": [
        "prediction = tester[['HomeTeam','AwayTeam','FTR']]\n",
        "\n",
        "result=pd.DataFrame(result)\n",
        "\n",
        "prediction['Draw']=result[[0]]\n",
        "prediction['Homewin']=result[[1]]\n",
        "prediction['awaywin']=result[[2]]\n",
        "\n",
        "prediction['Predicted result']=Class_prediction\n",
        "\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hujIHvrjfWct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prediction.replace({\"Predicted result\":{1:'H',2:'A',0:'D'}})"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhFNLrgjtyM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction.to_csv('gdrive/My Drive/Colab Notebooks/epl/log.csv',index=False)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqiqRB-owS4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(result[0,2])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkHbUavHxSOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNFH2wMcQwBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "14ce76e4-31ea-44f1-bfdc-331f7339ce7c"
      },
      "source": [
        "print(prediction)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           HomeTeam          AwayTeam FTR  ...   Homewin   awaywin  Predicted result\n",
            "0         Liverpool           Norwich   H  ...  0.843858  0.025708                 H\n",
            "1          West Ham          Man City   A  ...  0.009865  0.922031                 A\n",
            "2       Bournemouth  Sheffield United   D  ...  0.451155  0.188250                 H\n",
            "3           Burnley       Southampton   H  ...  0.591384  0.096262                 H\n",
            "4    Crystal Palace           Everton   D  ...  0.668478  0.075179                 H\n",
            "..              ...               ...  ..  ...       ...       ...               ...\n",
            "308     Southampton           Arsenal   A  ...  0.001801  0.964954                 A\n",
            "309         Chelsea          Man City   H  ...  0.998382  0.000017                 H\n",
            "310     Aston Villa            Wolves   A  ...  0.221179  0.334518                 D\n",
            "311         Watford       Southampton   A  ...  0.002729  0.959257                 A\n",
            "312  Crystal Palace           Burnley   A  ...  0.288222  0.332179                 D\n",
            "\n",
            "[313 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-6yup-MlbDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def single_predict(df,home,away):\n",
        "\n",
        "  filtered_data = df[(df.HomeTeam == home) & (df.AwayTeam == away)]\n",
        "  input_data= filtered_data[['HS','AS','HST','AST','HF','AF','HY','AY','HR','AR','HC','AC','HTR']]\n",
        "  input_data.replace({\"HTR\":{'H':1,'A':2,'D':0}},inplace=True) \n",
        "  Predicted=pd.DataFrame(model.predict(input_data))\n",
        "\n",
        "  classes=pd.DataFrame(model.predict_classes(input_data))\n",
        "\n",
        "  classes=classes.replace({0:{1:'H',2:'A',0:'D'}})\n",
        "  Predicted['Predicted result']=classes[0]\n",
        "  Predicted['HomeTeam']=home\n",
        "  Predicted['AwayTeam']=away\n",
        "\n",
        "\n",
        "  return Predicted"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN7I8b_iSigN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predictor(home,away):\n",
        "  # ht='Liverpool'\n",
        "  # at='Leicester'\n",
        "  result=prediction[(prediction.HomeTeam == home) & (prediction.AwayTeam == away)]  #query in the dataframe\n",
        "  \n",
        "  if int(result.shape[0]) > 0:\n",
        "    index=result.index[0]\n",
        "    print(\"The actual FTR is \",(result['FTR'][index]))\n",
        "    if (result['Predicted result'][index])=='H':\n",
        "      print(\"The predicted winner: \",home)\n",
        "    else:\n",
        "      print(\"The predicted winner: \",away)\n",
        "  else:\n",
        "    print(\"Prediction using data from previous matches\")\n",
        "    file_path= path + '/dataset/recent.csv'\n",
        "    old_data = pd.read_csv(file_path)\n",
        "    P = single_predict(old_data,home,away)\n",
        "    print(P['Predicted result'][0])\n",
        "    display(P)\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJzuNZrB_iiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='gdrive/My Drive/Colab Notebooks/epl'"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWTqDP50fNH-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "524c4d35-7733-4136-9482-0d66e597b8cd"
      },
      "source": [
        "predictor('Liverpool','Chelsea')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction using data from previous matches\n",
            "A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:6666: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  regex=regex,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>Predicted result</th>\n",
              "      <th>HomeTeam</th>\n",
              "      <th>AwayTeam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.023296</td>\n",
              "      <td>0.052899</td>\n",
              "      <td>0.923804</td>\n",
              "      <td>A</td>\n",
              "      <td>Liverpool</td>\n",
              "      <td>Chelsea</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2 Predicted result   HomeTeam AwayTeam\n",
              "0  0.023296  0.052899  0.923804                A  Liverpool  Chelsea"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soOOantIACuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predictor('Liverpool','Liverpool')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld7g6_SDosTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "10a731c6-fcfc-47c7-d069-fe5626033408"
      },
      "source": [
        "prediction"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HomeTeam</th>\n",
              "      <th>AwayTeam</th>\n",
              "      <th>FTR</th>\n",
              "      <th>Draw</th>\n",
              "      <th>Homewin</th>\n",
              "      <th>awaywin</th>\n",
              "      <th>Predicted result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Liverpool</td>\n",
              "      <td>Norwich</td>\n",
              "      <td>H</td>\n",
              "      <td>0.130434</td>\n",
              "      <td>0.843858</td>\n",
              "      <td>0.025708</td>\n",
              "      <td>H</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>West Ham</td>\n",
              "      <td>Man City</td>\n",
              "      <td>A</td>\n",
              "      <td>0.068104</td>\n",
              "      <td>0.009865</td>\n",
              "      <td>0.922031</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bournemouth</td>\n",
              "      <td>Sheffield United</td>\n",
              "      <td>D</td>\n",
              "      <td>0.360595</td>\n",
              "      <td>0.451155</td>\n",
              "      <td>0.188250</td>\n",
              "      <td>H</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Burnley</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>H</td>\n",
              "      <td>0.312354</td>\n",
              "      <td>0.591384</td>\n",
              "      <td>0.096262</td>\n",
              "      <td>H</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Crystal Palace</td>\n",
              "      <td>Everton</td>\n",
              "      <td>D</td>\n",
              "      <td>0.256343</td>\n",
              "      <td>0.668478</td>\n",
              "      <td>0.075179</td>\n",
              "      <td>H</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>Southampton</td>\n",
              "      <td>Arsenal</td>\n",
              "      <td>A</td>\n",
              "      <td>0.033244</td>\n",
              "      <td>0.001801</td>\n",
              "      <td>0.964954</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>Chelsea</td>\n",
              "      <td>Man City</td>\n",
              "      <td>H</td>\n",
              "      <td>0.001601</td>\n",
              "      <td>0.998382</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>H</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>Aston Villa</td>\n",
              "      <td>Wolves</td>\n",
              "      <td>A</td>\n",
              "      <td>0.444303</td>\n",
              "      <td>0.221179</td>\n",
              "      <td>0.334518</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>Watford</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>A</td>\n",
              "      <td>0.038014</td>\n",
              "      <td>0.002729</td>\n",
              "      <td>0.959257</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>Crystal Palace</td>\n",
              "      <td>Burnley</td>\n",
              "      <td>A</td>\n",
              "      <td>0.379599</td>\n",
              "      <td>0.288222</td>\n",
              "      <td>0.332179</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>313 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           HomeTeam          AwayTeam FTR  ...   Homewin   awaywin  Predicted result\n",
              "0         Liverpool           Norwich   H  ...  0.843858  0.025708                 H\n",
              "1          West Ham          Man City   A  ...  0.009865  0.922031                 A\n",
              "2       Bournemouth  Sheffield United   D  ...  0.451155  0.188250                 H\n",
              "3           Burnley       Southampton   H  ...  0.591384  0.096262                 H\n",
              "4    Crystal Palace           Everton   D  ...  0.668478  0.075179                 H\n",
              "..              ...               ...  ..  ...       ...       ...               ...\n",
              "308     Southampton           Arsenal   A  ...  0.001801  0.964954                 A\n",
              "309         Chelsea          Man City   H  ...  0.998382  0.000017                 H\n",
              "310     Aston Villa            Wolves   A  ...  0.221179  0.334518                 D\n",
              "311         Watford       Southampton   A  ...  0.002729  0.959257                 A\n",
              "312  Crystal Palace           Burnley   A  ...  0.288222  0.332179                 D\n",
              "\n",
              "[313 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Vxy5vQpkMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 54,
      "outputs": []
    }
  ]
}